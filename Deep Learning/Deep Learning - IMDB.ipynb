{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94471057",
   "metadata": {},
   "source": [
    "### Classificação de textos para análise de sentimentos\n",
    "\n",
    "### Aluno: Guilherme Cristiano Goll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80224ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T00:44:12.514627Z",
     "start_time": "2021-06-10T00:44:12.509125Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.legacy import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffccdf8-91e4-473c-8261-0c69e0ad89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Pytorch 1.8.1+cu111\n",
      "Versão do torchtext 0.9.1\n"
     ]
    }
   ],
   "source": [
    "print('Versão do Pytorch', torch.__version__)\n",
    "print('Versão do torchtext', torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d8776a-b7da-481d-9d7b-3810081481ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bc807adc70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 43\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907e3626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T00:50:27.424355Z",
     "start_time": "2021-06-10T00:49:16.448387Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = datasets.IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd27a9e3-1b7b-4b87-8f3e-153ec68e7b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823038fb-c557-4425-9fc9-c47f56bba84e",
   "metadata": {},
   "source": [
    "Ao invés de trabalhar com os cursores iterativos `train_iter` e `test_iter`, transformei ambos em `Numpy.array` e em seguida em um `Pandas.DataFrame`, que torna a leitura e manipulação dos dados mais simples.\n",
    "\n",
    "Obs: este procedimento é relativamente _lento_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b99f748-9109-4fe4-a184-8ab8643167bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = np.array(train_iter)\n",
    "df_train = pd.DataFrame(df_train, columns=['sentiment', 'review'])\n",
    "\n",
    "df_test = np.array(test_iter)\n",
    "df_test = pd.DataFrame(df_test, columns=['sentiment', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9279e16f-217d-4bb9-bfbc-a98afcb5f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(df, column, title_sufix: str=None):\n",
    "    '''\n",
    "    Plota a distribuição da coluna em um gráfico de barras horizontal.\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    grouped = df[column].value_counts().sort_values(ascending=True)\n",
    "\n",
    "    df_grouped = grouped.to_frame()\n",
    "    df_grouped.reset_index(level=0, inplace=True)\n",
    "    df_grouped.columns = ['item', 'count']\n",
    "    labels = df_grouped['item'].astype(str)\n",
    "    values = df_grouped['count']\n",
    "\n",
    "    ax.clear()\n",
    "    ax.barh(labels, values, alpha=0.75)\n",
    "    for i, (value, name) in enumerate(zip(values, labels)):\n",
    "        ax.text(value, i, value, size=12, ha='left', va='center')\n",
    "\n",
    "    ax.tick_params(axis='x', colors='#777777', labelsize=12)\n",
    "    ax.margins(0.003, 0.01)\n",
    "    ax.grid(which='major', axis='x', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    title = 'Distribuição de classes'\n",
    "    if title_sufix is not None:\n",
    "        title = f'{title} no conjunto de {title_sufix}'\n",
    "        \n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45960659-08d3-4183-b1fb-a6c42196462e",
   "metadata": {},
   "source": [
    "Após a leitura e transformação inicial do conjunto de dados, verificamos através do gráfico a seguir a distribuição dos registros no conjunto de treino.\n",
    "\n",
    "Felizmente, os dados estão balanceados, o que auxilia na redução do viés ao treinar e testar o modelo, mais a frente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0fc3214-4c2a-4027-a78a-061407c86f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution(df_train, 'sentiment', 'treino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472844a-9b51-493c-bfee-98528082f1d8",
   "metadata": {},
   "source": [
    "# Pré-tratamento dos dados\n",
    "\n",
    "Antes de iniciar a aplicação de algoritmos de ML para classificação dos dados, por se tratar de um conjunto de dados textual serão aplicados alguns tratamentos no texto.\n",
    "\n",
    "Em especial, serão removidas as _stopwords_ e a pontuação em geral, e é aplicada a técnica _stem_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1498d97-0845-4bb2-9f16-5bf5dfc8fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guilh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\guilh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8b4db7-9f87-4691-81b9-476395a35449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df, index: int=1, remove_punc: bool=True):\n",
    "    '''Realiza a limpeza e normalização do dataframe: remoção de stopwords e \n",
    "    pontuação, e aplicação de stem\n",
    "    df -> dataframe que será normalizado\n",
    "    index -> índice da coluna que será normalizada (aquela que contém o \n",
    "    texto). Default é 1\n",
    "    remove_punc -> Deve remover a pontuação? Padrão é True\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    t_df = df.copy()\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    def normalize(text: str):        \n",
    "        final_sent = []\n",
    "        if remove_punc:\n",
    "            tokenized_words = tokenizer.tokenize(text)\n",
    "            for word in tokenized_words:\n",
    "                if word not in stop_words:\n",
    "                    stemmed_word = ps.stem(word)\n",
    "                    final_sent.append(' ' + stemmed_word)\n",
    "        else:\n",
    "            tokenized_sent = sent_tokenize(text.strip())\n",
    "            for sent in tokenized_sent:\n",
    "                tokenized_words = word_tokenize(sent)\n",
    "                for word in tokenized_words:\n",
    "                    if word not in stop_words:\n",
    "                        stemmed_word = ps.stem(word)\n",
    "                        final_sent.append(' ' + stemmed_word)\n",
    "            \n",
    "        \n",
    "        return ''.join(final_sent)\n",
    "    \n",
    "    for idx in range(len(t_df)):\n",
    "        t_df.iat[idx, index] = normalize(t_df.iloc[idx, index])\n",
    "        if (idx + 1) % 5000 == 0:\n",
    "            print(f'{idx + 1} documentos normalizados')\n",
    "    \n",
    "    return t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af96590-8918-467f-afc9-0b2e563e892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 documentos normalizados\n",
      "10000 documentos normalizados\n",
      "15000 documentos normalizados\n",
      "20000 documentos normalizados\n",
      "25000 documentos normalizados\n"
     ]
    }
   ],
   "source": [
    "df_train_normalized = normalize_df(df_train, remove_punc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854395f-dc9a-489f-8a1b-e904661be187",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350b0fa-c407-4747-9a0b-17d72cc8054d",
   "metadata": {
    "tags": []
   },
   "source": [
    "O Word Embedding é um processo de transformação de palavras/ texto em números.\n",
    "\n",
    "Após realizar a limpeza dos dados no conjunto de treino completo, vamos transformá-lo em números utilizando o `TfIdfVectorizer`. O __TF-IDF__ mede estatisticamente a relevância de uma palavra (ou N-grams) junto a um conjunto de documentos, multiplicando a frequência da palavra no documento (__TF__) pela frequência inversa das palavras em um conjunto de documentos (__IDF__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357f7be5-bad2-4c54-961c-633b79b7e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separação das features (X) e variável predita (y)\n",
    "X, y = df_train_normalized['review'], df_train_normalized['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c143d9-8290-4cfe-83f1-3800337ca502",
   "metadata": {
    "tags": []
   },
   "source": [
    "__IMPORTANTE__: o enunciado deste problema nos fornece dois conjuntos de dados: treino e teste. No entanto, o conjunto de treino será sub-dividido durante o treinamento do modelo para fins de treino e teste (aferição inicial dos resultados). Em seguida, o conjunto original de teste será utilizado para __validação__, que também deve exibir a acurácia do modelo sob este conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882a5237-db2b-41a1-97f7-dcc25081f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251e4ec-5355-40b7-ae9b-8cc02cdfcd45",
   "metadata": {
    "tags": []
   },
   "source": [
    "Após dividir o conjunto inicial de treino, para treino e teste, será montado o vocabulário. Ele consiste numa lista de palavras, bi-gramas e tri-gramas (conjuntos de duas e três palavras respectivamente), que será transformado em números.\n",
    "\n",
    "Porém, serão utilizadas apenas as palavras/n-gramas que tenham uma frequência relevante para os dados avaliados.\n",
    "\n",
    "Este conjunto será a base para comparação textual e treinamento do modelo, logo, ele será a principal _feature_ do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c0adb3-2db3-432d-b7f9-e8109b5b481a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho final do vocabulário 2558\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = set([])\n",
    "for _class in np.unique(df_train_normalized['sentiment']):\n",
    "    words = []\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    df_words = dict()\n",
    "    df_bigrams = dict()\n",
    "    df_trigrams = dict()\n",
    "    doc_frequency = 0\n",
    "    \n",
    "    for line in df_train_normalized[df_train_normalized['sentiment'] == _class]['review'].values:\n",
    "        doc_words = []\n",
    "        doc_bigrams = []\n",
    "        doc_trigrams = []\n",
    "        doc_frequency += 1\n",
    "        \n",
    "        line_text = line.split(' ')\n",
    "        for idx, word in enumerate(line_text):\n",
    "            doc_words.append(word)\n",
    "            if idx > 0:\n",
    "                doc_bigrams.append(f'{line_text[idx - 1]} {line_text[idx]}')\n",
    "            if idx > 1:\n",
    "                doc_trigrams.append(f'{line_text[idx - 2]} {line_text[idx - 1]} {line_text[idx]}')\n",
    "        \n",
    "        words.extend(doc_words)\n",
    "        for d_w in set(doc_words):\n",
    "            if d_w in df_words:\n",
    "                df_words[d_w] += 1\n",
    "            else:\n",
    "                df_words[d_w] = 1\n",
    "                \n",
    "        bigrams.extend(doc_bigrams)\n",
    "        for d_b in set(doc_bigrams):\n",
    "            if d_b in df_bigrams:\n",
    "                df_bigrams[d_b] += 1\n",
    "            else:\n",
    "                df_bigrams[d_b] = 1\n",
    "        \n",
    "        trigrams.extend(doc_trigrams)\n",
    "        for d_t in set(doc_trigrams):\n",
    "            if d_t in df_trigrams:\n",
    "                df_trigrams[d_t] += 1\n",
    "            else:\n",
    "                df_trigrams[d_t] = 1\n",
    "    \n",
    "    size1 = max(2, int(doc_frequency * 0.01))\n",
    "    size2 = max(2, int(doc_frequency * 0.01))\n",
    "    size3 = max(2, int(doc_frequency * 0.01))\n",
    "    \n",
    "    c1 = Counter(words)\n",
    "    c2 = Counter(bigrams)\n",
    "    c3 = Counter(trigrams)\n",
    "    words = [word[0] for word in c1.most_common() if df_words[word[0]] >= size1]\n",
    "    bigrams = [bigram[0] for bigram in c2.most_common() if df_bigrams[bigram[0]] >= size2]\n",
    "    trigrams = [trigram[0] for trigram in c3.most_common() if df_trigrams[trigram[0]] >= size3]\n",
    "    vocabulary |= set(words)\n",
    "    vocabulary |= set(bigrams)\n",
    "    vocabulary |= set(trigrams)\n",
    "\n",
    "print('Tamanho final do vocabulário', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aef993-fcd3-4129-a410-21a39fee0648",
   "metadata": {
    "tags": []
   },
   "source": [
    "A seguir é realizado o 'treinamento', utilizando o conjunto __completo__, e as transformação dos sub-conjuntos utilizando as respectivas parcelas dos dados (treino e teste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3086518d-7ae8-4a2b-a813-ccf3c4da0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), use_idf=True, vocabulary=vocabulary)\n",
    "tfidf_vectorizer.fit(X.to_numpy())\n",
    "\n",
    "train_review_tf = tfidf_vectorizer.transform(X_train.to_numpy()).toarray()\n",
    "test_review_tf = tfidf_vectorizer.transform(X_test.to_numpy()).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700284f9-5bb7-4268-87a3-14450a60160e",
   "metadata": {
    "tags": []
   },
   "source": [
    "A saída abaixo representa as dimensões das features (17500 linhas por 2558 colunas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ac0951-91f5-4d6d-bd92-a5e6c1e882ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 2558)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ad38c-5dc0-46a8-8169-f6108e7b3043",
   "metadata": {
    "tags": []
   },
   "source": [
    "Uma vez realizada a transformação da parte textual semântica (review) do dataset, também é necessário aplicar um tratamento especial à variável predita: _sentiment_. Isto porque ela é do tipo texto. Para tratá-la, a mesma será transformada em uma variável categória numérica (0 e 1), utilizando o `LabelEncoder`, embora também seja possível utilizar o `LabelBinarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c31152-88ac-4116-b4dc-e392ec1395fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_y_train = le.fit_transform(y_train)\n",
    "le_y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea381c-ce3e-4c52-9486-099f734ae07f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Agora, contando com as _features_ e saídas respectivamente codificadas, as mesmas serão transformadas em um tensor do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49b0e132-cbef-4d17-b431-f609d10046cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = map(\n",
    "    torch.tensor, (train_review_tf, test_review_tf, le_y_train, le_y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b866fb3c-6533-44b0-ab7b-b45bd9f288da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = len(y.unique())\n",
    "bs = 500 #batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e182e-58d0-45fc-a919-151cf0a14f8f",
   "metadata": {},
   "source": [
    "Por fim, os sub-conjuntos de treino e teste serão inseridos em objetos do tipo `DataLoader`, uma abstração do PyTorch que facilita as iterações (épocas) e a utilização do _batch size_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c4855b3-d40c-4584-9ca2-880287b604ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_last = False\n",
    "\n",
    "if X_train.size(0) % bs > 0:\n",
    "    print('ATENÇÃO: como a quantidade de linhas do conjunto de treino não é divisível pelo batch size informado,',\n",
    "          'o modelo irá ignorar o último batch de dados')\n",
    "    drop_last = True\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=drop_last)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e0479f-4f6a-433d-8f72-dec63b52e3b5",
   "metadata": {},
   "source": [
    "# Treinamento do modelo\n",
    "\n",
    "O modelo abaixo é do tipo *LSTM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec90c2d7-57e4-4236-820e-237b85d40e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def acc(pred, label):\n",
    "    '''Verifica a acurária das predições realizadas'''\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fdd7bc9-d244-4775-9414-1dee31e6d2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, no_layers, hidden_dim, input_size, output_size, drop_prob=0.5):\n",
    "        super(LSTM, self).__init__() \n",
    "        self.output_dim = output_size\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.no_layers = no_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=no_layers, \n",
    "                            batch_first=True).double()        \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)        \n",
    "        lstm_out, hidden = self.lstm(x.double(), hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        # reshape para o \"batch_first=True\"\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # pega o último batch\n",
    "        \n",
    "        return sig_out, hidden                \n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        '''Inicializa o hidden'''\n",
    "        # Cria dois tensors com tamanho n_layers x batch_size x hidden_dim,\n",
    "        # inicializados com zero, para o hidden e cell status do LSTM\n",
    "        h0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (h0.double(), c0.double())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57ce7e-edac-44d1-8347-39c69eff6b5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "As funções `loss_batch` e `valid_batch` são responsáveis por realizar as predições durante as etapas de treino e validação, respectivamente. A primeira efetua também a atualização dos gradientes/ pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a371169-c20c-4311-8836-1c4442f7f361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, device, xb, yb, hidden, opt=None):\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    output, h = model(xb, h)\n",
    "    loss = criterion(output, yb.double())\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def valid_batch(model, device, xb, yb, hidden):\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    output, h = model(xb, h)\n",
    "    loss = criterion(output, yb.double())\n",
    "    correct = acc(output, yb)\n",
    "    \n",
    "    return loss.item(), correct, len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a2d6b-d847-43d9-b601-c7cf79d1f531",
   "metadata": {
    "tags": []
   },
   "source": [
    "Através da função `fit` é que será realizado o treinamento do modelo, através de um loop a ser executado por `epochs` vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5490e31f-c5ca-4afb-b6de-69d4b4ed53ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, opt, epochs, train_dl, test_dl, device):\n",
    "    scheduler = None\n",
    "    if opt is not None:\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=2, gamma=0.5)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Iniciando época {epoch + 1}/{epochs}')\n",
    "        model.train()\n",
    "        hidden = model.init_hidden(train_dl.batch_size, device)\n",
    "        losses, nums = zip(\n",
    "            *[loss_batch(model, device, xb, yb, hidden, opt) for xb, yb in train_dl])\n",
    "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            hidden = model.init_hidden(test_dl.batch_size, device)\n",
    "            losses, corrects, nums = zip(\n",
    "                *[valid_batch(model, device, xb, yb, hidden) for xb, yb in test_dl])\n",
    "            test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            test_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "            print(f'- Train loss: {train_loss:.6f}\\t',\n",
    "                  f'Test loss: {test_loss:.6f}\\t',\n",
    "                  f'Test accuracy: {test_accuracy:.3f}%')\n",
    "        \n",
    "        if opt is None:\n",
    "            print('Não há otimizador para este modelo. Loop de épocas será interrompido.')\n",
    "            return model\n",
    "        \n",
    "        scheduler.step()    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8878a177-45f9-4ce0-bb42-61d7318d1a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando época 1/2\n",
      "- Train loss: 0.655113\t Test loss: 0.528081\t Test accuracy: 83.280%\n",
      "Iniciando época 2/2\n",
      "- Train loss: 0.357720\t Test loss: 0.293522\t Test accuracy: 88.080%\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(2, 256, input_size, output_size, dropout)\n",
    "model = model.to(device)\n",
    "#necessário para 'uniformizar' o tratamento dos números de ponto flutuante \n",
    "#ao utilizar placa de video (device = cuda)\n",
    "model.double()\n",
    "criterion = nn.BCELoss()\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model_novo = fit(model, opt, epochs, train_dl, test_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe3035-981f-424a-b5a2-7c7f1ac5dfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Após retreinar o modelo algumas vezes e com parâmetros diferentes, observei que o melhor resultado (menor test loss e melhor test accuracy) ocorriam entre o primeiro e o segundo loop. Dali em diante o train loss continuava reduzindo, porém o test loss subia, indicando _overfitting_ no conjunto de treino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b988e-14cf-4ea4-af56-cf504cacdd50",
   "metadata": {},
   "source": [
    "# Aplicação do modelo no conjunto de validação\n",
    "\n",
    "Uma vez treinado o modelo, o mesmo será submetido ao conjunto de validação. Para isto, as mesmas etapas de pré-processamento junto ao texto serão aplicadas, tornando o _input_ apropriado ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11b5588-b872-47a5-aec2-608742a69374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 documentos normalizados\n",
      "10000 documentos normalizados\n",
      "15000 documentos normalizados\n",
      "20000 documentos normalizados\n",
      "25000 documentos normalizados\n"
     ]
    }
   ],
   "source": [
    "#pré-processamento do texto\n",
    "df_test_normalized = normalize_df(df_test, remove_punc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98f86770-3932-47a4-8972-176b18650e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformação do texto e variável predita\n",
    "Xn, yn = df_test_normalized['review'], df_test_normalized['sentiment']\n",
    "valid_review_tf = tfidf_vectorizer.transform(Xn.to_numpy()).toarray()\n",
    "valid_yn_le = le.transform(yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "410cbef7-0904-44bb-b498-f5ec06dea998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criação do DataLoader\n",
    "Xn, yn = map(\n",
    "    torch.tensor, (valid_review_tf, valid_yn_le)\n",
    ")\n",
    "\n",
    "valid_ds = TensorDataset(Xn, yn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8443b2a-d9d8-40b7-b5ba-7948afcaf31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, data_loader):\n",
    "    '''Function to run the model with de validation dataset.'''\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(data_loader.batch_size, 'cuda')\n",
    "        losses, corrects, nums = zip(\n",
    "                *[valid_batch(model, device, xb, yb, hidden) for xb, yb in test_dl])\n",
    "        valid_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        valid_acc = np.sum(corrects) / np.sum(nums) * 100\n",
    "        return valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd7b4b93-c3d0-44e0-985f-e53c6b74a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado da execução do modelo no conjunto de validação:\n",
      " \tLoss: 0.29352208110365147\n",
      " \tAcurácia: 88.08%\n"
     ]
    }
   ],
   "source": [
    "valid_loss, valid_acc = validate_model(model_novo, valid_dl)\n",
    "print('Resultado da execução do modelo no conjunto de validação:\\n',\n",
    "      f'\\tLoss: {valid_loss}\\n',\n",
    "      f'\\tAcurácia: {valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1d719-2796-4dc9-a0aa-9b0981e96a65",
   "metadata": {},
   "source": [
    "Com isto, observamos que o modelo retornou um resultado sustentável em relação ao período de treino. Ou seja, o modelo foi capaz de generalizar o conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf40e2f-8541-486b-a896-0838a573d53e",
   "metadata": {},
   "source": [
    "# Exportação do modelo\n",
    "\n",
    "A exportação do modelo contará com as seguintes etapas:\n",
    "- Salvar o modelo treinado\n",
    "- Salvar o vetorizador (transformador de 'texto' em palavras)\n",
    "- Salvar o encoder (tradução das labels da variável predita `sentiment` em categorias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f61d191-4525-4c99-81e7-5536b74e6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = 'modelo.pth'\n",
    "torch.save(model_novo, model_filepath)\n",
    "tfidf_filepath = 'tfidf.pickle'\n",
    "torch.save(tfidf_vectorizer, tfidf_filepath)\n",
    "encoder_filepath = 'encoder.pickle'\n",
    "torch.save(le, encoder_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e253aa-ebe7-43c0-9442-f4d519c8624b",
   "metadata": {},
   "source": [
    "# Importação do modelo e utilização em um ambiente simulado de produção"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874204e-36b3-4533-b692-1cc5ab004381",
   "metadata": {},
   "source": [
    "Após exportar o modelo, utilizaremos o mesmo num ambiente de produção simulado.\n",
    "Para isto, foi criada a classe ``ModeloProducao``, que possui uma função ``predict`` que recebe o texto do _review_ a ser classificado e retorna uma descrição amigável da classificação ([positivo|negativo]) e a probabilidade da mesma.\n",
    "\n",
    "**IMPORTANTE**: PARA QUE A IMPORTAÇÃO DO MODELO FUNCIONE CORRETAMENTE, ALÉM DO PRÓPRIO MODELO SALVO (EM FORMATO.pth) É NECESSÁRIA A EXECUÇÃO DA CÉLULA QUE DEFINE A CLASSE ``LSTM``, QUE É A BASE DO MODELO. OU SEJA, CASO DESEJA-SE UTILIZAR ESTE MODELO NUM AMBIENTE 'NOVO', DEVE-SE EXPORTAR E EXECUTAR A CÉLULA QUE CONTÉM A DEFINIÇÃO D CLASSE ``LSTM``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91d319e-3d26-445c-893a-e4b3f423659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeloProducao():\n",
    "    \n",
    "    def __init__(self, model_path, tfidf_path, encoder_filepath):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model.cpu()\n",
    "        self.hidden = self.model.init_hidden(1, 'cpu')\n",
    "        self.hidden = tuple([each.data.to('cpu') for each in self.hidden])\n",
    "        \n",
    "        self.tfidf_vectorizer = torch.load(tfidf_path)\n",
    "        \n",
    "        self.label_encoder = torch.load(encoder_filepath)\n",
    "    \n",
    "    def predict(self, input_text: str):\n",
    "        self.model.eval()\n",
    "        tf_input = self.tfidf_vectorizer.transform(np.array([input_text])).toarray()\n",
    "        ts_input = torch.tensor(tf_input).to('cpu')\n",
    "        with torch.no_grad():\n",
    "            output, h = self.model(ts_input, self.hidden)\n",
    "            return self.decode_label(output)\n",
    "        \n",
    "    def decode_label(self, input):\n",
    "        out = self.label_encoder.inverse_transform(\n",
    "           [np.array(torch.round(input.squeeze())).astype(int)] \n",
    "        )[0]\n",
    "        \n",
    "        pretty_desc = 'positivo'\n",
    "        proba = input[0] * 100\n",
    "        if out == 'neg':\n",
    "            proba = (1 - input[0]) * 100\n",
    "            pretty_desc = 'negativo'\n",
    "            \n",
    "        return pretty_desc, proba\n",
    "\n",
    "modelo_producao = ModeloProducao(model_filepath, tfidf_filepath, encoder_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ec3f41e-0652-47ef-b161-8c03014c2417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Há 85.75% de probabilidade do review acima ser positivo\n"
     ]
    }
   ],
   "source": [
    "good_review = \"It's the best movie I've ever seen!\"\n",
    "review_classification, proba = modelo_producao.predict(good_review)\n",
    "print(f'Há {proba:.2f}% de probabilidade do review acima ser {review_classification}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4c6affa-92d8-43fa-af67-6ea34b8b4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Há 85.71% de probabilidade do review acima ser negativo\n"
     ]
    }
   ],
   "source": [
    "bad_review = \"It's the worst movie I've ever seen!\"\n",
    "review_classification, proba = modelo_producao.predict(bad_review)\n",
    "print(f'Há {proba:.2f}% de probabilidade do review acima ser {review_classification}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
