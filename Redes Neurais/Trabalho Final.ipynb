{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a819eb6",
   "metadata": {},
   "source": [
    "# Guilherme Cristiano Goll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59d467",
   "metadata": {},
   "source": [
    "# Informações do trabalho:\n",
    "A base de dados que será utilizada nesse trabalho contém informações geológicas com etapas de pré-processamento já executadas.\n",
    "\n",
    "Foi criada por um time de especialistas em litologia e contém informações de sensores e 11 classes a granulométricas na coluna “FORCE_2020_LITHOFACIES_LITHOLOGY”:\n",
    "- 30000: Sandstone\n",
    "- 65030: Sandstone/Shale\n",
    "- 65000: Shale\n",
    "- 80000: Marl\n",
    "- 74000: Dolomite\n",
    "- 70000: Limestone\n",
    "- 70032: Chalk\n",
    "- 88000: Halite\n",
    "- 86000: Anhydrite\n",
    "- 99000: Tuff\n",
    "- 90000: Coal\n",
    "- 93000: Basement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad519da",
   "metadata": {},
   "source": [
    "# Carregamento do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a370e",
   "metadata": {},
   "source": [
    "Importação das bibliotecas que serão utilizadas neste experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516826b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522079b",
   "metadata": {},
   "source": [
    "Configuração de exibição das colunas do dataframe, e utilização de um fator fixo (_seed_)para permitir a reprodução dos experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b83517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29eb20cf490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 90)\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d54dc",
   "metadata": {},
   "source": [
    "Carregamento do dataframe completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78580770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('lithology.csv', sep=';' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63a2ea",
   "metadata": {},
   "source": [
    "Informações gerais sobre o conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe03249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1170511 entries, 0 to 1170510\n",
      "Data columns (total 31 columns):\n",
      " #   Column                            Non-Null Count    Dtype  \n",
      "---  ------                            --------------    -----  \n",
      " 0   DEPTH_MD                          1170511 non-null  float64\n",
      " 1   X_LOC                             1170511 non-null  float64\n",
      " 2   Y_LOC                             1170511 non-null  float64\n",
      " 3   Z_LOC                             1170511 non-null  float64\n",
      " 4   CALI                              1170511 non-null  float64\n",
      " 5   RSHA                              1170511 non-null  float64\n",
      " 6   RMED                              1170511 non-null  float64\n",
      " 7   RDEP                              1170511 non-null  float64\n",
      " 8   RHOB                              1170511 non-null  float64\n",
      " 9   GR                                1170511 non-null  float64\n",
      " 10  NPHI                              1170511 non-null  float64\n",
      " 11  PEF                               1170511 non-null  float64\n",
      " 12  DTC                               1170511 non-null  float64\n",
      " 13  SP                                1170511 non-null  float64\n",
      " 14  BS                                1170511 non-null  float64\n",
      " 15  ROP                               1170511 non-null  float64\n",
      " 16  DCAL                              1170511 non-null  float64\n",
      " 17  DRHO                              1170511 non-null  float64\n",
      " 18  MUDWEIGHT                         1170511 non-null  float64\n",
      " 19  RMIC                              1170511 non-null  float64\n",
      " 20  FORCE_2020_LITHOFACIES_LITHOLOGY  1170511 non-null  float64\n",
      " 21  Carbon_Index                      1170511 non-null  float64\n",
      " 22  Normalized_RHOB                   1170511 non-null  float64\n",
      " 23  Normalized_GR                     1170511 non-null  float64\n",
      " 24  Delta_DTC                         1170511 non-null  float64\n",
      " 25  Delta_RHOB                        1170511 non-null  float64\n",
      " 26  Delta_GR                          1170511 non-null  float64\n",
      " 27  Delta_DEPTH_MD                    1170511 non-null  float64\n",
      " 28  Delta_Carbon_Index                1170511 non-null  float64\n",
      " 29  GROUP_encoded                     1170511 non-null  int64  \n",
      " 30  FORMATION_encoded                 1170511 non-null  int64  \n",
      "dtypes: float64(29), int64(2)\n",
      "memory usage: 276.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3862d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.FORCE_2020_LITHOFACIES_LITHOLOGY = df_full.FORCE_2020_LITHOFACIES_LITHOLOGY.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394ce759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>PEF</th>\n",
       "      <th>DTC</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>Carbon_Index</th>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <th>Normalized_GR</th>\n",
       "      <th>Delta_DTC</th>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <th>Delta_GR</th>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.081904</td>\n",
       "      <td>0.249745</td>\n",
       "      <td>-0.987890</td>\n",
       "      <td>0.242287</td>\n",
       "      <td>0.072961</td>\n",
       "      <td>0.055864</td>\n",
       "      <td>0.062862</td>\n",
       "      <td>0.267399</td>\n",
       "      <td>0.098282</td>\n",
       "      <td>0.414375</td>\n",
       "      <td>0.201044</td>\n",
       "      <td>0.086242</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.618097</td>\n",
       "      <td>-0.031847</td>\n",
       "      <td>0.253296</td>\n",
       "      <td>0.249273</td>\n",
       "      <td>-0.182790</td>\n",
       "      <td>0.146459</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.267160</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.215984</td>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>-0.293998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_LOC</th>\n",
       "      <td>-0.081904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416875</td>\n",
       "      <td>0.085877</td>\n",
       "      <td>-0.249819</td>\n",
       "      <td>-0.164608</td>\n",
       "      <td>-0.044303</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.098533</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>0.089433</td>\n",
       "      <td>-0.035485</td>\n",
       "      <td>-0.076769</td>\n",
       "      <td>-0.121053</td>\n",
       "      <td>-0.092630</td>\n",
       "      <td>-0.078123</td>\n",
       "      <td>-0.215753</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>-0.333802</td>\n",
       "      <td>0.223622</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.167773</td>\n",
       "      <td>-0.071248</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>-0.225151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y_LOC</th>\n",
       "      <td>0.249745</td>\n",
       "      <td>0.416875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.259623</td>\n",
       "      <td>-0.148181</td>\n",
       "      <td>-0.175650</td>\n",
       "      <td>0.109941</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.106056</td>\n",
       "      <td>0.206453</td>\n",
       "      <td>0.068874</td>\n",
       "      <td>0.071257</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>-0.076501</td>\n",
       "      <td>-0.167017</td>\n",
       "      <td>-0.274776</td>\n",
       "      <td>-0.143624</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>-0.738548</td>\n",
       "      <td>0.059190</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>0.102346</td>\n",
       "      <td>0.105965</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>-0.163860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_LOC</th>\n",
       "      <td>-0.987890</td>\n",
       "      <td>0.085877</td>\n",
       "      <td>-0.259623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.234391</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>-0.087831</td>\n",
       "      <td>-0.111010</td>\n",
       "      <td>-0.277483</td>\n",
       "      <td>-0.101529</td>\n",
       "      <td>-0.411152</td>\n",
       "      <td>-0.197759</td>\n",
       "      <td>-0.081243</td>\n",
       "      <td>0.174611</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>-0.245064</td>\n",
       "      <td>-0.256565</td>\n",
       "      <td>0.193531</td>\n",
       "      <td>-0.150293</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>-0.262971</td>\n",
       "      <td>-0.277247</td>\n",
       "      <td>-0.043703</td>\n",
       "      <td>-0.208389</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.291771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALI</th>\n",
       "      <td>0.242287</td>\n",
       "      <td>-0.249819</td>\n",
       "      <td>-0.148181</td>\n",
       "      <td>-0.234391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230859</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>-0.131479</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.253697</td>\n",
       "      <td>0.336849</td>\n",
       "      <td>0.117929</td>\n",
       "      <td>-0.093683</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>0.281891</td>\n",
       "      <td>0.172453</td>\n",
       "      <td>0.095982</td>\n",
       "      <td>-0.038173</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.290592</td>\n",
       "      <td>-0.133204</td>\n",
       "      <td>0.394364</td>\n",
       "      <td>0.290467</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.290466</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>0.017484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSHA</th>\n",
       "      <td>0.072961</td>\n",
       "      <td>-0.164608</td>\n",
       "      <td>-0.175650</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>0.230859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.113860</td>\n",
       "      <td>-0.043932</td>\n",
       "      <td>0.108301</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.068859</td>\n",
       "      <td>0.262062</td>\n",
       "      <td>-0.077242</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.211634</td>\n",
       "      <td>0.083668</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>0.166244</td>\n",
       "      <td>-0.041135</td>\n",
       "      <td>0.112537</td>\n",
       "      <td>0.113915</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.113818</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>0.113820</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMED</th>\n",
       "      <td>0.055864</td>\n",
       "      <td>-0.044303</td>\n",
       "      <td>0.109941</td>\n",
       "      <td>-0.087831</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>0.204210</td>\n",
       "      <td>0.117150</td>\n",
       "      <td>0.087206</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.354425</td>\n",
       "      <td>0.248956</td>\n",
       "      <td>-0.084409</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.178310</td>\n",
       "      <td>0.172003</td>\n",
       "      <td>-0.063313</td>\n",
       "      <td>0.075098</td>\n",
       "      <td>-0.083413</td>\n",
       "      <td>0.204782</td>\n",
       "      <td>0.204219</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>0.350722</td>\n",
       "      <td>0.204328</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.204323</td>\n",
       "      <td>-0.031007</td>\n",
       "      <td>-0.022347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RDEP</th>\n",
       "      <td>0.062862</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>-0.111010</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>-0.049338</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.022012</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.090665</td>\n",
       "      <td>0.114003</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.114004</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>-0.006239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHOB</th>\n",
       "      <td>0.267399</td>\n",
       "      <td>0.098533</td>\n",
       "      <td>0.106056</td>\n",
       "      <td>-0.277483</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>0.113860</td>\n",
       "      <td>0.204210</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>0.474023</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>0.424886</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>-0.336723</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.291568</td>\n",
       "      <td>0.882447</td>\n",
       "      <td>-0.062072</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>-0.023634</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062674</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>-0.127749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GR</th>\n",
       "      <td>0.098282</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>0.206453</td>\n",
       "      <td>-0.101529</td>\n",
       "      <td>-0.131479</td>\n",
       "      <td>-0.043932</td>\n",
       "      <td>0.117150</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169099</td>\n",
       "      <td>0.049547</td>\n",
       "      <td>-0.077324</td>\n",
       "      <td>-0.161919</td>\n",
       "      <td>-0.182325</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>-0.093328</td>\n",
       "      <td>-0.026357</td>\n",
       "      <td>-0.119144</td>\n",
       "      <td>0.153378</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>-0.006083</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>-0.002677</td>\n",
       "      <td>-0.051292</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.002682</td>\n",
       "      <td>0.062451</td>\n",
       "      <td>-0.172180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPHI</th>\n",
       "      <td>0.414375</td>\n",
       "      <td>0.089433</td>\n",
       "      <td>0.068874</td>\n",
       "      <td>-0.411152</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.108301</td>\n",
       "      <td>0.087206</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.474023</td>\n",
       "      <td>0.169099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091392</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>-0.221162</td>\n",
       "      <td>-0.528386</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>0.182312</td>\n",
       "      <td>0.420416</td>\n",
       "      <td>-0.025233</td>\n",
       "      <td>0.127524</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>0.467362</td>\n",
       "      <td>0.473941</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>0.250326</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.473602</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>-0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEF</th>\n",
       "      <td>0.201044</td>\n",
       "      <td>-0.035485</td>\n",
       "      <td>0.071257</td>\n",
       "      <td>-0.197759</td>\n",
       "      <td>0.253697</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>0.049547</td>\n",
       "      <td>0.091392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196911</td>\n",
       "      <td>-0.067748</td>\n",
       "      <td>-0.107047</td>\n",
       "      <td>-0.019449</td>\n",
       "      <td>0.246924</td>\n",
       "      <td>0.473065</td>\n",
       "      <td>-0.017896</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>-0.057656</td>\n",
       "      <td>0.457744</td>\n",
       "      <td>0.458447</td>\n",
       "      <td>-0.063188</td>\n",
       "      <td>0.213564</td>\n",
       "      <td>0.458318</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.458312</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>-0.007112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTC</th>\n",
       "      <td>0.086242</td>\n",
       "      <td>-0.076769</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>-0.081243</td>\n",
       "      <td>0.336849</td>\n",
       "      <td>0.068859</td>\n",
       "      <td>0.354425</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>0.424886</td>\n",
       "      <td>-0.077324</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.196911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200751</td>\n",
       "      <td>-0.099723</td>\n",
       "      <td>-0.038438</td>\n",
       "      <td>0.298947</td>\n",
       "      <td>0.378234</td>\n",
       "      <td>0.009189</td>\n",
       "      <td>-0.027449</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.428413</td>\n",
       "      <td>0.424933</td>\n",
       "      <td>-0.019223</td>\n",
       "      <td>0.973028</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.424818</td>\n",
       "      <td>0.028162</td>\n",
       "      <td>-0.028480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.121053</td>\n",
       "      <td>-0.076501</td>\n",
       "      <td>0.174611</td>\n",
       "      <td>0.117929</td>\n",
       "      <td>0.262062</td>\n",
       "      <td>0.248956</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>-0.161919</td>\n",
       "      <td>-0.221162</td>\n",
       "      <td>-0.067748</td>\n",
       "      <td>0.200751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101639</td>\n",
       "      <td>-0.123584</td>\n",
       "      <td>0.080626</td>\n",
       "      <td>0.133060</td>\n",
       "      <td>0.143451</td>\n",
       "      <td>-0.135047</td>\n",
       "      <td>-0.044260</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>-0.089115</td>\n",
       "      <td>0.170855</td>\n",
       "      <td>0.074868</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.074867</td>\n",
       "      <td>-0.072891</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BS</th>\n",
       "      <td>-0.618097</td>\n",
       "      <td>-0.092630</td>\n",
       "      <td>-0.167017</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>-0.093683</td>\n",
       "      <td>-0.077242</td>\n",
       "      <td>-0.084409</td>\n",
       "      <td>-0.049338</td>\n",
       "      <td>-0.336723</td>\n",
       "      <td>-0.182325</td>\n",
       "      <td>-0.528386</td>\n",
       "      <td>-0.107047</td>\n",
       "      <td>-0.099723</td>\n",
       "      <td>0.101639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073968</td>\n",
       "      <td>-0.112606</td>\n",
       "      <td>-0.257413</td>\n",
       "      <td>0.109289</td>\n",
       "      <td>-0.115779</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.086266</td>\n",
       "      <td>-0.194345</td>\n",
       "      <td>-0.336376</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>-0.336375</td>\n",
       "      <td>-0.019857</td>\n",
       "      <td>0.319115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROP</th>\n",
       "      <td>-0.031847</td>\n",
       "      <td>-0.078123</td>\n",
       "      <td>-0.274776</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>-0.019449</td>\n",
       "      <td>-0.038438</td>\n",
       "      <td>-0.123584</td>\n",
       "      <td>0.073968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>-0.043523</td>\n",
       "      <td>0.345665</td>\n",
       "      <td>0.105159</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>-0.058482</td>\n",
       "      <td>-0.058498</td>\n",
       "      <td>-0.025989</td>\n",
       "      <td>-0.037838</td>\n",
       "      <td>-0.058467</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>-0.058465</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.037182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DCAL</th>\n",
       "      <td>0.253296</td>\n",
       "      <td>-0.215753</td>\n",
       "      <td>-0.143624</td>\n",
       "      <td>-0.245064</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>0.211634</td>\n",
       "      <td>0.178310</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.291568</td>\n",
       "      <td>-0.093328</td>\n",
       "      <td>0.182312</td>\n",
       "      <td>0.246924</td>\n",
       "      <td>0.298947</td>\n",
       "      <td>0.080626</td>\n",
       "      <td>-0.112606</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.278694</td>\n",
       "      <td>0.163768</td>\n",
       "      <td>0.109184</td>\n",
       "      <td>-0.029420</td>\n",
       "      <td>0.287960</td>\n",
       "      <td>0.291523</td>\n",
       "      <td>-0.099243</td>\n",
       "      <td>0.358258</td>\n",
       "      <td>0.291377</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.291375</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>-0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRHO</th>\n",
       "      <td>0.249273</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>-0.256565</td>\n",
       "      <td>0.281891</td>\n",
       "      <td>0.083668</td>\n",
       "      <td>0.172003</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.882447</td>\n",
       "      <td>-0.026357</td>\n",
       "      <td>0.420416</td>\n",
       "      <td>0.473065</td>\n",
       "      <td>0.378234</td>\n",
       "      <td>0.133060</td>\n",
       "      <td>-0.257413</td>\n",
       "      <td>-0.043523</td>\n",
       "      <td>0.278694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074903</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.882056</td>\n",
       "      <td>0.882461</td>\n",
       "      <td>-0.097750</td>\n",
       "      <td>0.414080</td>\n",
       "      <td>0.882169</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.882166</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>-0.116644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <td>-0.182790</td>\n",
       "      <td>-0.333802</td>\n",
       "      <td>-0.738548</td>\n",
       "      <td>0.193531</td>\n",
       "      <td>0.172453</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>-0.063313</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>-0.062072</td>\n",
       "      <td>-0.119144</td>\n",
       "      <td>-0.025233</td>\n",
       "      <td>-0.017896</td>\n",
       "      <td>0.009189</td>\n",
       "      <td>0.143451</td>\n",
       "      <td>0.109289</td>\n",
       "      <td>0.345665</td>\n",
       "      <td>0.163768</td>\n",
       "      <td>-0.074903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037929</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>-0.058653</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>-0.087257</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>0.125224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMIC</th>\n",
       "      <td>0.146459</td>\n",
       "      <td>0.223622</td>\n",
       "      <td>0.059190</td>\n",
       "      <td>-0.150293</td>\n",
       "      <td>0.095982</td>\n",
       "      <td>0.166244</td>\n",
       "      <td>0.075098</td>\n",
       "      <td>0.022012</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>0.153378</td>\n",
       "      <td>0.127524</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>-0.027449</td>\n",
       "      <td>-0.135047</td>\n",
       "      <td>-0.115779</td>\n",
       "      <td>0.105159</td>\n",
       "      <td>0.109184</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>-0.037929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014886</td>\n",
       "      <td>0.080279</td>\n",
       "      <td>0.084487</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>0.072768</td>\n",
       "      <td>-0.101801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <td>-0.007422</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>-0.038173</td>\n",
       "      <td>-0.041135</td>\n",
       "      <td>-0.083413</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>-0.023634</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>-0.057656</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>-0.044260</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>-0.029420</td>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>-0.014886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025120</td>\n",
       "      <td>-0.023649</td>\n",
       "      <td>0.156830</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>-0.023719</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>-0.023732</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>-0.100015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon_Index</th>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.102346</td>\n",
       "      <td>-0.262971</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.112537</td>\n",
       "      <td>0.204782</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>-0.006083</td>\n",
       "      <td>0.467362</td>\n",
       "      <td>0.457744</td>\n",
       "      <td>0.428413</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.058482</td>\n",
       "      <td>0.287960</td>\n",
       "      <td>0.882056</td>\n",
       "      <td>-0.058653</td>\n",
       "      <td>0.080279</td>\n",
       "      <td>-0.025120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>-0.064837</td>\n",
       "      <td>0.462292</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>-0.122091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <td>0.267160</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.105965</td>\n",
       "      <td>-0.277247</td>\n",
       "      <td>0.290592</td>\n",
       "      <td>0.113915</td>\n",
       "      <td>0.204219</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>0.473941</td>\n",
       "      <td>0.458447</td>\n",
       "      <td>0.424933</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.058498</td>\n",
       "      <td>0.291523</td>\n",
       "      <td>0.882461</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>0.084487</td>\n",
       "      <td>-0.023649</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062715</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>-0.127675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized_GR</th>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.167773</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>-0.043703</td>\n",
       "      <td>-0.133204</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>-0.062674</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>-0.063188</td>\n",
       "      <td>-0.019223</td>\n",
       "      <td>-0.089115</td>\n",
       "      <td>-0.086266</td>\n",
       "      <td>-0.025989</td>\n",
       "      <td>-0.099243</td>\n",
       "      <td>-0.097750</td>\n",
       "      <td>-0.087257</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.156830</td>\n",
       "      <td>-0.064837</td>\n",
       "      <td>-0.062715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>-0.062719</td>\n",
       "      <td>-0.039971</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.062720</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>-0.183059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_DTC</th>\n",
       "      <td>0.215984</td>\n",
       "      <td>-0.071248</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>-0.208389</td>\n",
       "      <td>0.394364</td>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.350722</td>\n",
       "      <td>0.090665</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>0.250326</td>\n",
       "      <td>0.213564</td>\n",
       "      <td>0.973028</td>\n",
       "      <td>0.170855</td>\n",
       "      <td>-0.194345</td>\n",
       "      <td>-0.037838</td>\n",
       "      <td>0.358258</td>\n",
       "      <td>0.414080</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.462292</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462362</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.462344</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>-0.073349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>0.290467</td>\n",
       "      <td>0.113818</td>\n",
       "      <td>0.204328</td>\n",
       "      <td>0.114003</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>-0.002677</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.458318</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>0.074868</td>\n",
       "      <td>-0.336376</td>\n",
       "      <td>-0.058467</td>\n",
       "      <td>0.291377</td>\n",
       "      <td>0.882169</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>-0.023719</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>-0.062719</td>\n",
       "      <td>0.462362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>-0.127563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_GR</th>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.051292</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.039971</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>0.290466</td>\n",
       "      <td>0.113820</td>\n",
       "      <td>0.204323</td>\n",
       "      <td>0.114004</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>-0.002682</td>\n",
       "      <td>0.473602</td>\n",
       "      <td>0.458312</td>\n",
       "      <td>0.424818</td>\n",
       "      <td>0.074867</td>\n",
       "      <td>-0.336375</td>\n",
       "      <td>-0.058465</td>\n",
       "      <td>0.291375</td>\n",
       "      <td>0.882166</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>-0.023732</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>-0.062720</td>\n",
       "      <td>0.462344</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>-0.127561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>-0.031007</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.062451</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>0.028162</td>\n",
       "      <td>-0.072891</td>\n",
       "      <td>-0.019857</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>0.072768</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FORMATION_encoded</th>\n",
       "      <td>-0.293998</td>\n",
       "      <td>-0.225151</td>\n",
       "      <td>-0.163860</td>\n",
       "      <td>0.291771</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>-0.022347</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>-0.127749</td>\n",
       "      <td>-0.172180</td>\n",
       "      <td>-0.225500</td>\n",
       "      <td>-0.007112</td>\n",
       "      <td>-0.028480</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.319115</td>\n",
       "      <td>0.037182</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.116644</td>\n",
       "      <td>0.125224</td>\n",
       "      <td>-0.101801</td>\n",
       "      <td>-0.100015</td>\n",
       "      <td>-0.122091</td>\n",
       "      <td>-0.127675</td>\n",
       "      <td>-0.183059</td>\n",
       "      <td>-0.073349</td>\n",
       "      <td>-0.127563</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>-0.000826</td>\n",
       "      <td>-0.127561</td>\n",
       "      <td>-0.025375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DEPTH_MD     X_LOC     Y_LOC     Z_LOC  \\\n",
       "DEPTH_MD                          1.000000 -0.081904  0.249745 -0.987890   \n",
       "X_LOC                            -0.081904  1.000000  0.416875  0.085877   \n",
       "Y_LOC                             0.249745  0.416875  1.000000 -0.259623   \n",
       "Z_LOC                            -0.987890  0.085877 -0.259623  1.000000   \n",
       "CALI                              0.242287 -0.249819 -0.148181 -0.234391   \n",
       "RSHA                              0.072961 -0.164608 -0.175650 -0.088610   \n",
       "RMED                              0.055864 -0.044303  0.109941 -0.087831   \n",
       "RDEP                              0.062862  0.054780  0.009231 -0.111010   \n",
       "RHOB                              0.267399  0.098533  0.106056 -0.277483   \n",
       "GR                                0.098282  0.231190  0.206453 -0.101529   \n",
       "NPHI                              0.414375  0.089433  0.068874 -0.411152   \n",
       "PEF                               0.201044 -0.035485  0.071257 -0.197759   \n",
       "DTC                               0.086242 -0.076769  0.030885 -0.081243   \n",
       "SP                               -0.212204 -0.121053 -0.076501  0.174611   \n",
       "BS                               -0.618097 -0.092630 -0.167017  0.615375   \n",
       "ROP                              -0.031847 -0.078123 -0.274776  0.038685   \n",
       "DCAL                              0.253296 -0.215753 -0.143624 -0.245064   \n",
       "DRHO                              0.249273  0.055254  0.090255 -0.256565   \n",
       "MUDWEIGHT                        -0.182790 -0.333802 -0.738548  0.193531   \n",
       "RMIC                              0.146459  0.223622  0.059190 -0.150293   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.007422 -0.019755 -0.034079  0.019270   \n",
       "Carbon_Index                      0.252801  0.096850  0.102346 -0.262971   \n",
       "Normalized_RHOB                   0.267160  0.098485  0.105965 -0.277247   \n",
       "Normalized_GR                     0.044830  0.167773  0.131760 -0.043703   \n",
       "Delta_DTC                         0.215984 -0.071248  0.047832 -0.208389   \n",
       "Delta_RHOB                        0.266868  0.098435  0.105851 -0.276951   \n",
       "Delta_GR                          0.000432 -0.000287 -0.000184 -0.000414   \n",
       "Delta_DEPTH_MD                    0.002445  0.000974  0.002194 -0.002509   \n",
       "Delta_Carbon_Index                0.266868  0.098434  0.105851 -0.276951   \n",
       "GROUP_encoded                     0.056807  0.094923  0.024585 -0.044176   \n",
       "FORMATION_encoded                -0.293998 -0.225151 -0.163860  0.291771   \n",
       "\n",
       "                                      CALI      RSHA      RMED      RDEP  \\\n",
       "DEPTH_MD                          0.242287  0.072961  0.055864  0.062862   \n",
       "X_LOC                            -0.249819 -0.164608 -0.044303  0.054780   \n",
       "Y_LOC                            -0.148181 -0.175650  0.109941  0.009231   \n",
       "Z_LOC                            -0.234391 -0.088610 -0.087831 -0.111010   \n",
       "CALI                              1.000000  0.230859  0.200187  0.059856   \n",
       "RSHA                              0.230859  1.000000  0.038447  0.041122   \n",
       "RMED                              0.200187  0.038447  1.000000  0.093780   \n",
       "RDEP                              0.059856  0.041122  0.093780  1.000000   \n",
       "RHOB                              0.290623  0.113860  0.204210  0.113988   \n",
       "GR                               -0.131479 -0.043932  0.117150  0.009550   \n",
       "NPHI                              0.159466  0.108301  0.087206  0.086625   \n",
       "PEF                               0.253697 -0.048833  0.196880  0.014515   \n",
       "DTC                               0.336849  0.068859  0.354425  0.076234   \n",
       "SP                                0.117929  0.262062  0.248956  0.023633   \n",
       "BS                               -0.093683 -0.077242 -0.084409 -0.049338   \n",
       "ROP                               0.061482 -0.000514 -0.014198  0.004839   \n",
       "DCAL                              0.911076  0.211634  0.178310  0.055512   \n",
       "DRHO                              0.281891  0.083668  0.172003  0.106038   \n",
       "MUDWEIGHT                         0.172453  0.202040 -0.063313  0.003755   \n",
       "RMIC                              0.095982  0.166244  0.075098  0.022012   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.038173 -0.041135 -0.083413  0.009211   \n",
       "Carbon_Index                      0.288104  0.112537  0.204782  0.114447   \n",
       "Normalized_RHOB                   0.290592  0.113915  0.204219  0.113994   \n",
       "Normalized_GR                    -0.133204 -0.063521  0.087406  0.003944   \n",
       "Delta_DTC                         0.394364  0.088760  0.350722  0.090665   \n",
       "Delta_RHOB                        0.290467  0.113818  0.204328  0.114003   \n",
       "Delta_GR                          0.000167 -0.000398  0.001224  0.000662   \n",
       "Delta_DEPTH_MD                    0.000338 -0.001663  0.000175  0.000325   \n",
       "Delta_Carbon_Index                0.290466  0.113820  0.204323  0.114004   \n",
       "GROUP_encoded                     0.055227 -0.011004 -0.031007  0.045048   \n",
       "FORMATION_encoded                 0.017484  0.001706 -0.022347 -0.006239   \n",
       "\n",
       "                                      RHOB        GR      NPHI       PEF  \\\n",
       "DEPTH_MD                          0.267399  0.098282  0.414375  0.201044   \n",
       "X_LOC                             0.098533  0.231190  0.089433 -0.035485   \n",
       "Y_LOC                             0.106056  0.206453  0.068874  0.071257   \n",
       "Z_LOC                            -0.277483 -0.101529 -0.411152 -0.197759   \n",
       "CALI                              0.290623 -0.131479  0.159466  0.253697   \n",
       "RSHA                              0.113860 -0.043932  0.108301 -0.048833   \n",
       "RMED                              0.204210  0.117150  0.087206  0.196880   \n",
       "RDEP                              0.113988  0.009550  0.086625  0.014515   \n",
       "RHOB                              1.000000 -0.002582  0.474023  0.458521   \n",
       "GR                               -0.002582  1.000000  0.169099  0.049547   \n",
       "NPHI                              0.474023  0.169099  1.000000  0.091392   \n",
       "PEF                               0.458521  0.049547  0.091392  1.000000   \n",
       "DTC                               0.424886 -0.077324  0.176056  0.196911   \n",
       "SP                                0.074638 -0.161919 -0.221162 -0.067748   \n",
       "BS                               -0.336723 -0.182325 -0.528386 -0.107047   \n",
       "ROP                              -0.058474  0.034934  0.025787 -0.019449   \n",
       "DCAL                              0.291568 -0.093328  0.182312  0.246924   \n",
       "DRHO                              0.882447 -0.026357  0.420416  0.473065   \n",
       "MUDWEIGHT                        -0.062072 -0.119144 -0.025233 -0.017896   \n",
       "RMIC                              0.084565  0.153378  0.127524  0.023661   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.023634  0.163939 -0.027391 -0.057656   \n",
       "Carbon_Index                      0.999727 -0.006083  0.467362  0.457744   \n",
       "Normalized_RHOB                   1.000000 -0.002662  0.473941  0.458447   \n",
       "Normalized_GR                    -0.062674  0.583831  0.091346 -0.063188   \n",
       "Delta_DTC                         0.462418 -0.074397  0.250326  0.213564   \n",
       "Delta_RHOB                        0.999583 -0.002677  0.473603  0.458318   \n",
       "Delta_GR                          0.000408 -0.051292  0.000093  0.000100   \n",
       "Delta_DEPTH_MD                    0.000908 -0.000304  0.001268  0.002135   \n",
       "Delta_Carbon_Index                0.999579 -0.002682  0.473602  0.458312   \n",
       "GROUP_encoded                     0.022361  0.062451  0.063828  0.024317   \n",
       "FORMATION_encoded                -0.127749 -0.172180 -0.225500 -0.007112   \n",
       "\n",
       "                                       DTC        SP        BS       ROP  \\\n",
       "DEPTH_MD                          0.086242 -0.212204 -0.618097 -0.031847   \n",
       "X_LOC                            -0.076769 -0.121053 -0.092630 -0.078123   \n",
       "Y_LOC                             0.030885 -0.076501 -0.167017 -0.274776   \n",
       "Z_LOC                            -0.081243  0.174611  0.615375  0.038685   \n",
       "CALI                              0.336849  0.117929 -0.093683  0.061482   \n",
       "RSHA                              0.068859  0.262062 -0.077242 -0.000514   \n",
       "RMED                              0.354425  0.248956 -0.084409 -0.014198   \n",
       "RDEP                              0.076234  0.023633 -0.049338  0.004839   \n",
       "RHOB                              0.424886  0.074638 -0.336723 -0.058474   \n",
       "GR                               -0.077324 -0.161919 -0.182325  0.034934   \n",
       "NPHI                              0.176056 -0.221162 -0.528386  0.025787   \n",
       "PEF                               0.196911 -0.067748 -0.107047 -0.019449   \n",
       "DTC                               1.000000  0.200751 -0.099723 -0.038438   \n",
       "SP                                0.200751  1.000000  0.101639 -0.123584   \n",
       "BS                               -0.099723  0.101639  1.000000  0.073968   \n",
       "ROP                              -0.038438 -0.123584  0.073968  1.000000   \n",
       "DCAL                              0.298947  0.080626 -0.112606  0.067100   \n",
       "DRHO                              0.378234  0.133060 -0.257413 -0.043523   \n",
       "MUDWEIGHT                         0.009189  0.143451  0.109289  0.345665   \n",
       "RMIC                             -0.027449 -0.135047 -0.115779  0.105159   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY  0.008990 -0.044260  0.031248  0.014388   \n",
       "Carbon_Index                      0.428413  0.079151 -0.327054 -0.058482   \n",
       "Normalized_RHOB                   0.424933  0.074799 -0.336593 -0.058498   \n",
       "Normalized_GR                    -0.019223 -0.089115 -0.086266 -0.025989   \n",
       "Delta_DTC                         0.973028  0.170855 -0.194345 -0.037838   \n",
       "Delta_RHOB                        0.424812  0.074868 -0.336376 -0.058467   \n",
       "Delta_GR                         -0.000272 -0.000084 -0.000202 -0.000208   \n",
       "Delta_DEPTH_MD                    0.000017 -0.002900  0.001167  0.000544   \n",
       "Delta_Carbon_Index                0.424818  0.074867 -0.336375 -0.058465   \n",
       "GROUP_encoded                     0.028162 -0.072891 -0.019857  0.017402   \n",
       "FORMATION_encoded                -0.028480  0.065500  0.319115  0.037182   \n",
       "\n",
       "                                      DCAL      DRHO  MUDWEIGHT      RMIC  \\\n",
       "DEPTH_MD                          0.253296  0.249273  -0.182790  0.146459   \n",
       "X_LOC                            -0.215753  0.055254  -0.333802  0.223622   \n",
       "Y_LOC                            -0.143624  0.090255  -0.738548  0.059190   \n",
       "Z_LOC                            -0.245064 -0.256565   0.193531 -0.150293   \n",
       "CALI                              0.911076  0.281891   0.172453  0.095982   \n",
       "RSHA                              0.211634  0.083668   0.202040  0.166244   \n",
       "RMED                              0.178310  0.172003  -0.063313  0.075098   \n",
       "RDEP                              0.055512  0.106038   0.003755  0.022012   \n",
       "RHOB                              0.291568  0.882447  -0.062072  0.084565   \n",
       "GR                               -0.093328 -0.026357  -0.119144  0.153378   \n",
       "NPHI                              0.182312  0.420416  -0.025233  0.127524   \n",
       "PEF                               0.246924  0.473065  -0.017896  0.023661   \n",
       "DTC                               0.298947  0.378234   0.009189 -0.027449   \n",
       "SP                                0.080626  0.133060   0.143451 -0.135047   \n",
       "BS                               -0.112606 -0.257413   0.109289 -0.115779   \n",
       "ROP                               0.067100 -0.043523   0.345665  0.105159   \n",
       "DCAL                              1.000000  0.278694   0.163768  0.109184   \n",
       "DRHO                              0.278694  1.000000  -0.074903  0.012851   \n",
       "MUDWEIGHT                         0.163768 -0.074903   1.000000 -0.037929   \n",
       "RMIC                              0.109184  0.012851  -0.037929  1.000000   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.029420 -0.027083   0.027054 -0.014886   \n",
       "Carbon_Index                      0.287960  0.882056  -0.058653  0.080279   \n",
       "Normalized_RHOB                   0.291523  0.882461  -0.062000  0.084487   \n",
       "Normalized_GR                    -0.099243 -0.097750  -0.087257  0.011413   \n",
       "Delta_DTC                         0.358258  0.414080  -0.005210  0.004595   \n",
       "Delta_RHOB                        0.291377  0.882169  -0.061885  0.084397   \n",
       "Delta_GR                          0.000301  0.000319   0.000001  0.000390   \n",
       "Delta_DEPTH_MD                    0.000455  0.001025  -0.001124 -0.000824   \n",
       "Delta_Carbon_Index                0.291375  0.882166  -0.061885  0.084397   \n",
       "GROUP_encoded                     0.059265  0.036976  -0.061223  0.072768   \n",
       "FORMATION_encoded                -0.000308 -0.116644   0.125224 -0.101801   \n",
       "\n",
       "                                  FORCE_2020_LITHOFACIES_LITHOLOGY  \\\n",
       "DEPTH_MD                                                 -0.007422   \n",
       "X_LOC                                                    -0.019755   \n",
       "Y_LOC                                                    -0.034079   \n",
       "Z_LOC                                                     0.019270   \n",
       "CALI                                                     -0.038173   \n",
       "RSHA                                                     -0.041135   \n",
       "RMED                                                     -0.083413   \n",
       "RDEP                                                      0.009211   \n",
       "RHOB                                                     -0.023634   \n",
       "GR                                                        0.163939   \n",
       "NPHI                                                     -0.027391   \n",
       "PEF                                                      -0.057656   \n",
       "DTC                                                       0.008990   \n",
       "SP                                                       -0.044260   \n",
       "BS                                                        0.031248   \n",
       "ROP                                                       0.014388   \n",
       "DCAL                                                     -0.029420   \n",
       "DRHO                                                     -0.027083   \n",
       "MUDWEIGHT                                                 0.027054   \n",
       "RMIC                                                     -0.014886   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY                          1.000000   \n",
       "Carbon_Index                                             -0.025120   \n",
       "Normalized_RHOB                                          -0.023649   \n",
       "Normalized_GR                                             0.156830   \n",
       "Delta_DTC                                                 0.005092   \n",
       "Delta_RHOB                                               -0.023719   \n",
       "Delta_GR                                                  0.001524   \n",
       "Delta_DEPTH_MD                                            0.000752   \n",
       "Delta_Carbon_Index                                       -0.023732   \n",
       "GROUP_encoded                                             0.086791   \n",
       "FORMATION_encoded                                        -0.100015   \n",
       "\n",
       "                                  Carbon_Index  Normalized_RHOB  \\\n",
       "DEPTH_MD                              0.252801         0.267160   \n",
       "X_LOC                                 0.096850         0.098485   \n",
       "Y_LOC                                 0.102346         0.105965   \n",
       "Z_LOC                                -0.262971        -0.277247   \n",
       "CALI                                  0.288104         0.290592   \n",
       "RSHA                                  0.112537         0.113915   \n",
       "RMED                                  0.204782         0.204219   \n",
       "RDEP                                  0.114447         0.113994   \n",
       "RHOB                                  0.999727         1.000000   \n",
       "GR                                   -0.006083        -0.002662   \n",
       "NPHI                                  0.467362         0.473941   \n",
       "PEF                                   0.457744         0.458447   \n",
       "DTC                                   0.428413         0.424933   \n",
       "SP                                    0.079151         0.074799   \n",
       "BS                                   -0.327054        -0.336593   \n",
       "ROP                                  -0.058482        -0.058498   \n",
       "DCAL                                  0.287960         0.291523   \n",
       "DRHO                                  0.882056         0.882461   \n",
       "MUDWEIGHT                            -0.058653        -0.062000   \n",
       "RMIC                                  0.080279         0.084487   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY     -0.025120        -0.023649   \n",
       "Carbon_Index                          1.000000         0.999734   \n",
       "Normalized_RHOB                       0.999734         1.000000   \n",
       "Normalized_GR                        -0.064837        -0.062715   \n",
       "Delta_DTC                             0.462292         0.462418   \n",
       "Delta_RHOB                            0.999323         0.999583   \n",
       "Delta_GR                              0.000334         0.000407   \n",
       "Delta_DEPTH_MD                        0.000882         0.000908   \n",
       "Delta_Carbon_Index                    0.999323         0.999579   \n",
       "GROUP_encoded                         0.019290         0.022306   \n",
       "FORMATION_encoded                    -0.122091        -0.127675   \n",
       "\n",
       "                                  Normalized_GR  Delta_DTC  Delta_RHOB  \\\n",
       "DEPTH_MD                               0.044830   0.215984    0.266868   \n",
       "X_LOC                                  0.167773  -0.071248    0.098435   \n",
       "Y_LOC                                  0.131760   0.047832    0.105851   \n",
       "Z_LOC                                 -0.043703  -0.208389   -0.276951   \n",
       "CALI                                  -0.133204   0.394364    0.290467   \n",
       "RSHA                                  -0.063521   0.088760    0.113818   \n",
       "RMED                                   0.087406   0.350722    0.204328   \n",
       "RDEP                                   0.003944   0.090665    0.114003   \n",
       "RHOB                                  -0.062674   0.462418    0.999583   \n",
       "GR                                     0.583831  -0.074397   -0.002677   \n",
       "NPHI                                   0.091346   0.250326    0.473603   \n",
       "PEF                                   -0.063188   0.213564    0.458318   \n",
       "DTC                                   -0.019223   0.973028    0.424812   \n",
       "SP                                    -0.089115   0.170855    0.074868   \n",
       "BS                                    -0.086266  -0.194345   -0.336376   \n",
       "ROP                                   -0.025989  -0.037838   -0.058467   \n",
       "DCAL                                  -0.099243   0.358258    0.291377   \n",
       "DRHO                                  -0.097750   0.414080    0.882169   \n",
       "MUDWEIGHT                             -0.087257  -0.005210   -0.061885   \n",
       "RMIC                                   0.011413   0.004595    0.084397   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY       0.156830   0.005092   -0.023719   \n",
       "Carbon_Index                          -0.064837   0.462292    0.999323   \n",
       "Normalized_RHOB                       -0.062715   0.462418    0.999583   \n",
       "Normalized_GR                          1.000000  -0.029608   -0.062719   \n",
       "Delta_DTC                             -0.029608   1.000000    0.462362   \n",
       "Delta_RHOB                            -0.062719   0.462362    1.000000   \n",
       "Delta_GR                              -0.039971   0.001921    0.000409   \n",
       "Delta_DEPTH_MD                         0.000512   0.000595   -0.002774   \n",
       "Delta_Carbon_Index                    -0.062720   0.462344    0.999997   \n",
       "GROUP_encoded                          0.005206   0.059329    0.022265   \n",
       "FORMATION_encoded                     -0.183059  -0.073349   -0.127563   \n",
       "\n",
       "                                  Delta_GR  Delta_DEPTH_MD  \\\n",
       "DEPTH_MD                          0.000432        0.002445   \n",
       "X_LOC                            -0.000287        0.000974   \n",
       "Y_LOC                            -0.000184        0.002194   \n",
       "Z_LOC                            -0.000414       -0.002509   \n",
       "CALI                              0.000167        0.000338   \n",
       "RSHA                             -0.000398       -0.001663   \n",
       "RMED                              0.001224        0.000175   \n",
       "RDEP                              0.000662        0.000325   \n",
       "RHOB                              0.000408        0.000908   \n",
       "GR                               -0.051292       -0.000304   \n",
       "NPHI                              0.000093        0.001268   \n",
       "PEF                               0.000100        0.002135   \n",
       "DTC                              -0.000272        0.000017   \n",
       "SP                               -0.000084       -0.002900   \n",
       "BS                               -0.000202        0.001167   \n",
       "ROP                              -0.000208        0.000544   \n",
       "DCAL                              0.000301        0.000455   \n",
       "DRHO                              0.000319        0.001025   \n",
       "MUDWEIGHT                         0.000001       -0.001124   \n",
       "RMIC                              0.000390       -0.000824   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY  0.001524        0.000752   \n",
       "Carbon_Index                      0.000334        0.000882   \n",
       "Normalized_RHOB                   0.000407        0.000908   \n",
       "Normalized_GR                    -0.039971        0.000512   \n",
       "Delta_DTC                         0.001921        0.000595   \n",
       "Delta_RHOB                        0.000409       -0.002774   \n",
       "Delta_GR                          1.000000        0.009598   \n",
       "Delta_DEPTH_MD                    0.009598        1.000000   \n",
       "Delta_Carbon_Index                0.000385       -0.002758   \n",
       "GROUP_encoded                     0.001176        0.001489   \n",
       "FORMATION_encoded                 0.000555       -0.000826   \n",
       "\n",
       "                                  Delta_Carbon_Index  GROUP_encoded  \\\n",
       "DEPTH_MD                                    0.266868       0.056807   \n",
       "X_LOC                                       0.098434       0.094923   \n",
       "Y_LOC                                       0.105851       0.024585   \n",
       "Z_LOC                                      -0.276951      -0.044176   \n",
       "CALI                                        0.290466       0.055227   \n",
       "RSHA                                        0.113820      -0.011004   \n",
       "RMED                                        0.204323      -0.031007   \n",
       "RDEP                                        0.114004       0.045048   \n",
       "RHOB                                        0.999579       0.022361   \n",
       "GR                                         -0.002682       0.062451   \n",
       "NPHI                                        0.473602       0.063828   \n",
       "PEF                                         0.458312       0.024317   \n",
       "DTC                                         0.424818       0.028162   \n",
       "SP                                          0.074867      -0.072891   \n",
       "BS                                         -0.336375      -0.019857   \n",
       "ROP                                        -0.058465       0.017402   \n",
       "DCAL                                        0.291375       0.059265   \n",
       "DRHO                                        0.882166       0.036976   \n",
       "MUDWEIGHT                                  -0.061885      -0.061223   \n",
       "RMIC                                        0.084397       0.072768   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY           -0.023732       0.086791   \n",
       "Carbon_Index                                0.999323       0.019290   \n",
       "Normalized_RHOB                             0.999579       0.022306   \n",
       "Normalized_GR                              -0.062720       0.005206   \n",
       "Delta_DTC                                   0.462344       0.059329   \n",
       "Delta_RHOB                                  0.999997       0.022265   \n",
       "Delta_GR                                    0.000385       0.001176   \n",
       "Delta_DEPTH_MD                             -0.002758       0.001489   \n",
       "Delta_Carbon_Index                          1.000000       0.022264   \n",
       "GROUP_encoded                               0.022264       1.000000   \n",
       "FORMATION_encoded                          -0.127561      -0.025375   \n",
       "\n",
       "                                  FORMATION_encoded  \n",
       "DEPTH_MD                                  -0.293998  \n",
       "X_LOC                                     -0.225151  \n",
       "Y_LOC                                     -0.163860  \n",
       "Z_LOC                                      0.291771  \n",
       "CALI                                       0.017484  \n",
       "RSHA                                       0.001706  \n",
       "RMED                                      -0.022347  \n",
       "RDEP                                      -0.006239  \n",
       "RHOB                                      -0.127749  \n",
       "GR                                        -0.172180  \n",
       "NPHI                                      -0.225500  \n",
       "PEF                                       -0.007112  \n",
       "DTC                                       -0.028480  \n",
       "SP                                         0.065500  \n",
       "BS                                         0.319115  \n",
       "ROP                                        0.037182  \n",
       "DCAL                                      -0.000308  \n",
       "DRHO                                      -0.116644  \n",
       "MUDWEIGHT                                  0.125224  \n",
       "RMIC                                      -0.101801  \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY          -0.100015  \n",
       "Carbon_Index                              -0.122091  \n",
       "Normalized_RHOB                           -0.127675  \n",
       "Normalized_GR                             -0.183059  \n",
       "Delta_DTC                                 -0.073349  \n",
       "Delta_RHOB                                -0.127563  \n",
       "Delta_GR                                   0.000555  \n",
       "Delta_DEPTH_MD                            -0.000826  \n",
       "Delta_Carbon_Index                        -0.127561  \n",
       "GROUP_encoded                             -0.025375  \n",
       "FORMATION_encoded                          1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f9399",
   "metadata": {},
   "source": [
    "Analisando a tabela de correlação entre as colunas do dataset acima, nota-se uma forte relação entre algumas delas.\n",
    "\n",
    "Aparentemente, as variáveis abaixo tratam-se da mesma informação, que passou por alguma regra de normalização linear e que são reforçadas pela relação entre o nome destas variáveis:\n",
    "- RHOB x Delta_RHOB x Normalized_RHOB\n",
    "- Carbon_Index x Delta_Carbon_Index\n",
    "- DTC x Delta_DTC\n",
    "\n",
    "Por este motivo, as colunas originais e/ou normalizadas serão desconsideradas deste experimento, a fim de reduzir a complexidade do modelo. Logo, Entre as variáveis listadas acima, serão mantidas apenas: `Delta_RHOB`, `Delta_Carbon_Index` e `Delta_DTC`.\n",
    "\n",
    "Outra colunas, como `GR` e `DEPTH_MD`, que têm outras variáveis com um nome corresponde `Normalized_` e `Delta_`, respectivamente, não apresentaram correlação direta. Portanto, serão mantidas neste experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce98748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>PEF</th>\n",
       "      <th>DTC</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>Carbon_Index</th>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <th>Normalized_GR</th>\n",
       "      <th>Delta_DTC</th>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <th>Delta_GR</th>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "      <td>1.170511e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.184087e+03</td>\n",
       "      <td>4.856292e+05</td>\n",
       "      <td>6.679845e+06</td>\n",
       "      <td>-2.128037e+03</td>\n",
       "      <td>-6.280493e+01</td>\n",
       "      <td>-4.549949e+02</td>\n",
       "      <td>-2.845864e+01</td>\n",
       "      <td>1.189409e+00</td>\n",
       "      <td>-1.356686e+02</td>\n",
       "      <td>7.091370e+01</td>\n",
       "      <td>-3.455267e+02</td>\n",
       "      <td>-4.221022e+02</td>\n",
       "      <td>7.842011e+01</td>\n",
       "      <td>-2.171530e+02</td>\n",
       "      <td>1.187501e+01</td>\n",
       "      <td>-4.795367e+02</td>\n",
       "      <td>-8.728321e+01</td>\n",
       "      <td>-1.612117e+02</td>\n",
       "      <td>-7.288450e+02</td>\n",
       "      <td>-8.474787e+02</td>\n",
       "      <td>6.138598e+04</td>\n",
       "      <td>-1.279143e+02</td>\n",
       "      <td>-1.371562e+02</td>\n",
       "      <td>2.641028e-01</td>\n",
       "      <td>-3.303349e+01</td>\n",
       "      <td>-1.377378e+02</td>\n",
       "      <td>-7.344170e-04</td>\n",
       "      <td>1.530734e-01</td>\n",
       "      <td>-1.377387e+02</td>\n",
       "      <td>6.627583e+00</td>\n",
       "      <td>4.129572e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.971821e+02</td>\n",
       "      <td>3.445166e+04</td>\n",
       "      <td>1.290797e+05</td>\n",
       "      <td>9.725712e+02</td>\n",
       "      <td>2.667498e+02</td>\n",
       "      <td>5.087190e+02</td>\n",
       "      <td>1.880157e+02</td>\n",
       "      <td>1.495506e+02</td>\n",
       "      <td>3.451085e+02</td>\n",
       "      <td>3.423149e+01</td>\n",
       "      <td>4.754047e+02</td>\n",
       "      <td>4.972170e+02</td>\n",
       "      <td>2.045332e+02</td>\n",
       "      <td>4.701992e+02</td>\n",
       "      <td>3.052529e+00</td>\n",
       "      <td>1.184783e+03</td>\n",
       "      <td>2.841910e+02</td>\n",
       "      <td>3.675140e+02</td>\n",
       "      <td>4.441372e+02</td>\n",
       "      <td>3.616692e+02</td>\n",
       "      <td>1.389170e+04</td>\n",
       "      <td>3.482977e+02</td>\n",
       "      <td>3.445138e+02</td>\n",
       "      <td>1.806016e-01</td>\n",
       "      <td>1.786926e+02</td>\n",
       "      <td>3.444248e+02</td>\n",
       "      <td>3.516985e+00</td>\n",
       "      <td>3.212288e-01</td>\n",
       "      <td>3.444257e+02</td>\n",
       "      <td>3.125934e+00</td>\n",
       "      <td>2.282790e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.360860e+02</td>\n",
       "      <td>4.268988e+05</td>\n",
       "      <td>6.406641e+06</td>\n",
       "      <td>-5.395563e+03</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>1.092843e-01</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-2.685117e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.418597e+03</td>\n",
       "      <td>4.548018e+05</td>\n",
       "      <td>6.591140e+06</td>\n",
       "      <td>-2.804552e+03</td>\n",
       "      <td>8.874150e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>8.711777e-01</td>\n",
       "      <td>8.963877e-01</td>\n",
       "      <td>2.000454e+00</td>\n",
       "      <td>4.762722e+01</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>8.526329e+01</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>8.500000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-5.335999e-02</td>\n",
       "      <td>-2.112656e-02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>2.852751e+00</td>\n",
       "      <td>3.509227e-01</td>\n",
       "      <td>1.209661e-01</td>\n",
       "      <td>-7.185020e-01</td>\n",
       "      <td>-1.644254e-02</td>\n",
       "      <td>-1.380630e+00</td>\n",
       "      <td>1.520000e-01</td>\n",
       "      <td>-5.005838e-01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.076605e+03</td>\n",
       "      <td>4.769078e+05</td>\n",
       "      <td>6.732205e+06</td>\n",
       "      <td>-2.030516e+03</td>\n",
       "      <td>1.242991e+01</td>\n",
       "      <td>5.516068e-01</td>\n",
       "      <td>1.400712e+00</td>\n",
       "      <td>1.428157e+00</td>\n",
       "      <td>2.243782e+00</td>\n",
       "      <td>6.836763e+01</td>\n",
       "      <td>2.352654e-01</td>\n",
       "      <td>2.899809e+00</td>\n",
       "      <td>1.069366e+02</td>\n",
       "      <td>4.044000e+01</td>\n",
       "      <td>1.225000e+01</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>3.736696e-01</td>\n",
       "      <td>-2.317054e-03</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>7.653391e+00</td>\n",
       "      <td>5.256773e-01</td>\n",
       "      <td>2.238263e-01</td>\n",
       "      <td>-1.633453e-02</td>\n",
       "      <td>-2.060652e-03</td>\n",
       "      <td>-1.548767e-03</td>\n",
       "      <td>1.520000e-01</td>\n",
       "      <td>-6.042549e-02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.864393e+03</td>\n",
       "      <td>5.201532e+05</td>\n",
       "      <td>6.784878e+06</td>\n",
       "      <td>-1.373344e+03</td>\n",
       "      <td>1.574931e+01</td>\n",
       "      <td>1.510922e+00</td>\n",
       "      <td>2.587100e+00</td>\n",
       "      <td>2.537876e+00</td>\n",
       "      <td>2.462978e+00</td>\n",
       "      <td>8.903551e+01</td>\n",
       "      <td>3.653732e-01</td>\n",
       "      <td>4.619021e+00</td>\n",
       "      <td>1.401350e+02</td>\n",
       "      <td>7.039606e+01</td>\n",
       "      <td>1.225000e+01</td>\n",
       "      <td>1.502688e+01</td>\n",
       "      <td>1.693081e+00</td>\n",
       "      <td>1.489764e-02</td>\n",
       "      <td>1.330073e-01</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>1.550824e+01</td>\n",
       "      <td>6.792460e-01</td>\n",
       "      <td>3.778281e-01</td>\n",
       "      <td>5.912170e-01</td>\n",
       "      <td>5.912662e-03</td>\n",
       "      <td>1.371843e+00</td>\n",
       "      <td>1.520000e-01</td>\n",
       "      <td>1.740417e-01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>6.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.436632e+03</td>\n",
       "      <td>5.726328e+05</td>\n",
       "      <td>6.856661e+06</td>\n",
       "      <td>-1.110860e+02</td>\n",
       "      <td>2.827900e+01</td>\n",
       "      <td>2.193905e+03</td>\n",
       "      <td>1.988616e+03</td>\n",
       "      <td>1.999887e+03</td>\n",
       "      <td>3.457820e+00</td>\n",
       "      <td>1.076964e+03</td>\n",
       "      <td>9.995703e-01</td>\n",
       "      <td>3.831300e+02</td>\n",
       "      <td>1.555313e+03</td>\n",
       "      <td>5.265473e+02</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>4.701512e+04</td>\n",
       "      <td>7.999847e+00</td>\n",
       "      <td>1.467912e+00</td>\n",
       "      <td>1.857309e+02</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>9.900000e+04</td>\n",
       "      <td>1.570291e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.596500e+02</td>\n",
       "      <td>1.120613e+00</td>\n",
       "      <td>2.665444e+02</td>\n",
       "      <td>2.693440e+02</td>\n",
       "      <td>1.107053e+02</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>6.900000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DEPTH_MD         X_LOC         Y_LOC         Z_LOC          CALI  \\\n",
       "count  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06   \n",
       "mean   2.184087e+03  4.856292e+05  6.679845e+06 -2.128037e+03 -6.280493e+01   \n",
       "std    9.971821e+02  3.445166e+04  1.290797e+05  9.725712e+02  2.667498e+02   \n",
       "min    1.360860e+02  4.268988e+05  6.406641e+06 -5.395563e+03 -9.990000e+02   \n",
       "25%    1.418597e+03  4.548018e+05  6.591140e+06 -2.804552e+03  8.874150e+00   \n",
       "50%    2.076605e+03  4.769078e+05  6.732205e+06 -2.030516e+03  1.242991e+01   \n",
       "75%    2.864393e+03  5.201532e+05  6.784878e+06 -1.373344e+03  1.574931e+01   \n",
       "max    5.436632e+03  5.726328e+05  6.856661e+06 -1.110860e+02  2.827900e+01   \n",
       "\n",
       "               RSHA          RMED          RDEP          RHOB            GR  \\\n",
       "count  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06   \n",
       "mean  -4.549949e+02 -2.845864e+01  1.189409e+00 -1.356686e+02  7.091370e+01   \n",
       "std    5.087190e+02  1.880157e+02  1.495506e+02  3.451085e+02  3.423149e+01   \n",
       "min   -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02  1.092843e-01   \n",
       "25%   -9.990000e+02  8.711777e-01  8.963877e-01  2.000454e+00  4.762722e+01   \n",
       "50%    5.516068e-01  1.400712e+00  1.428157e+00  2.243782e+00  6.836763e+01   \n",
       "75%    1.510922e+00  2.587100e+00  2.537876e+00  2.462978e+00  8.903551e+01   \n",
       "max    2.193905e+03  1.988616e+03  1.999887e+03  3.457820e+00  1.076964e+03   \n",
       "\n",
       "               NPHI           PEF           DTC            SP            BS  \\\n",
       "count  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06   \n",
       "mean  -3.455267e+02 -4.221022e+02  7.842011e+01 -2.171530e+02  1.187501e+01   \n",
       "std    4.754047e+02  4.972170e+02  2.045332e+02  4.701992e+02  3.052529e+00   \n",
       "min   -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02  6.000000e+00   \n",
       "25%   -9.990000e+02 -9.990000e+02  8.526329e+01 -9.990000e+02  8.500000e+00   \n",
       "50%    2.352654e-01  2.899809e+00  1.069366e+02  4.044000e+01  1.225000e+01   \n",
       "75%    3.653732e-01  4.619021e+00  1.401350e+02  7.039606e+01  1.225000e+01   \n",
       "max    9.995703e-01  3.831300e+02  1.555313e+03  5.265473e+02  2.600000e+01   \n",
       "\n",
       "                ROP          DCAL          DRHO     MUDWEIGHT          RMIC  \\\n",
       "count  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06   \n",
       "mean  -4.795367e+02 -8.728321e+01 -1.612117e+02 -7.288450e+02 -8.474787e+02   \n",
       "std    1.184783e+03  2.841910e+02  3.675140e+02  4.441372e+02  3.616692e+02   \n",
       "min   -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02   \n",
       "25%   -9.990000e+02 -5.335999e-02 -2.112656e-02 -9.990000e+02 -9.990000e+02   \n",
       "50%   -9.990000e+02  3.736696e-01 -2.317054e-03 -9.990000e+02 -9.990000e+02   \n",
       "75%    1.502688e+01  1.693081e+00  1.489764e-02  1.330073e-01 -9.990000e+02   \n",
       "max    4.701512e+04  7.999847e+00  1.467912e+00  1.857309e+02  1.000000e+04   \n",
       "\n",
       "       FORCE_2020_LITHOFACIES_LITHOLOGY  Carbon_Index  Normalized_RHOB  \\\n",
       "count                      1.170511e+06  1.170511e+06     1.170511e+06   \n",
       "mean                       6.138598e+04 -1.279143e+02    -1.371562e+02   \n",
       "std                        1.389170e+04  3.482977e+02     3.445138e+02   \n",
       "min                        3.000000e+04 -9.990000e+02    -9.990000e+02   \n",
       "25%                        6.500000e+04  2.852751e+00     3.509227e-01   \n",
       "50%                        6.500000e+04  7.653391e+00     5.256773e-01   \n",
       "75%                        6.500000e+04  1.550824e+01     6.792460e-01   \n",
       "max                        9.900000e+04  1.570291e+02     1.000000e+00   \n",
       "\n",
       "       Normalized_GR     Delta_DTC    Delta_RHOB      Delta_GR  \\\n",
       "count   1.170511e+06  1.170511e+06  1.170511e+06  1.170511e+06   \n",
       "mean    2.641028e-01 -3.303349e+01 -1.377378e+02 -7.344170e-04   \n",
       "std     1.806016e-01  1.786926e+02  3.444248e+02  3.516985e+00   \n",
       "min     0.000000e+00 -9.990000e+02 -9.990000e+02 -2.685117e+02   \n",
       "25%     1.209661e-01 -7.185020e-01 -1.644254e-02 -1.380630e+00   \n",
       "50%     2.238263e-01 -1.633453e-02 -2.060652e-03 -1.548767e-03   \n",
       "75%     3.778281e-01  5.912170e-01  5.912662e-03  1.371843e+00   \n",
       "max     1.000000e+00  7.596500e+02  1.120613e+00  2.665444e+02   \n",
       "\n",
       "       Delta_DEPTH_MD  Delta_Carbon_Index  GROUP_encoded  FORMATION_encoded  \n",
       "count    1.170511e+06        1.170511e+06   1.170511e+06       1.170511e+06  \n",
       "mean     1.530734e-01       -1.377387e+02   6.627583e+00       4.129572e+01  \n",
       "std      3.212288e-01        3.444257e+02   3.125934e+00       2.282790e+01  \n",
       "min      0.000000e+00       -9.990000e+02   0.000000e+00      -1.000000e+00  \n",
       "25%      1.520000e-01       -5.005838e-01   5.000000e+00       2.400000e+01  \n",
       "50%      1.520000e-01       -6.042549e-02   6.000000e+00       3.900000e+01  \n",
       "75%      1.520000e-01        1.740417e-01   9.000000e+00       6.600000e+01  \n",
       "max      2.693440e+02        1.107053e+02   1.300000e+01       6.900000e+01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_full\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc22d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de classes presentes neste dataset: 12\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de classes presentes neste dataset:', len(df['FORCE_2020_LITHOFACIES_LITHOLOGY'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126f6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(df, column, title_sufix: str=None):\n",
    "    '''\n",
    "    Plota a distribuição da coluna em um gráfico de barras horizontal.\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(16,9))\n",
    "    grouped = df[column].value_counts().sort_values(ascending=True)\n",
    "\n",
    "    df_grouped = grouped.to_frame()\n",
    "    df_grouped.reset_index(level=0, inplace=True)\n",
    "    df_grouped.columns = ['item', 'count']\n",
    "    labels = df_grouped['item'].astype(str)\n",
    "    values = df_grouped['count']\n",
    "\n",
    "    ax.clear()\n",
    "    ax.barh(labels, values, alpha=0.75)\n",
    "    for i, (value, name) in enumerate(zip(values, labels)):\n",
    "        ax.text(value, i, value, size=12, ha='left', va='center')\n",
    "\n",
    "    ax.tick_params(axis='x', colors='#777777', labelsize=12)\n",
    "    ax.margins(0.003, 0.01)\n",
    "    ax.grid(which='major', axis='x', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    title = 'Distribuição de classes'\n",
    "    if title_sufix is not None:\n",
    "        title = f'{title} no conjunto de {title_sufix}'\n",
    "        \n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36cb2f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAIbCAYAAAAZypalAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABel0lEQVR4nO3de5yVZb3//9cHhsPIwHBUBExNUXRMLdEs0zTdgrRDozzgCdIiQ+3gLl0WJSnZ8mc7yy/BjtSCNMnEA7bxlDvNzFRUMPAIigoeEXAYEHSG6/fHupkGGGBAbobD6/l43A/Wuk73da9LGN9zH1aklJAkSZIkSZtWi+aegCRJkiRJ2yIDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuStloR8T8R8cNNNNZHIqImIlpm7++PiK9u5Fjfj4hrmtj23oj4e7b/WzZmf2sZd6Pnv44xfxcRozblmJIkbcvKmnsCkiQ1JiLmADsBtUAd8DQwARiXUloBkFI6ZwPG+mpK6S9ra5NSegWo+HCzrh/r8ibOqzMwF7gemARcuin2L0mStgwGbknSluwLKaW/REQl8Fngl8Anga9syp1ERFlKqXZTjtkUKaUF/PtYDt7c+5ckSfnyknJJ0hYvpfRuSmkycDIwJCL2g1UvcY6IrhHx54hYFBELIuLBiGgREb8HPgLckV0yfmFE7BYRKSLOjohXgP9rUNbwl9F7RMSjEVEdEbdnZ6SJiCMjYm7DOUbEnIg4Jns9MiKub1D3mYj4Rza3VyNiaFb++Yh4Mhv/1YgYudqYAyNiZtbv/ojYZ22fUUT8R0Q8GxHvRsRoIFarPysinomIhRFxd0Tsuo6xGp3vam06ZZ/329mYf46IXg3qh0bEixGxOCJeiojTsvI9I+KBbJ7zI+KPDfr0yS6xXxARz0XESQ3qBkTE09l48yLiu2ubvyRJWwoDtyRpq5FSepTSJdiHN1L9X1ldN0qXon+/1CWdAbxC6Wx5RUrp/2vQ57PAPkC/tezyTOAsYGdKl7ZfvaFzzoLtncD/y+Z2IDAtq16S7aMj8HngGxFxQtZvL+BG4NtZvymUfmnQupF9dAVuAUYAXYHZwGEN6o+n9HkMysZ6MBt7Q+fbUAvgt8CulH6h8R4wOhujHaXP6riUUnvg0w3GuAy4B+gE9Mr2s7LPvcAfgB2BU4AxEbFv1u9a4OvZePsB/9fY/CVJ2pIYuCVJW5vXgM6NlH9AKRjvmlL6IKX0YEoprWeskSmlJSml99ZS//uU0oyU0hLgh8BJKx+qtgFOBf6SUroxm9c7KaVpACml+1NK/0oprUgpPUUpBH8263cy8L8ppXtTSh8APwPKKYXX1Q0AZqaUbs7a/gJ4o0H9OcBPU0rPZJfOXw4cuJaz3Gudb0NZ+aSU0tKU0mLgJw3mDrAC2C8iylNKr6eUZmblH1AK6T1SSstSSn/Pyv8TmJNS+m1KqTal9CSl+9pPbNBv34jokFJamFJ6opG5S5K0RTFwS5K2Nj2BBY2UXwnMAu7JLmUuNGGsVzeg/mWgFaUzyBtiF0pnnNcQEZ+MiL9ml2W/SykYrxy/R7ZPALIHxb1K6fhX16PhXLNfNDSc+67AL7NLxBdR+vxiLWOtdb6rzX2HiPh1RLwcEdXA34COEdEy+wXFydnxvB4R/xsRfbKuF2b7fjS7XP6sBnP85Mo5ZvM8Deie1X+J0i8WXs4uSf/U+uYoSVJzM3BLkrYaEXEwpZD499XrUkqLU0r/lVL6KDAQuCAijl5ZvZYh13cGfJcGrz9C6SzrfEqXgu/QYF4tKV1+3ZhXgT3WUvcHYDKwS0qpEvgf/n3v9WuUQujKfUQ2n3mNjPN6w7k2aNtwDl9PKXVssJWnlP6xgfNt6L+AvYFPppQ6AEes3D1ASunulNJ/ULrq4FngN1n5Gymlr6WUegBfp3TZ+J7Zfh9YbY4VKaVvZP0eSykdT+ly89uAm5owR0mSmpWBW5K0xYuIDhHxn8BE4PqU0r8aafOf2QO5AniX0leJrciq3wQ+uhG7Pj0i9o2IHSh9ZdfNKaU64HmgbfbQs1aU7p1us5YxbgCOiYiTIqIsIrpExIFZXXtgQUppWUQcQuly7pVuAj4fEUdn+/gvYDnQWEj+X6AqIgZF6aFv3+TfZ4ahFOQvjogqgIiojIgTGxlnffNtqD2l+7YXRelhcpesrIiInSLi+Oy+7OVADdlaRMSJDR6utpDSLz1WAH8G9oqIMyKiVbYdHBH7RETriDgtIiqzS+ar+ffaSpK0xTJwS5K2ZHdExGJKZz9/APyctX8lWG/gL5TC3cPAmJTSX7O6nwIjskuVN+Tp1r8Hfkfpfui2lIIsKaV3geHANZTOOC+h9MC2NWTf7z2AUmD+AJgBHJBVDwcuzY7xRzQ4a5tSeg44ndJDxeYDX6D04Lf3G9nHfEr3OheBd7LP4qEG9bcCVwATs8u/ZwDHNWG+Cyg97OyARpr+gtI95fOBfwJ3NahrAVxA6Sz9Akr3dn8jqzsYeCQiaiid3f9WSunF7D7wYyk9LO01Sp/5Ffz7FxlnAHOy+Z9D6XJzSZK2aLH+58lIkqRNISLOAFqnlK5t7rlIkqT8eYZbkqTNICIqKH092VHNPRdJkrR5GLglSdo8fgvcQek7riVJ0nZgqw3chUJhWHPPQZuHa719cb23H9vbWqeUTkwpdUgp3dDcc2kO29t6b89c6+2L6739cK03zlYbuAEXfPvhWm9fXO/th2u9fXG9tx+u9fbF9d5+uNYbYWsO3JIkSZIkbbG22qeUt2/fPu29997NPQ1tBgsWLKBz587NPQ1tJq739sO13r643tsP13r74npvP5pjrR9//PH5KaVum3Wnm1hZc09gY/Xo0YOpU6c29zS0Gdx///0ceeSRzT0NbSau9/bDtd6+uN7bD9d6++J6bz+aY60j4uXNusMceEm5JEmSJEk5MHBLkiRJkpSDrfYe7spevdNxP5rQ3NPQZrBo0SI6duzY3NPQZuJ6bz9c6+2L6739cK23L6739mNzrfXEYZ+qfx0Rj6eU+ua+0xx5hluSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkSVuMiooKKioqAD4eETURURcR/w8gIg6NiHsjYkFEvB0Rf4qInVf2jZIrIuKdbLsiIqJB/eci4omIqI6IFyNiWMN9R8SpEfFyRCyJiNsionODuusj4vWs7/MR8dX1HUuTAndEdIyImyPi2Yh4JiI+FREjI2JeREzLtgEN2l8cEbMi4rmI6NegvH9WNisiCg3Kd4+IR7LyP0ZE66bMS5IkSZK0bampqaGmpgbgSaA78B7wp6y6EzAO2A3YFVgM/LZB92HACcABwP7AF4CvA0REK+BW4NdAJXAy8POIOCCrr8rqzgB2ApYCYxqM/VNgt5RSB2AgMCoiDlrXsTT1DPcvgbtSSn2yiT+TlV+VUjow26Zkk9wXOAWoAvoDYyKiZUS0BH4FHAfsCwzO2gJckY21J7AQOLuJ85IkSZIkbbu+BLwFPAiQUrozpfSnlFJ1SmkpMBo4rEH7IcB/p5TmppTmAf8NDM3qOgMdgN+nkscoZduVufQ04I6U0t9SSjXAD4FBEdE+2/fMlNLyrG3Ktj3WNfn1Bu6IqASOAK7NdvJ+SmnROrocD0xMKS1PKb0EzAIOybZZKaUXU0rvAxOB47PT+58Dbs76j6f0GwlJkiRJ0vZtCDAhpZTWUn8EMLPB+ypgeoP307MyUkpvAjcCX8lOCn+K0lnyvzfWN6U0G3gf2GtlWUSMiYilwLPA68CUdU2+KWe4dwfeBn4bEU9GxDUR0S6rOy8inoqI6yKiU1bWE3i1Qf+5WdnayrsAi1JKtauVS5IkSZK2X62Bz1I6KbuGiNgf+BHwvQbFFcC7Dd6/C1Q0uI/7xqzPckpnzX+QUnp1LX1X9m+/8k1KaXj2/nDglmyctWpK4C4DPgGMTSl9HFgCFICxlE6fH0gp2f93E8b6UCJiWERMjYiptbV1ee9OkiRJktR8dqF0H/WfshxY/4CziNgTuBP4VkrpwQZ9aihdNr5SB6AmpZQiog+lK63PpBTmq4ALI+Lza+m7sv/ihgUppbqU0t+BXsA31nUATQncc4G5KaVHsvc3A59IKb2Z7WgF8BtKl4wDzKP0wazUKytbW/k7QMeIKFutfA0ppXEppb4ppb5lZS2bMHVJkiRJ0laqnFKg7ptt4wAiYlfgL8BlKaXfr9ZnJqXnjq10AP++5Hw/4PmU0t0ppRUppeeA/6X0nLE1+kbER4E2wPNrmV8ZH/Ye7pTSG8CrEbF3VnQ08HTDR68DXwRmZK8nA6dERJuI2B3oDTwKPAb0zp5I3prSg9UmZ9fi/xX4ctZ/CHD7+uYlSZIkSdo2/eMf/wBoxb+fTg5ARPQE/g8YnVL6n0a6TgAuiIieEdED+C/gd1ndk5Qy6eeyrw/bA/hP4Kms/gbgCxFxeHYb9aXALSmlxRGxY0ScEhEV2f3f/YDBwH3rOo6ydVU2cD5wQxaUXwS+AlwdEQdSejLbHLJHraeUZkbETcDTQC1wbkqpLvtwzgPuBloC16WUVv6m4SJgYkSMyj6Ea5s4L0mSJEnSNmb8+PFQetbX4tWqvgp8FBgZESNXFqaUKrKXv87q/5W9vyYrI6U0OyLOAq6m9LC0dymF7Guy+pkRcU5W1oXSWfSvrNwFpcvH/4fSieuXgW+nlCav6zhi7Q9727JV9uqdjvvRhOaehjaDRYsW0bFjx+aehjYT13v74VpvX1zv7YdrvX1xvbcfm2utJw77VP3riHg8pdQ3953mqKnfwy1JkiRJkjaAgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKQVlzT2BjdW/XgonDPtXc09BmcP/993Pkka719sL13n641tsX13v74VpvX1zv7YdrvXE8wy1JkiRJUg4M3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg622u/hfmPJCk4Z93BzT0ObwaJF7/E/z7vWm5LfYS9JkiTlzzPckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3NJ2bPTo0fTt25c2bdowdOjQVeqWLl3K8OHD6dq1K5WVlRxxxBH1dcuXL+ecc85hp512onPnznzhC19g3rx59fXPPPMMn/vc56isrGTPPffk1ltvra97+umn6du3L506daJTp04cc8wxPP300/X1F110ERUVFfVb69at+djHPpbfhyBJkiTlxMAtbcd69OjBiBEjOOuss9aoGzZsGAsWLOCZZ55hwYIFXHXVVfV1v/zlL3n44Yd56qmneO211+jUqRPnn38+ALW1tRx//PH853/+JwsWLGDcuHGcfvrpPP/88/X7vPnmm1mwYAHz589n4MCBnHLKKfVjX3HFFdTU1NRvn/70pznxxBNz/iQkSZKkTW+9gTsi2kbEoxExPSJmRsSPs/LdI+KRiJgVEX+MiNZZeZvs/aysfrcGY12clT8XEf0alPfPymZFRCGH45TUiEGDBnHCCSfQpUuXVcqfffZZJk+ezLhx4+jWrRstW7bkoIMOqq9/6aWX6NevHzvttBNt27bl5JNPZubMmfV9X3vtNb7zne/QsmVLPve5z3HYYYfx+9//HoCOHTuy2267ERGklGjZsiWzZs1qdH5z5szhwQcf5Mwzz8zpE5AkSZLy05Qz3MuBz6WUDgAOBPpHxKHAFcBVKaU9gYXA2Vn7s4GFWflVWTsiYl/gFKAK6A+MiYiWEdES+BVwHLAvMDhrK6mZPProo+y6665ccskldO3alY997GNMmjSpvv7ss8/moYce4rXXXmPp0qXccMMNHHfccWsdL6XEjBkzVinr2LEjbdu25fzzz+f73/9+o/0mTJjA4Ycfzm677bZJjkuSJEnanNYbuFNJTfa2VbYl4HPAzVn5eOCE7PXx2Xuy+qMjIrLyiSml5Smll4BZwCHZNiul9GJK6X1gYtZWUjOZO3cuM2bMoLKyktdee43Ro0czZMgQnnnmGQB69+7NLrvsQs+ePenQoQPPPPMMP/rRjwDYe++92XHHHbnyyiv54IMPuOeee3jggQdYunTpKvtYtGgR7777LqNHj+bjH/94o/OYMGHCGveWS5IkSVuLJt3DnZ2Jnga8BdwLzAYWpZRqsyZzgZ7Z657AqwBZ/btAl4blq/VZW3lj8xgWEVMjYmptbV1Tpi5pI5SXl9OqVStGjBhB69at+exnP8tRRx3FPffcA8C5557L8uXLeeedd1iyZAmDBg2qP8PdqlUrbrvtNv73f/+X7t2789///d+cdNJJ9OrVa439tGvXjnPOOYczzzyTt956a5W6v//977zxxht8+ctfzv+AJUmStCXqujL/Zduw5p7QhmpS4E4p1aWUDgR6UToj3SfPSa1jHuNSSn1TSn3Lylo2xxSk7cL++++/RlnpQpWSadOmMXToUDp37kybNm04//zzefTRR5k/f359/wceeIB33nmHu+++mxdffJFDDjmk0X2tWLGCpUuXrvKUc4Dx48czaNAgKioqNuGRSZIkaSsyf2X+y7ZxzT2hDbVBTylPKS0C/gp8CugYEWVZVS9g5f8tzwN2AcjqK4F3Gpav1mdt5ZJyVltby7Jly6irq6Ouro5ly5ZRW1vLEUccwUc+8hF++tOfUltby0MPPcRf//pX+vUrPevw4IMPZsKECbz77rt88MEHjBkzhh49etC1a1cAnnrqKZYtW8bSpUv52c9+xuuvv15/afi9997Lk08+SV1dHdXV1VxwwQV06tSJffbZp35e7733HjfddJOXk0uSJGmr1pSnlHeLiI7Z63LgP4BnKAXvldd6DgFuz15Pzt6T1f9fSill5adkTzHfHegNPAo8BvTOnnremtKD1SZvgmOTtB6jRo2ivLycYrHI9ddfT3l5OaNGjaJVq1bcfvvtTJkyhcrKSr72ta8xYcIE+vQpXdzys5/9jLZt29K7d2+6devGlClTVvmu7d///vfsvPPO7Ljjjtx3333ce++9tGnTBijduz148GAqKyvZY489mD17NnfddRdt27at73/bbbfRsWNHjjrqqM37gUiSJEmbUNn6m7AzMD57mngL4KaU0p8j4mlgYkSMAp4Ers3aXwv8PiJmAQsoBWhSSjMj4ibgaaAWODelVAcQEecBdwMtgetSSjM32RFKWquRI0cycuTIRuuqqqp4+OGHG63r0qULN9xww1rHvfLKK7nyyisbrTvxxBPX+73agwcPZvDgwetsI0mSJG3p1hu4U0pPAWs8Qjil9CKl+7lXL18GNPp/0ymlnwA/aaR8CjClCfOVJEmSJGmrsEH3cEuSJEmSpKYxcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5KGvuCWys7u1aMHHYp5p7GtoM7r//fo480rWWJEmStHXxDLckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTnYar+H+40lKzhl3MPNPQ1tBosWvcf/PL/9rbXfMy9JkiRt3TzDLUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1t4UaPHk3fvn1p06YNQ4cOrS+fM2cOEUFFRUX9dtlll9XXL1++nLPOOosOHTrQvXt3fv7znzc6/qWXXkpE8Je//KW+bOjQobRu3XqVsevq6pq0X0mSJEklZc09AUnr1qNHD0aMGMHdd9/Ne++9t0b9okWLKCtb86/yyJEjeeGFF3j55Zd54403OOqoo9h3333p379/fZvZs2fzpz/9iZ133nmN/hdeeCGjRo1a67zWtl9JkiRJJU06wx0RHSPi5oh4NiKeiYhPRcTIiJgXEdOybUCD9hdHxKyIeC4i+mVlbSPi0YiYHhEzI+LHDdrvHhGPZH3+GBGtN/2hSlunQYMGccIJJ9ClS5cN6jd+/Hh++MMf0qlTJ/bZZx++9rWv8bvf/W6VNueeey5XXHEFrVv7V06SJEna1Jp6SfkvgbtSSn2AA4BnsvKrUkoHZtsUgIjYFzgFqAL6A2MioiWwHPhcSukA4ECgf0Qcmo1zRTbWnsBC4OwPf2jS9mHXXXelV69efOUrX2H+/PkALFy4kNdff50DDjigvt0BBxzAzJkz69//6U9/ok2bNgwYMGCNMQHGjBlD586dOeigg5g0aVKT9itJkiTp39YbuCOiEjgCuBYgpfR+SmnROrocD0xMKS1PKb0EzAIOSSU1WZtW2ZYiIoDPATdndeOBEzbiWKTtSteuXXnsscd4+eWXefzxx1m8eDGnnXYaADU1pb9qlZWV9e0rKytZvHgxAIsXL+b73/8+v/zlLxsd+5vf/CYvvPACb731FpdddhlDhw7loYceWu9+JUmSJP1bU85w7w68Dfw2Ip6MiGsiol1Wd15EPBUR10VEp6ysJ/Bqg/5zszIiomVETAPeAu5NKT0CdAEWpZRqV2+/uogYFhFTI2JqbW3dBhymtO2pqKigb9++lJWVsdNOOzF69GjuueceFi9eTEVFBQDV1dX17aurq2nfvj1Qur/7jDPOYLfddmt07E984hN06dKFsrIyBgwYwGmnncYtt9yy3v1KkiRJm1DXlfkv24Y194Q2VFMCdxnwCWBsSunjwBKgAIwF9qB0efjrwH+vb6CUUl1K6UCgF3BIROy3IZNNKY1LKfVNKfUtK2u5IV2lbV7pYhFYsWIFnTp1Yuedd2b69On19dOnT6eqqgqA++67j6uvvpru3bvTvXt3Xn31VU466SSuuOKKtY6dUlrvfiVJkqRNaP7K/Jdt45p7QhuqKYF7LjA3OxsNpUu/P5FSejML0CuA3wCHZPXzgF0a9O+VldXLLkn/K6V7vN8BOkZE2draS9uz2tpali1bRl1dHXV1dSxbtoza2loeeeQRnnvuOVasWME777zDN7/5TY488sj6y8jPPPNMRo0axcKFC3n22Wf5zW9+U/+1Yvfddx8zZsxg2rRpTJs2jR49evDrX/+ac889F4Cbb76ZmpoaVqxYwT333MP111/PwIEDAda7X0mSJEkl6w3cKaU3gFcjYu+s6Gjg6Yho+D1CXwRmZK8nA6dERJuI2B3oDTwaEd0ioiNARJQD/wE8m0qnzf4KfDnrPwS4/cMdlrTtGDVqFOXl5RSLRa6//nrKy8sZNWoUL774Iv3796d9+/bst99+tGnThhtvvLG+349//GP22GMPdt11Vz772c/yve99r/4rwbp06VJ/drt79+60bNmSTp061V+K/stf/pKePXvSsWNHvve97/Gb3/yGI488EmC9+5UkSZJU0tQv0T0fuCH7uq4Xga8AV0fEgUAC5gBfB0gpzYyIm4CngVrg3JRSXRbQx2dPLG8B3JRS+nM2/kXAxIgYBTxJ9oA2SaX7rUeOHNlo3eDBg9far02bNlx33XVcd911693HnDlzVnn/4IMPrrXt4MGD17lfSZIkSSVNCtwppWlA39WKz1hH+58AP1mt7Cng42tp/yL/viRdkiRJkqStXlO/h1uSJEmSJG0AA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOShr7glsrO7tWjBx2KeaexraDO6//36OPNK1liRJkrR18Qy3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk52Gq/h/uNJSs4ZdzDzT2NLYrfSy5JkiRJWw7PcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA/c26Mgjj6Rt27ZUVFRQUVHB3nvvXV/39ttvc+qpp1JZWUmnTp047bTT6usuvPBCdtllFzp06MCuu+7K5ZdfXl/3/PPPc/zxx9OtWzc6d+5Mv379eO6551bZ71VXXUX37t3p0KEDZ511FsuXL8//YCVJkiRpC7XewB0Re0fEtAZbdUR8OyI6R8S9EfFC9menrH1ExNURMSsinoqITzQYa0jW/oWIGNKg/KCI+FfW5+qIiHwOd/sxevRoampqqKmpWSUYDxo0iO7du/PKK6/w1ltv8d3vfre+7uyzz+bZZ5+lurqaf/zjH9xwww3ccsstACxatIiBAwfy3HPP8eabb3LIIYdw/PHH1/e9++67KRaL3Hfffbz88su8+OKLXHLJJZvvgCVJkiRpC7PewJ1Sei6ldGBK6UDgIGApcCtQAO5LKfUG7sveAxwH9M62YcBYgIjoDFwCfBI4BLhkZUjP2nytQb/+m+LgtKp77rmHV199lSuvvJLKykpatWrFxz/+8fr6vffem3bt2tW/b9GiBbNmzQLgkEMO4eyzz6Zz5860atWK73znOzz33HO88847AIwfP56zzz6bqqoqOnXqxA9/+EN+97vfbdbjkyRJkqQtyYZeUn40MDul9DJwPDA+Kx8PnJC9Ph6YkEr+CXSMiJ2BfsC9KaUFKaWFwL1A/6yuQ0rpnymlBExoMJY20sUXX0zXrl057LDDuP/++wH45z//yd57782QIUPo0qULBx98MA888MAq/YrFIhUVFfTq1YslS5Zw6qmnNjr+3/72N7p3706XLl0AmDlzJgcccEB9/QEHHMCbb75ZH8glSZIkaXuzoYH7FODG7PVOKaXXs9dvADtlr3sCrzboMzcrW1f53EbKtZGuuOIKXnzxRebNm8ewYcP4whe+wOzZs5k7dy733HMPRx11FG+88Qb/9V//xfHHH8/8+fPr+xYKBRYvXswTTzzBGWecQWVl5Rrjz507l3PPPZef//zn9WU1NTWrtF35evHixTkeqSRJkiRtuZocuCOiNTAQ+NPqddmZ6bQJ57W2OQyLiKkRMbW2ti7v3W21PvnJT9K+fXvatGnDkCFDOOyww5gyZQrl5eXstttunH322bRq1YpTTjmFXXbZhYceemiV/hHBxz/+ccrLy9e4D/vtt9/m2GOPZfjw4QwePLi+vKKigurq6vr3K1+3b98+xyOVJEmStA3rujL/Zduw5p7QhtqQM9zHAU+klN7M3r+ZXQ5O9udbWfk8YJcG/XplZesq79VI+RpSSuNSSn1TSn3LylpuwNS3bxFBSon999+f1Z9Ht67n09XW1jJ79uz69wsXLuTYY49l4MCB/OAHP1ilbVVVFdOnT69/P336dHbaaaf6S84lSZIkaQPNX5n/sm1cc09oQ21I4B7Mvy8nB5gMrHzS+BDg9gblZ2ZPKz8UeDe79Pxu4NiI6JQ9LO1Y4O6srjoiDs2eTn5mg7G0gRYtWsTdd9/NsmXLqK2t5YYbbuBvf/sb/fv354tf/CILFy5k/Pjx1NXVcfPNNzN37lwOO+wwVqxYwa9//WsWLlxISolHH32UX/3qVxx99NFA6Yx1v379OOywwygWi2vs98wzz+Taa6/l6aefZtGiRYwaNYqhQ4du5qOXJEmSpC1HWVMaRUQ74D+ArzcoLgI3RcTZwMvASVn5FGAAMIvSE82/ApBSWhARlwGPZe0uTSktyF4PB34HlAN3Zps2wgcffMCIESN49tlnadmyJX369OG2225jr732AmDy5MkMHz6cc889lz59+nD77bfTtWtXVqxYwa233srFF1/M+++/T48ePTj//PM5//zzAbj11lt57LHHmDlz5ipPH3/66af5yEc+Qv/+/bnwwgs56qijeO+99/jSl77Ej3/84+b4CCRJkiRpi9CkwJ1SWgJ0Wa3sHUpPLV+9bQLOXcs41wHXNVI+FdivKXPRunXr1o3HHntsrfWHH344//rXv9Yob9GiBXfdddda+w0ZMoQhQ4astR7gggsu4IILLmj6ZCVJkiRpG7ahTymXJEmSJElNYOCWJEmSJCkHBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmSclDW3BPYWN3btWDisE819zQkSZIkSWqUZ7glSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScrBVvs93G8sWcEp4x5u7mmsk98TLkmSJEnbL89wS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcG8Gp59+OjvvvDMdOnRgr7324pprrgHg6aefpm/fvnTq1IlOnTpxzDHH8PTTT9f3u+qqq/joRz9Khw4d6NGjB9/5zneora0F4K233mLw4MH06NGDyspKDjvsMB555JH6viklfvKTn/CRj3yEDh06cMopp1BdXb15D1ySJEmStmMG7s3g4osvZs6cOVRXVzN58mRGjBjB448/To8ePbj55ptZsGAB8+fPZ+DAgZxyyin1/QYOHMgTTzxBdXU1M2bMYPr06Vx99dUA1NTUcPDBB/P444+zYMEChgwZwuc//3lqamoAmDBhAr///e956KGHeO2113jvvfc4//zzm+X4JUmSJGl71KTAHRHfiYiZETEjIm6MiLYRsXtEPBIRsyLijxHROmvbJns/K6vfrcE4F2flz0VEvwbl/bOyWRFR2ORH2cyqqqpo06YNABFBRDB79mw6duzIbrvtRkSQUqJly5bMmjWrvt8ee+xBx44dgdIZ6xYtWtTXf/SjH+WCCy5g5513pmXLlgwbNoz333+f5557DoA77riDs88+m1122YWKigouuugi/vjHP7J06dLNe/CSJEmStJ1ab+COiJ7AN4G+KaX9gJbAKcAVwFUppT2BhcDZWZezgYVZ+VVZOyJi36xfFdAfGBMRLSOiJfAr4DhgX2Bw1nabMnz4cHbYYQf69OnDzjvvzIABA+rrOnbsSNu2bTn//PP5/ve/v0q/P/zhD3To0IGuXbsyffp0vv71rzc6/rRp03j//ffZc88968tSSqu8Xr58OS+88MImPjJJkiRJUmOaekl5GVAeEWXADsDrwOeAm7P68cAJ2evjs/dk9UdHRGTlE1NKy1NKLwGzgEOybVZK6cWU0vvAxKztNmXMmDEsXryYBx98kEGDBtWf8QZYtGgR7777LqNHj+bjH//4Kv1OPfVUqquref755znnnHPYaaed1hi7urqaM844g0suuYTKykoA+vfvzzXXXMOcOXN49913ueKKKwA8wy1JkiRJm8l6A3dKaR7wM+AVSkH7XeBxYFFKqTZrNhfomb3uCbya9a3N2ndpWL5an7WVryEihkXE1IiYWltb15Tj26K0bNmSz3zmM8ydO5exY8euUteuXTvOOecczjzzTN566601+vbu3ZuqqiqGDx++Svl7773HF77wBQ499FAuvvji+vKzzjqLwYMHc+SRR1JVVcVRRx0FQK9evXI4MkmSJEna5LquzH/ZNqy5J7ShmnJJeSdKZ5x3B3oA7ShdEr7ZpZTGpZT6ppT6lpW1bI4pbBK1tbXMnj17jfIVK1awdOlS5s2b16R+y5cv54QTTqBXr178+te/XqVtixYt+PGPf8ycOXOYO3cuVVVV9OzZk549G/1dhiRJkiRtaeavzH/ZNq65J7ShmnJJ+THASymlt1NKHwC3AIcBHbNLzAF6AStT4jxgF4CsvhJ4p2H5an3WVr5NeOutt5g4cSI1NTXU1dVx9913c+ONN3L00Udz77338uSTT1JXV0d1dTUXXHABnTp1Yp999gHgmmuuqT/b/fTTT/PTn/6Uo48+GoAPPviAL3/5y5SXlzN+/HhatFh1KRcsWMDs2bNJKfH0009zwQUX8KMf/WiNdpIkSZKkfDQlfb0CHBoRO2T3Yh8NPA38Ffhy1mYIcHv2enL2nqz+/1Lp6V2TgVOyp5jvDvQGHgUeA3pnTz1vTenBapM//KFtGSKCsWPH0qtXLzp16sR3v/tdfvGLXzBw4EAWLVrE4MGDqaysZI899mD27NncddddtG3bFoCHHnqIj33sY7Rr144BAwYwYMAALr/8cgD+8Y9/8Oc//5l77rmHjh07UlFRQUVFBQ8++CAA8+fPZ8CAAbRr147jjjuOs846i2HDtrorMCRJkiRpq1W2vgYppUci4mbgCaAWeBIYB/wvMDEiRmVl12ZdrgV+HxGzgAWUAjQppZkRcROlsF4LnJtSqgOIiPOAuyk9Af26lNLMTXeIzatbt2488MADjdadeOKJnHjiiWvt+9vf/natdZ/97GdXeQr56vbaa6/6rwiTJEmSJG1+6w3cACmlS4BLVit+kdITxldvuwxoNEWmlH4C/KSR8inAlKbMRZIkSZKkrYE39EqSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5KGvuCWys7u1aMHHYp5p7GpIkSZIkNcoz3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5cDALUmSJElSDsqaewIb640lKzhl3MObZKyJwz61ScaRJEmSJGklz3BLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwZ0aPHk3fvn1p06YNQ4cOrS+fM2cOEUFFRUX9dtlll9XXf/e736V37960b9+ePn36MGHChEbHnzBhAhHBNddcU182cuRIWrVqtcrYL774Ym7HKEmSJEnafJoUuCPiWxExIyJmRsS3s7IDIuLhiPhXRNwRER0atL84ImZFxHMR0a9Bef+sbFZEFBqU7x4Rj2Tlf4yI1pvwGJukR48ejBgxgrPOOqvR+kWLFlFTU0NNTQ0//OEP68vbtWvHHXfcwbvvvsv48eP51re+xT/+8Y9V+i5cuJDLL7+cqqqqNcY9+eST68etqanhox/96KY9MEmSJElSs1hv4I6I/YCvAYcABwD/GRF7AtcAhZTSx4Bbge9l7fcFTgGqgP7AmIhoGREtgV8BxwH7AoOztgBXAFellPYEFgJnb7pDbJpBgwZxwgkn0KVLlw3q9+Mf/5g+ffrQokULPvnJT3L44Yfz8MMPr9Lm4osv5pvf/CZdu3bdlFOWJEmSJG3BmnKGex/gkZTS0pRSLfAAMAjYC/hb1uZe4EvZ6+OBiSml5Smll4BZlML6IcCslNKLKaX3gYnA8RERwOeAm7P+44ETPvSRbWK77rorvXr14itf+Qrz589vtM17773HY489tsqZ7EcffZSpU6dyzjnnNNrnjjvuoHPnzlRVVTF27Nhc5i5JkiRJ2vyaErhnAIdHRJeI2AEYAOwCzKQUrgFOzMoAegKvNug/NytbW3kXYFEW5huWbxG6du3KY489xssvv8zjjz/O4sWLOe200xpte84553DAAQfQr1/pKvq6ujqGDx/O6NGjadFizY/6pJNO4plnnuHtt9/mN7/5DZdeeik33nhjrscjSZIkSdo81hu4U0rPULrk+x7gLmAaUAecBQyPiMeB9sD7+U2zJCKGRcTUiJhaW1uX9+4AqKiooG/fvpSVlbHTTjsxevRo7rnnHhYvXrxKu+9973vMmDGDm266idJJexgzZgz7778/hx56aKNj77vvvvTo0YOWLVvy6U9/mm9961vcfPPNjbaVJEmSpO1M15X5L9uGNfeENlRZUxqllK4FrgWIiMuBuSmlZ4Fjs7K9gM9nzefx77PdAL2yMtZS/g7QMSLKsrPcDduvPo9xwDiAyl69U1PmvqmtDNMrVqyoL7vkkku48847eeCBB+jQof7Zcdx333088MADTJkyBYAFCxbw5JNPMm3aNEaPHt3o2Ck1y2FJkiRJ0pZmfkqpb3NP4sNoUuCOiB1TSm9FxEco3b99aIOyFsAI4H+y5pOBP0TEz4EeQG/gUSCA3hGxO6VAfQpwakopRcRfgS9Tuq97CHD7pjvEpqmtraW2tpa6ujrq6upYtmwZZWVlPP7443Ts2JHevXuzcOFCvvnNb3LkkUdSWVkJwE9/+lP+8Ic/8OCDD67xwLXf/e53LFu2rP79oEGD+PKXv8zZZ5eeCXf77bdzxBFH0LFjRx577DGuvvpqLr/88s130JIkSZKk3DT1e7gnRcTTwB3AuSmlRZSeMv488CzwGvBbgJTSTOAm4GlKl6Cfm1Kqy85enwfcDTwD3JS1BbgIuCAiZlG6p/vaTXFwG2LUqFGUl5dTLBa5/vrrKS8vZ9SoUbz44ov079+f9u3bs99++9GmTZtV7rP+/ve/zyuvvMKee+5Z/13aK0Nzx44d6d69e/3WunVrOnToUB/WJ06cyJ577kn79u0588wzueiiixgyZMjmPnRJkiRJUg6aekn54Y2U/RL45Vra/wT4SSPlU4ApjZS/SOkp5s1m5MiRjBw5stG6wYMHr7XfhlwCfv/996/y3gekSZIkSdK2q6lnuCVJkiRJ0gYwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5KGvuCWys7u1aMHHYp5p7GpIkSZIkNcoz3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5WCr/R7uN5as4JRxD29QH7+3W5IkSZK0uXiGW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBxsd4F79OjR9O3blzZt2jB06NBV6u677z769OnDDjvswFFHHcXLL79cXzd06FBat25NRUVF/VZXVwfA+++/z5e//GV22203IoL7779/lXGvuuoqPvrRj9KhQwd69OjBd77zHWpra/M+VEmSJElSM1pv4I6IvSNiWoOtOiK+HRGdI+LeiHgh+7NT1j4i4uqImBURT0XEJ7LyXSPiiWyMmRFxTla+Q0T8b0Q8m5UX8zzgHj16MGLECM4666xVyufPn8+gQYO47LLLWLBgAX379uXkk09epc2FF15ITU1N/dayZcv6us985jNcf/31dO/efY19Dhw4kCeeeILq6mpmzJjB9OnTufrqq/M5QEmSJEnSFqFsfQ1SSs8BBwJEREtgHnArUADuSykVI6KQvb8IOA7onW2fBMZmf74OfCqltDwiKoAZETEZWAT8LKX014hoDdwXEcellO7cpEeaGTRoEABTp05l7ty59eW33HILVVVVnHjiiQCMHDmSrl278uyzz9KnT591jtm6dWu+/e1vA6wSwlfaY4896l+nlGjRogWzZs36sIciSZIkSdqCbegl5UcDs1NKLwPHA+Oz8vHACdnr44EJqeSfQMeI2Dml9H5KaXnWps3KfaeUlqaU/pq9fh94Aui1sQe0sWbOnMkBBxxQ/75du3bssccezJw5s75szJgxdO7cmYMOOohJkyZt0Ph/+MMf6NChA127dmX69Ol8/etf32RzlyRJkiRteTY0cJ8C3Ji93iml9Hr2+g1gp+x1T+DVBn3mZmVExC4R8VRWf0VK6bWGg0dER+ALwH0bOK8PraamhsrKylXKKisrWbx4MQDf/OY3eeGFF3jrrbe47LLLGDp0KA899FCTxz/11FOprq7m+eef55xzzmGnnXZafydJkiRJ0laryYE7u9x7IPCn1etSSglI6xsjpfRqSml/YE9gSETUp86IKKMU5q9OKb24ljkMi4ipETG1trauqVNvkoqKCqqrq1cpq66upn379gB84hOfoEuXLpSVlTFgwABOO+00brnllg3eT+/evamqqmL48OGbZN6SJEmStI3qujL/Zduw5p7QhtqQM9zHAU+klN7M3r8ZETsDZH++lZXPA3Zp0K9XVlYvO7M9Azi8QfE44IWU0i/WNoGU0riUUt+UUt+ysjXvlf4wqqqqmD59ev37JUuWMHv2bKqqqhptHxGUfs+w4Wpra5k9e/ZG9ZUkSZKk7cT8lfkv28Y194Q21IYE7sH8+3JygMnAkOz1EOD2BuVnZk8rPxR4N6X0ekT0iohygOyJ5p8BnsvejwIqgW9v7IE0VW1tLcuWLaOuro66ujqWLVtGbW0tX/ziF5kxYwaTJk1i2bJlXHrppey///71D0y7+eabqampYcWKFdxzzz1cf/31DBw4sH7c5cuXs2zZMqD0NWHLli2rD+TXXHMNb71V+n3E008/zU9/+lOOPvrovA9VkiRJktSMmhS4I6Id8B9Aw2uoi8B/RMQLwDHZe4ApwIvALOA3wMprp/cBHomI6cADlJ5M/q+I6AX8ANgXWPm1YV/9cIe1dqNGjaK8vJxiscj1119PeXk5o0aNolu3bkyaNIkf/OAHdOrUiUceeYSJEyfW9/vlL39Jz5496dixI9/73vf4zW9+w5FHHllfv/fee1NeXs68efPo168f5eXl9d/j/dBDD/Gxj32Mdu3aMWDAAAYMGMDll1+e1yFKkiRJkrYA6/1aMICU0hKgy2pl71B6avnqbRNwbiPl9wL7N1I+F4gmzvdDGzlyJCNHjmy07phjjuHZZ59ttO7BBx9c57hz5sxZa91vf/vbpk5PkiRJkrSN2NCnlEuSJEmSpCYwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5KGvuCWys7u1aMHHYp5p7GpIkSZIkNcoz3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5WCr/R7uN5as4JRxD6+3nd/VLUmSJElqDp7hliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHBm5JkiRJknKw3QTuOXPmMGDAADp16kT37t0577zzqK2t5fnnn+f444+nW7dudO7cmX79+vHcc8/V95sxYwb9+vWja9euRMQa455++unsvPPOdOjQgb322otrrrlmcx6WJEmSJGkLtd0E7uHDh7Pjjjvy+uuvM23aNB544AHGjBnDokWLGDhwIM899xxvvvkmhxxyCMcff3x9v1atWnHSSSdx7bXXNjruxRdfzJw5c6iurmby5MmMGDGCxx9/fHMdliRJkiRpC9WkwB0R34mImRExIyJujIi2EXF0RDwREdMi4u8RsWfWtk1E/DEiZkXEIxGxW4NxLs7Kn4uIfg3K+2dlsyKisMmPEnjppZc46aSTaNu2Ld27d6d///7MnDmTQw45hLPPPpvOnTvTqlUrvvOd7/Dcc8/xzjvvALD33ntz9tlnU1VV1ei4VVVVtGnTZuVxEBHMnj07j0OQJEmSJG1F1hu4I6In8E2gb0ppP6AlcAowFjgtpXQg8AdgRNblbGBhSmlP4CrgimycfbN+VUB/YExEtIyIlsCvgOOAfYHBWdtN6tvf/jYTJ05k6dKlzJs3jzvvvJP+/fuv0e5vf/sb3bt3p0uXLk0ee/jw4eywww706dOHnXfemQEDBmzKqUuSJEmStkJNvaS8DCiPiDJgB+A1IAEdsvrKrAzgeGB89vpm4Ogo3fx8PDAxpbQ8pfQSMAs4JNtmpZReTCm9D0zM2m5SRxxxBDNnzqRDhw706tWLvn37csIJJ6zSZu7cuZx77rn8/Oc/36Cxx4wZw+LFi3nwwQcZNGhQ/RlvSZIkSdL2a72BO6U0D/gZ8ArwOvBuSuke4KvAlIiYC5wBFLMuPYFXs761wLtAl4blmblZ2drK1xARwyJiakRMra2ta+oxsmLFCvr378+gQYNYsmQJ8+fPZ+HChVx00UX1bd5++22OPfZYhg8fzuDBg5s89kotW7bkM5/5DHPnzmXs2LEb3F+SJEmStIquK/Nftg1r7gltqKZcUt6J0hnn3YEeQLuIOB34DjAgpdQL+C2wYaeFN0JKaVxKqW9KqW9ZWcsm91uwYAGvvPIK5513Hm3atKFLly585StfYcqUKQAsXLiQY489loEDB/KDH/zgQ82xtrbWe7glSZIk6cObvzL/Zdu45p7QhmrKJeXHAC+llN5OKX0A3AIcBhyQUnoka/NH4NPZ63nALgDZJeiVwDsNyzO9srK1lW8yXbt2Zffdd2fs2LHU1tayaNEixo8fz/777091dTX9+vXjsMMOo1gsrtE3pcSyZct4//33AVi2bBnLly8H4K233mLixInU1NRQV1fH3XffzY033sjRRx+9KacvSZIkSdoKNSVwvwIcGhE7ZPdiHw08DVRGxF5Zm/8AnsleTwaGZK+/DPxfSill5adkTzHfHegNPAo8BvSOiN0jojWlB6tN3gTHtopbbrmFu+66i27durHnnnvSqlUrrrrqKm699VYee+wxfvvb31JRUVG/vfLKKwC8/PLLlJeX1z+lvLy8nL333hsoPZV87Nix9OrVi06dOvHd736XX/ziFwwcOHBTT1+SJEmStJUpW1+DlNIjEXEz8ARQCzwJjKN0r/WkiFgBLATOyrpcC/w+ImYBCygFaFJKMyPiJkphvRY4N6VUBxAR5wF3U3oC+nUppZmb7hBLDjzwQO6///41yocMGcKQIUPW7JDZbbfdKP2+YE3dunXjgQce2FRTlCRJkiRtQ9YbuAFSSpcAl6xWfGu2rd52GXDiWsb5CfCTRsqnAFOaMhdJkiRJkrYGTf1aMEmSJEmStAEM3JIkSZIk5cDALUmSJElSDgzckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5cDALUmSJElSDgzckiRJkiTloKy5J7CxurdrwcRhn2ruaUiSJEmS1CjPcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOShr7glsrDeWrOCUcQ/Xv5847FPNOBtJkiRJklblGW5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpB9tc4D799NPZeeed6dChA3vttRfXXHNNfd1NN93EPvvsQ/v27dl333257bbb6uvGjx/PQQcdRIcOHejVqxcXXnghtbW19fULFizgi1/8Iu3atWPXXXflD3/4w+Y8LEmSJEnSVqZJgTsivhURMyJiZkR8OyvrHBH3RsQL2Z+dsvKIiKsjYlZEPBURn2gwzpCs/QsRMaRB+UER8a+sz9URERt7QBdffDFz5syhurqayZMnM2LECB5//HHmzZvH6aefzs9//nOqq6u58sorOfXUU3nrrbcAWLp0Kb/4xS+YP38+jzzyCPfddx8/+9nP6sc999xzad26NW+++SY33HAD3/jGN5g5c+bGTlOSJEmStI1bb+COiP2ArwGHAAcA/xkRewIF4L6UUm/gvuw9wHFA72wbBozNxukMXAJ8MhvrkpUhPWvztQb9+m/sAVVVVdGmTZuVcycimD17NnPnzqVjx44cd9xxRASf//znadeuHbNnzwbgG9/4BocffjitW7emZ8+enHbaaTz00EMALFmyhEmTJnHZZZdRUVHBZz7zGQYOHMjvf//7jZ2mJEmSJGkb15Qz3PsAj6SUlqaUaoEHgEHA8cD4rM144ITs9fHAhFTyT6BjROwM9APuTSktSCktBO4F+md1HVJK/0wpJWBCg7E2yvDhw9lhhx3o06cPO++8MwMGDKBv377ss88+TJ48mbq6Om677TbatGnD/vvv3+gYf/vb36iqqgLg+eefp6ysjL322qu+/oADDvAMtyRJkiRprcqa0GYG8JOI6AK8BwwApgI7pZRez9q8AeyUve4JvNqg/9ysbF3lcxsp32hjxozh//2//8fDDz/M/fffT5s2bWjZsiVnnnkmp556KsuWLaN169b86U9/ol27dmv0v+6665g6dWr9/d81NTV06NBhlTaVlZUsXrz4w0xTkiRJkrQNW+8Z7pTSM8AVwD3AXcA0oG61NglIOcxvFRExLCKmRsTU2tq6dbZt2bIln/nMZ5g7dy5jx47lL3/5CxdeeCH3338/77//Pg888ABf/epXmTZt2ir9brvtNi6++GLuvPNOunbtCkBFRQXV1dWrtKuurqZ9+/ab9PgkSZIkSfW6rsx/2TasuSe0oZr00LSU0rUppYNSSkcAC4HngTezy8HJ/nwraz4P2KVB915Z2brKezVS3tg8xqWU+qaU+paVtWzK1KmtrWX27NlMmzaNI444gr59+9KiRQsOPvhgPvnJT/KXv/ylvu1dd93F1772Ne644w4+9rGP1Zfvtdde1NbW8sILL9SXTZ8+vf6Sc0mSJEnSJjd/Zf7LtnHNPaEN1dSnlO+Y/fkRSvdv/wGYDKx80vgQ4Pbs9WTgzOxp5YcC72aXnt8NHBsRnbKHpR0L3J3VVUfEodnTyc9sMNYGeeutt5g4cSI1NTXU1dVx9913c+ONN3L00Udz8MEH8+CDD9af0X7yySd58MEH6+/h/r//+z9OO+00Jk2axCGHHLLKuO3atWPQoEH86Ec/YsmSJTz00EPcfvvtnHHGGRszTUmSJEnSdqAp93ADTMru4f4AODeltCgiisBNEXE28DJwUtZ2CqX7vGcBS4GvAKSUFkTEZcBjWbtLU0oLstfDgd8B5cCd2bbBIoKxY8dyzjnnsGLFCnbddVd+8YtfMHDgQABGjhzJl7/8Zd588026devG97//fY499lgALrvsMt59910GDBhQP97hhx/OnXeWpjJmzBjOOussdtxxR7p06cLYsWM9wy1JkiRJWqsmBe6U0uGNlL0DHN1IeQLOXcs41wHXNVI+FdivKXNZl27duvHAAw+stf68887jvPPOa7Tur3/96zrH7ty5M7fddtuHmZ4kSZIkaTvSpEvKJUmSJEnShjFwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkoa+4JbKzu7VowcdinmnsakiRJkiQ1yjPckiRJkiTlwMAtSZIkSVIODNySJEmSJOXAwC1JkiRJUg4M3JIkSZIk5cDALUmSJElSDgzckiRJkiTlYKsN3G8sWcEp4x5u7mlIkiRJktSorTZwS5IkSZK0JTNwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTnYJgL36NGj6du3L23atGHo0KGr1C1dupThw4fTtWtXKisrOeKII+rrli9fzjnnnMNOO+1E586d+cIXvsC8efPq6+fMmcOAAQPo1KkT3bt357zzzqO2tnZzHZYkSZIkaSu23sAdEXtHxLQGW3VEfLtB/X9FRIqIrtn7iIirI2JWRDwVEZ9o0HZIRLyQbUMalB8UEf/K+lwdEbEhB9GjRw9GjBjBWWedtUbdsGHDWLBgAc888wwLFizgqquuqq/75S9/ycMPP8xTTz3Fa6+9RqdOnTj//PPr64cPH86OO+7I66+/zrRp03jggQcYM2bMhkxNkiRJkrSdKltfg5TSc8CBABHREpgH3Jq93wU4FnilQZfjgN7Z9klgLPDJiOgMXAL0BRLweERMTiktzNp8DXgEmAL0B+5s6kEMGjQIgKlTpzJ37tz68meffZbJkyczd+5cOnToAMBBBx1UX//SSy/Rr18/dtppJwBOPvlkLrjgglXqzzvvPNq2bUv37t3p378/M2fObOq0JEmSJEnbsQ29pPxoYHZK6eXs/VXAhZQC9ErHAxNSyT+BjhGxM9APuDeltCAL2fcC/bO6Dimlf6aUEjABOGHjD+nfHn30UXbddVcuueQSunbtysc+9jEmTZpUX3/22Wfz0EMP8dprr7F06VJuuOEGjjvuuPr6b3/720ycOJGlS5cyb9487rzzTvr3778ppiZJkiRJ2sZtaOA+BbgRICKOB+allKav1qYn8GqD93OzsnWVz22kfA0RMSwipkbE1NrauvVOdu7cucyYMYPKykpee+01Ro8ezZAhQ3jmmWcA6N27N7vssgs9e/akQ4cOPPPMM/zoRz+q73/EEUcwc+ZMOnToQK9evejbty8nnHDCevcrSZIkSfrQuq7Mf9k2rLkntKGaHLgjojUwEPhTROwAfB/40bp7bVoppXEppb4ppb5lZS3X2768vJxWrVoxYsQIWrduzWc/+1mOOuoo7rnnHgDOPfdcli9fzjvvvMOSJUsYNGhQ/RnuFStW0L9/fwYNGsSSJUuYP38+Cxcu5KKLLsr1GCVJkiRJAMxfmf+ybVxzT2hDbcgZ7uOAJ1JKbwJ7ALsD0yNiDtALeCIiulO6x3uXBv16ZWXrKu/VSPmHtv/++69R1vB5bNOmTWPo0KF07tyZNm3acP755/Poo48yf/58FixYwCuvvMJ5551HmzZt6NKlC1/5yleYMmXKppiaJEmSJGkbtyGBezDZ5eQppX+llHZMKe2WUtqN0mXgn0gpvQFMBs7MnlZ+KPBuSul14G7g2IjoFBGdKD1s7e6srjoiDs2eTn4mcPuGHERtbS3Lli2jrq6Ouro6li1bRm1tLUcccQQf+chH+OlPf0ptbS0PPfQQf/3rX+nXrx8ABx98MBMmTODdd9/lgw8+YMyYMfTo0YOuXbvStWtXdt99d8aOHUttbS2LFi1i/PjxjYZ4SZIkSZJW16TAHRHtgP8AbmlC8ynAi8As4DfAcICU0gLgMuCxbLs0KyNrc03WZzYb8IRygFGjRlFeXk6xWOT666+nvLycUaNG0apVK26//XamTJlCZWUlX/va15gwYQJ9+vQB4Gc/+xlt27ald+/edOvWjSlTpnDrrbfWj3vLLbdw11130a1bN/bcc09atWq1yteKSZIkSZK0Nuv9WjCAlNISoMs66ndr8DoB566l3XXAdY2UTwX2a8pcGjNy5EhGjhzZaF1VVRUPP/xwo3VdunThhhtuWOu4Bx54IPfff//GTkuSJEmStB3b0KeUS5IkSZKkJjBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTnYagN393YtmDjsU809DUmSJEmSGrXVBm5JkiRJkrZkBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHW23gfmPJiuaegiRJkiRJa7XVBm5JkiRJkrZkBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScmDgliRJkiQpBwZuSZIkSZJyYOCWJEmSJCkHBm5JkiRJknJg4JYkSZIkKQcGbkmSJEmScrDVB+7Ro0fTt29f2rRpw9ChQ1epu+++++jTpw877LADRx11FC+//HJ93YIFCzj55JPp0qULXbt25bTTTqO6urq+frfddqO8vJyKigoqKio49thjN9chSZIkSZK2AVt94O7RowcjRozgrLPOWqV8/vz5DBo0iMsuu4wFCxbQt29fTj755Pr6ESNGsHDhQl566SVmz57Nm2++yciRI1cZ44477qCmpoaamhruueeezXE4kiRJkqRtRJMCd0R8JyJmRsSMiLgxItpGyU8i4vmIeCYivpm1jYi4OiJmRcRTEfGJBuMMiYgXsm1Ig/KDIuJfWZ+rIyKaegCDBg3ihBNOoEuXLquU33LLLVRVVXHiiSfStm1bRo4cyfTp03n22WcBeOmllzjhhBPo0KEDlZWVfPGLX2TmzJlN3a0kSZIkSeu03sAdET2BbwJ9U0r7AS2BU4ChwC5An5TSPsDErMtxQO9sGwaMzcbpDFwCfBI4BLgkIjplfcYCX2vQr/+HPbCZM2dywAEH1L9v164de+yxR32oPvfcc/nzn//MwoULWbhwIZMmTeK4445bZYzTTjuNbt26ceyxxzJ9+vQPOyVJkiRJ0nakqZeUlwHlEVEG7AC8BnwDuDSltAIgpfRW1vZ4YEIq+SfQMSJ2BvoB96aUFqSUFgL3Av2zug4ppX+mlBIwATjhwx5YTU0NlZWVq5RVVlayePFiAD7xiU/w/vvv06VLF7p06ULLli0ZPnx4fdsbbriBOXPm8PLLL3PUUUfRr18/Fi1a9GGnJUmSJEnaTqw3cKeU5gE/A14BXgfeTSndA+wBnBwRUyPizojonXXpCbzaYIi5Wdm6yuc2Ur6GiBiW7W9qbW3dOuddUVGxykPQAKqrq2nfvj0AJ510EnvttReLFy+murqaPfbYg9NPP72+7WGHHUZ5eTk77LADF198MR07duTBBx9c5z4lSZIkSZtM15X5L9uGNfeENlRTLinvROms9e5AD6BdRJwOtAGWpZT6Ar8BrstzogAppXEppb4ppb5lZS3X2baqqmqVy8CXLFnC7NmzqaqqAmDatGl8/etfp127dlRUVHDOOecwZcqUtY4XEZROwEuSJEmSNoP5K/Nfto1r7gltqKZcUn4M8FJK6e2U0gfALcCnKZ2JviVrcyuwf/Z6HqV7u1fqlZWtq7xXI+VNUltby7Jly6irq6Ouro5ly5ZRW1vLF7/4RWbMmMGkSZNYtmwZl156Kfvvvz99+vQB4OCDD+aaa67hvffe47333mPcuHHsv3/pEF555RUeeugh3n//fZYtW8aVV17J/PnzOeyww5o6LUmSJEnSdq4pgfsV4NCI2CF7evjRwDPAbcBRWZvPAs9nrycDZ2ZPKz+U0iXorwN3A8dGRKfsrPmxwN1ZXXVEHJqNfyZwe1MPYNSoUZSXl1MsFrn++uspLy9n1KhRdOvWjUmTJvGDH/yATp068cgjjzBx4sT6ftdddx1z5syhV69e9OzZkxdffJHx48cDsHjxYr7xjW/QqVMnevbsyV133cWdd965xpPQJUmSJElam7L1NUgpPRIRNwNPALXAk8A4oBy4ISK+A9QAX826TAEGALOApcBXsnEWRMRlwGNZu0tTSguy18OB32Vj3pltTTJy5Mg1vj97pWOOOab+a8BWt/vuu3PHHXc0WldVVcVTTz3V1ClIkiRJkrSG9QZugJTSJZS+0quh5cDnG2mbgHPXMs51NHKvd0ppKrBfU+YiSZIkSdLWoKlfCyZJkiRJkjaAgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHGy1gbt7u6126pIkSZKk7YCpVZIkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHBi4JUmSJEnKgYFbkiRJkqQcGLglSZIkScqBgVuSJEmSpBwYuCVJkiRJyoGBW5IkSZKkHGzVgXv06NH07duXNm3aMHTo0FXq7rvvPvr06cMOO+zAUUcdxcsvv1xfd+GFF7LLLrvQoUMHdt11Vy6//PLNPHNJkiRJ0rauSYE7Ir4VETMiYmZEfDsruywinoqIaRFxT0T0yMojIq6OiFlZ/ScajDMkIl7ItiENyg+KiH9lfa6OiGjKvHr06MGIESM466yzVimfP38+gwYN4rLLLmPBggX07duXk08+ub7+7LPP5tlnn6W6upp//OMf3HDDDdxyyy1N2aUkSZIkSU2y3sAdEfsBXwMOAQ4A/jMi9gSuTCntn1I6EPgz8KOsy3FA72wbBozNxukMXAJ8MhvrkojolPUZm+1jZb/+TZn8oEGDOOGEE+jSpcsq5bfccgtVVVWceOKJtG3blpEjRzJ9+nSeffZZAPbee2/atWv37w+hRQtmzZrVlF1KkiRJktQkTTnDvQ/wSEppaUqpFngAGJRSqm7Qph2QstfHAxNSyT+BjhGxM9APuDeltCCltBC4F+if1XVIKf0zpZSACcAJH+agZs6cyQEHHPDvybVrxx577MHMmTPry4rFIhUVFfTq1YslS5Zw6qmnfphdSpIkSZK0iqYE7hnA4RHRJSJ2AAYAuwBExE8i4lXgNP59hrsn8GqD/nOzsnWVz22kfKPV1NRQWVm5SlllZSWLFy+uf18oFFi8eDFPPPEEZ5xxxhrtJUmSJEn6MNYbuFNKzwBXAPcAdwHTgLqs7gcppV2AG4Dz8ptmSUQMi4ipETH13XffXWu7iooKqqurVymrrq6mffv2q4/Hxz/+ccrLy7nkkktymbMkSZIkaaN0XZn/sm1Yc09oQzXpoWkppWtTSgellI4AFgLPr9bkBuBL2et5ZGfAM72ysnWV92qkvLF5jEsp9U0p9V3XGemqqiqmT59e/37JkiXMnj2bqqqqRtvX1tYye/bstY4nSZIkSdrs5q/Mf9k2rrkntKGa+pTyHbM/PwIMAv4QEb0bNDkeeDZ7PRk4M3ta+aHAuyml14G7gWMjolP2sLRjgbuzuuqIODR7OvmZwO1NmVdtbS3Lli2jrq6Ouro6li1bRm1tLV/84heZMWMGkyZNYtmyZVx66aXsv//+9OnThxUrVvDrX/+ahQsXklLi0Ucf5Ve/+hVHH310U3YpSZIkSVKTNPV7uCdFxNPAHcC5KaVFQDH7qrCnKIXnb2VtpwAvArOA3wDDAVJKC4DLgMey7dKsjKzNNVmf2cCdTZnUqFGjKC8vp1gscv3111NeXs6oUaPo1q0bkyZN4gc/+AGdOnXikUceYeLEifX9br31VvbYYw/at2/P6aefzvnnn8/555/fxI9CkiRJkqT1K2tKo5TS4Y2UfWktbRNw7lrqrgOua6R8KrBfU+bS0MiRIxk5cmSjdcccc0z914A11KJFC+66664N3ZUkSZIkSRukqWe4JUmSJEnSBjBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpSDSCk19xw2SllZ2eK6urrnmnseyt8OO+zQdenSpfObex7aPFzv7YdrvX1xvbcfrvX2xfXefjTTWu+aUuq2mfe5aaWUtsrtoosumtrcc3Bzrd1cbzfX2s31dnOtt+fN9d5+Ntd64zYvKZckSZIkKQcGbkmSJEmScrA1B+5xzT0BbTau9fbF9d5+uNbbF9d7++Fab19c7+2Ha70RttqHpkmSJEmStCXbms9wS5IkSZK0xTJwS5IkSZKUg7LmnsCGKhQKnYFrgWOB+cDFxWLxD807K61UKBTOA4YCHwNuLBaLQxvUHQ38CvgI8AgwtFgsvpzVtQHGAl8GlgL/X7FY/HnefbXxss99DHAM0BmYTenv451Zveu9DSkUCtcDRwPtgDcofe7XZHWu9TaqUCj0Bv4F3FwsFk/Pyk4Ffgp0Be4FzioWiwuyunX+jM6rrz6cQqFwP3AoUJsVzSsWi3tnda73NqZQKJwCXELp3843KP3b+aD/lm9bCoVCzWpF5cCYYrF4flbvem8mW+MZ7l8B7wM7AacBYwuFQlXzTkkNvAaMAq5rWFgoFLoCtwA/pBTOpgJ/bNBkJNAb2BU4CriwUCj03wx9tfHKgFeBzwKVwAjgpkKhsJvrvU36KbBbsVjsAAwERhUKhYNc623er4DHVr7Jft7+GjiD0s/hpZR+8dawfaM/o3Puqw/vvGKxWJFtK8O2672NKRQK/wFcAXwFaA8cAbzov+XbngZ/nyuA7sB7wJ/A/y/f3LaqwF0oFNoBXwJ+WCwWa4rF4t+ByZT+QdYWoFgs3lIsFm8D3lmtahAws1gs/qlYLC6j9JfxgEKh0CerHwJcViwWFxaLxWeA31A6U553X22kYrG4pFgsjiwWi3OKxeKKYrH4Z+Al4CBc721OsVicWSwWl2dvU7btgWu9zcrOgi0C7mtQfBpwR7FY/FuxWKyh9D9NgwqFQvsm/IzOpW+OH4Fc723Rj4FLi8XiP7Of3fOKxeI8/Ld8W/cl4C3gwey9670ZbVWBG9gLqC0Wi883KJsOeIZ7y1dFaa2AUlijdAlyVaFQ6ATs3LCeVdc1l76b5KhUr1Ao7ETp7+hMXO9tUqFQGFMoFJYCzwKvA1NwrbdJhUKhA3ApcMFqVat/7rMpnaXci/X/jM6rrzaNnxYKhfmFQuGhQqFwZFbmem9DCoVCS6Av0K1QKMwqFApzC4XC6EKhUI7/lm/rhgATisXiyq+ncr03o60tcFcA1auVvUvpkhht2SoorVVDK9euosH71evy7KtNpFAotAJuAMYXi8Vncb23ScVicTilz/JwSpeELce13lZdBlxbLBbnrla+vjVb18/ovPrqw7sI+CjQk9L37N5RKBT2wPXe1uwEtKJ0b+3hwIHAxyndEua/5duoQqGwK6Xb/8Y3KHa9N6Ot7aFpNUCH1co6AIubYS7aMOtau5oG75etVpdnX20ChUKhBfB7SmcfzsuKXe9tVLFYrAP+XigUTge+gWu9zSkUCgdSehjixxupXtfnvmIddXn21YdULBYfafB2fKFQGAwMwPXe1ryX/fn/isXi6wCFQuHnlAL33/Df8m3VGcDfi8XiSw3K/Nm9GW1tZ7ifB8qyp6audAClS1i1ZZtJaa2A+vvx96B0H8dCSpenHtCgfcN1zaXvJjmq7VyhUAhKT5ndCfhSsVj8IKtyvbd9Zfz7s3Wtty1HArsBrxQKhTeA7wJfKhQKT7Dm5/5RoA2ln8/r+xmdV19tegkIXO9tSvbv6lxK67vSytf+W77tOpNVz26D671ZbVVnuIvF4pJCoXALcGmhUPgqpUthjgc+3awTU71CoVBG6b+rlkDLQqHQltLXjNwKXFkoFL4E/C/wI+Cp7PJjgAnAiEKhMJVSePsapSdoknNffThjgX2AY4rF4nsNyl3vbUihUNgR+BzwZ0pnSI4BBmfbw7jW25pxwMQG779LKYB/A9gReLhQKBwOPEHpPu9bisXiYoD1/Iy+Ia++2niFQqEj8EngAUo/r0+m9OTqb1G6/Nj13rb8Fji/UCjcBXwAfIfSv+3+3N4GFQqFT1O6VeRPq1W53pvR1naGG2A4pe+Rewu4EfhGsVjcpn8rspUZQel/yAvA6dnrEcVi8W1KT0j8CbCQ0g/3Uxr0u4TSQxNepvRD/8pisXgXQM59tZGye4K+Tul/lN4oFAo12Xaa673NSZTC1lxKn+vPgG8Xi8XJrvW2p1gsLi0Wi2+s3ChdArisWCy+nf28PYdSIHqL0n13wxt0X+vP6Jz7auO1ovR1nm9T+j7s84ETisXi8673NukySl/19zzwDPAk8BP/Ld9mDaGRX1a53ptXpJTW30qSJEmSJG2QrfEMtyRJkiRJWzwDtyRJkiRJOTBwS5IkSZKUAwO3JEmSJEk5MHBLkiRJkpQDA7ckSZIkSTkwcEuSJEmSlAMDtyRJkiRJOTBwS5IkSZKUg/8fgjYXJOF3xOUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution(df, 'FORCE_2020_LITHOFACIES_LITHOLOGY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeed8c5",
   "metadata": {},
   "source": [
    "Através do gráfico acima nota-se um forte desbalanceamento no conjunto de dados, com uma classe predominante (`65000`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc835142",
   "metadata": {},
   "source": [
    "## Separação do dataset entre as variáveis preditoras `X` e variável predita `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4419865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df.pop('FORCE_2020_LITHOFACIES_LITHOLOGY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103d9fb",
   "metadata": {},
   "source": [
    "## Extração das features do dataset\n",
    "\n",
    "Para as colunas preditoras, os encoders utilizados serão armazenados em memória para reutilização na leitura e processamento do dataset `hidden`.\n",
    "\n",
    "Neste momento é que descartaremos as variáveis que apresentam diferentes manipulações delas mesmas, mencionadas anteriormente.\n",
    "\n",
    "**IMPORTANTE**: A variável categórica `FORMATION_encoded`, no conjunto de treino (`lithology.csv`) não contém o valor `12` em seu range de valores válidos, enquanto no conjunto `hidden.csv` ela existe. Consequentemente, não é possível, apenas considerando os dados do conjunto de treino fornecido, codificar apropriadamente esta variável. Seria tecnicamente possível, através de _data augmentation_, por exemplo, criar alguns registros _fakes_ no conjunto de treino, com o valor 12, para assim treinar o _encoder_ de modo a suportar este valor no conjunto `hidden.csv`. No entanto, isto provavelmente serviria apenas para \"iludir\" o modelo, uma vez que são dados oriundos de outros tipos de rochas. Portanto, esta variável não será utilizada neste experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4f608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "mms_columns = ['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
    "               'GR', 'NPHI', 'PEF', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC', \n",
    "               'Normalized_GR', 'Delta_DTC', 'Delta_RHOB', 'Delta_GR', 'Delta_DEPTH_MD',\n",
    "               'Delta_Carbon_Index']\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe_columns = ['GROUP_encoded']\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "def return_features(X_train, X_test, y_train, y_test):\n",
    "    '''Este método realiza os encodes necessários nas variáveis preditoras e predita\n",
    "    para que o modelo de rede neural possa interpretá-los adequadamente.\n",
    "    Neste caso são utilizados três encoders:\n",
    "    - OneHotEncoder - para variáveis categóricas nos conjuntos de variáveis preditoras\n",
    "    - MinMaxScaler - para variáveis contínuas nos conjuntos de variáveis preditoras\n",
    "    - LabelEncoder - para variáveis categóricas nos conjuntos da variável predita\n",
    "    '''    \n",
    "    \n",
    "    X_train_mms = mms.fit_transform(X_train[mms_columns])\n",
    "    X_test_mms = mms.transform(X_test[mms_columns])    \n",
    "    \n",
    "    X_train_ohe = ohe.fit_transform(X_train[ohe_columns])\n",
    "    X_test_ohe = ohe.transform(X_test[ohe_columns])\n",
    "    \n",
    "    X_train_processed = np.hstack([X_train_ohe, X_train_mms])\n",
    "    X_test_processed = np.hstack([X_test_ohe, X_test_mms])\n",
    "    \n",
    "    y_train_le = le.fit_transform(y_train)\n",
    "    y_test_le = le.transform(y_test)\n",
    "    \n",
    "    return X_train_processed, X_test_processed, y_train_le, y_test_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3567e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelencoder_mapping(le):\n",
    "    '''\n",
    "    Retorna um dicionário das labels mapeadas pelo LabelEncoder informado, linkadas ao seu código/ inteiro.\n",
    "    Isto permite que tenhamos a 'descrição' da classe, informando seu código.\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({le.transform([cl])[0]:cl})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb19ab",
   "metadata": {},
   "source": [
    "## Divisão do conjunto entre treino, teste e validação\n",
    "\n",
    "O conjunto de treino será dividido em três sub-conjuntos:\n",
    "- Treino: com (por padrão) 70% do volume total de registros.\n",
    "- Teste: com (por padrão) 75% dos 30% de dados restantes (após a divisão de treino) totalizando aproximadamente 22,5% do conjunto total.\n",
    "- Validação: com (por padrão) 25% dos 30% dos dados restantes (após a divisão de treino) totalizando aproximadamente 7,5% do conjunto total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1fc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y, train_size=0.7, valid_size=0.25):\n",
    "    '''Divisão do dataset de treino, teste e validação.\n",
    "    -> train_size: proporção do conjunto de treino em relação ao dataset completo\n",
    "    -> valid_size: proporção do conjunto de validação em relação ao dataset de teste \n",
    "    (diferença entre o dataset completo e o de treino)\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, shuffle=True, random_state=42, )\n",
    "    X_train, X_test, y_train, y_test = return_features(X_train, X_test, y_train, y_test)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=valid_size, shuffle=True, random_state=42)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aabcc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X, y, train_size=0.7, valid_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6e560",
   "metadata": {},
   "source": [
    "## Visualização da distribuição de classes nos subconjuntos\n",
    "\n",
    "A visualização ajuda a identificar se a distribuição entre estes subconjuntos está minimamente similar à distribuição do conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8480f8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8EAAAIbCAYAAADLmpz9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABGmUlEQVR4nO3de5xVdb3/8dcHhsvAAIIgiCh4x0jUwsyOKV5SUFPDS+IdOaHhtY6/3BUqKun2dE6lh/TIsRJF5ZiX0lIrO6HlHW28IHnBwPCOgIA4yMD398feMw3DcJs9zAys1/Px2I9mr7W+6/tZM19s3vP97rUipYQkSZIkSVnQpqULkCRJkiSpuRiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWpEaKiP+OiEua6FzbRcSSiGhbfD8tIv61kef6XkTctJ7H/iEi/lLs/57G9LeG8za6/rWc8+aImNCU58yaiPhyRLzS0nU0JCLGR8SUZu5zaETMrfN+RkQMXZ9jS+izd0TMiYiJEfG1iDi/1HNKkjZMWUsXIEmtUUTMBnoD1cAK4GXgFmBSSmklQErp7A0417+mlB5e0zEppTeBitKqrj3XVetZVw9gLjAFuBu4oin6V+uVUvozsGtTnCsibgbmppTGNcX5WoOU0qBm6GYI8AOgK3Ax8PVm6FOSVIchWJLW7KsppYcjohtwAHAtsA8wqik7iYiylFJ1U55zfaSU5vPPa9m7ufuXsiil9Ns6b/+jxQqRpAxzObQkrUNK6aOU0n0UZmxOj4jPwqrLcyOiZ0T8JiIWRsT8iPhzRLSJiFuB7YD7i8udvxMRAyIiRcToiHgT+L862+r+cXLHiHg6IhZFxK+LM7cNLsuMiNkRcUjx61WWlUbEfhHxeLG2f0TEGcXtR0TEX4vn/0dEjK93zqOKy0MXFpc377am71FEfCUi/hYRH0XERCDq7T8zImZGxIKI+F1E9F/LuRqst94x3Yvf7w+K5/xNRPSrs/+MiHgjIhZHxN8j4uTi9p0i4pFinfMi4n/rtBlYXB4+PyJeiYgT6uw7PCJeLp7vrYi4aA21n1FcXv4fxbr+HhHD6+zvGxH3Fft4PSK+sZbvQ3lE/Gdx6exHxfOWF/et8WdTHAsXRcQLxXb/GxEdi/vqL/9NEbFTnfd1x/TQiJgbEf8WEe9HxDsRMaq4bwxwMvCd4ri+v7h9t2I9C4v1HbWW69u++LNYHBF/AHrW2//FOuPg+VjzMuWLI+KuetuujYjril+PKo69xcUxcdZaaqr776i8+P1YEBEvU+8PRRGRi4hZxfO+HBFfq7f/G3X6fTkiPreudlH4b8a44s/8/Yi4JQp/hJMkNSFDsCStp5TS0xSWD3+5gd3/VtzXi8Iy6u8VmqRTgTcpzCpXpJT+vU6bA4DdgMPW0OVpwJnA1hSWZV+3oTVHIWw+CPxXsbY9gcri7o+LfWwBHAF8MyKOKbbbBbgDuLDY7gEKQb59A330BO4BxlEIMrOAf6mz/2gK348RxXP9uXjuDa23rjbAL4D+FP7I8AkwsXiOzhS+V8NTSl2AL9U5x5XA74HuQL9iPzVt/gDcDmwFnAhcHxGfKbb7GXBW8XyfBf6vofqL9gFeKX4v/h34WUTU/FFgKoVx0hc4DrgqIg5aw3n+A/h8sf4ewHeAlev5szkBGAZsDwwGzlhLvWvTB+gGbAOMBn4aEd1TSpOA24B/L47rr0ZEO+B+Ct/frYDzgNsiYk3Lr28HnqXwfboSOL1mR0RsA/wWmFC89ouAuyOiVwPnmQocHhFdim3bFq//9uL+94EjKSw/HgX8uCaQrsNlwI7F12F16yuaReG/Bd2Ay4EpEbF1sYbjgfEU/n11BY4CPlxXOwo/pzOAA4EdKHxEYuJ61CpJ2gCGYEnaMG9T+KW8vuUUwmr/lNLylNKfU0ppHecan1L6OKX0yRr235pSeiml9DFwCXBC8Rf8DXES8HBK6Y5iXR+mlCoBUkrTUkovppRWppReoBCsDii2+zrw25TSH1JKyykEsnIKgay+w4EZKaW7isf+BHi3zv6zgatTSjOLy76vAvaMhmeD11hvXcXtd6eUlqaUFlP4jOUBdQ5ZCXw2IspTSu+klGYUty+nEJz7ppSqUkp/KW4/EpidUvpFSqk6pfRXCp+TPr5Ou89ERNeU0oKU0nMN1F5jTkrpf1JKK4DJFMZF74jYlsIfBy4u9l0J3EQhKK0iItpQ+APIBSmlt1JKK1JKj6eUlrF+P5vrUkpvF5e830/hjwmNsRy4ovizeABYwpo/U/xFCqEtn1L6NKX0f8BvgJENXN92FGZWL0kpLUspPVqss8YpwAMppQeK4/MPwHQKY20VKaU5wHNAzYzqQcDSlNKTxf2/TSnNSgWPUAjpDf0hq74TgB+klOanlP5BvT9CpZR+Wfwer0wp/S/wGvCF4u5/pfAHgmeK/b5erHNd7U4GfpRSeiOltAT4LnBirLpCRJJUIkOwJG2YbYD5DWz/IfA68PviksvcepzrHxuwfw7QjnpLRtfDthRmnlYTEftExJ+isKT4Iwphteb8fYt9AlC8Gdg/KFx/fX3r1loM/3Vr7w9cW1zWupDC9y/WcK411luv9k4RcWNx2egi4FFgi4hoW/yjwdeL1/NORPw2IgYWm36n2PfTxeW6Z9apcZ+aGot1nkxhJhTgWAoBbE5xCe++aymv9g8AKaWlxS8rKHyf5hdDe405a/g+9AQ6ruF7sT4/m7p/hFhK42+69mG9z6uv7Vx9gX/U3DiuaE3X1xdYUPxZ1T22Rn/g+Ho/j/0o/EGhIbfzz7B9Ev+cBSYihkfEk1FYgr6Qws9xff4drTKu69VHRJwWEZV16vtsnfOu7d/d2tqt8rMtfl1GYXWJJKmJGIIlaT1FxN4UfqH/S/19KaXFKaV/SyntQGHp47cj4uCa3Ws45bpmiret8/V2FGbl5lFYxtypTl1tKSyLbcg/KCznbMjtwH3AtimlbsB/88/P8r5NIYjU9BHFet5q4Dzv1K21zrF1azgrpbRFnVd5SunxDay3rn+jMCO5T0qpK7B/TfcAKaXfpZS+QiE0/Q34n+L2d1NK30gp9QXOorDkeadiv4/Uq7EipfTNYrtnUkpHU1jm+yvgzvWosb63gR41y3aLtqPh7+k8oIqGvxcb8rNZl6XUGUv8M/Svj/rj921g2+Isdo01Xd87QPfiMvS6x9b4B4WVEHV/Hp1TSvk11PJLYGgUPhf+NYohOCI6UJjR/w+gd0ppCwrLx2MN56lfY/1/gxTP25/CmDoX2LJ43pfqnLfBcbwe7Vb52Rb7rAbeW496JUnryRAsSesQEV0j4kgKnz2cklJ6sYFjjozCTZcC+IjCY5VqZsTeo/D5vg11SkR8JiI6UXh80V3FJbavAh2jcGOrdhQ+i9thDee4DTgkIk6IiLKI2DIi9izu60JhZrIqIr5AYQatxp3AERFxcLGPfwOWAQ0F198CgyJiRHHZ5vmsGqb+G/huRAwCiIhuxc9Mbmi9dXWh8DnghVG4YdhlNTui8BzWo4sBaxmFJbwri/uOj3/eQGsBhSC3ksKy3V0i4tSIaFd87R2FGz21j4iTI6JbcfnxIv75s11vxSW1jwNXR0THiBhM4XO2qz0btzib+nPgR1G4mVbbiNi3GOo25GezLpXAScXzD2PVJeXrUn9cP0UhVH+n+P0bCnyVwr+bVRSXBk8HLi9+f/crHltjCvDViDisWFvHKNyoq1/9cxXP9wEwjcLnxP+eUppZ3NWewr+ND4DqKNyk7ND1vL47KYzb7sV+z6uzrzOFsfMBFG6+RWFGt8ZNwEUR8fko2KkYgNfV7g7gW1G4aVgFhY8O/G9qgbvHS9LmzBAsSWt2f0QspjCr833gR6z58Ug7Aw9TCFxPANenlP5U3Hc1MK64/LHBuwqvwa3AzRSWtnakEC5JKX0EjKXwi/ZbFGaG5zZ0glR4/vDhFILScgqzTnsUd48Frihe46XUmd1MKb1C4XOZ/0VhVvKrFG7u9WkDfcyj8NnZPIWb/+wMPFZn/73ANcDU4tLll4Dh9c/TQL3zKYS0PRo49CcUPgc7D3gSeKjOvjbAtynMqs2nEOy+Wdy3N/BURCyhMAt+QfHzl4sphKMTi+3eLdZc88eFU4HZxfrPprBUujFGAgOKfdwLXJbW/Pzoi4AXgWeK13EN0GZDfjbr4YJi+4UUrulXG9D2ZxQ+J70wIn5V7P+rFH6284DrgdNSSn9bQ/uTKNxEbD6FP2LcUrOj+AeDmhuqfUDh3+D/Y+2/t9wOHEKdpdDFn+v5FMb2gmKf963n9V1OYTny3yl8jvjWOud9GfhPCv/W3wN2Z9Ux/0sKn1O/ncIfTH4F9FhXOwp/+LiVwvL+v1NYDVA3fEuSmkCkdd63RZK0OYiIU4H2KaWftXQtahlRuBP1TcVl+2oGEXEj8J8ppVdbuhZJUoEzwZKUAcWllW9SePSKsuuzFGYY1QyK/+7e5p+fWZcktQLecl+SsuEXFJ51+s11HajNU0RcS+GmbfWfd6uNZxaF5dAHr+tASVLzadaZ4FwuN6Y5+5PWxTGp1mRjjseU0vEppa4ppds2Vh9q3VJKF6SUti8+k3e9+N/I0qSUeqeUti5+FlhNwDGp1sYxuWlq7uXQDhK1No5JtSaOR7U2jkm1No5JtTaOyU2QnwmWJEmSJGVGs94dukuXLmnXXXdttv6kdZk/fz49evRo6TIkwPGo1scxqdbGManWxjG5umeffXZeSqlXS9exNs16Y6y+ffsyffr05uxSWqtp06YxdOjQli5DAhyPan0ck2ptHJNqbRyTq4uIOS1dw7q4HFqSJEmSlBmGYEmSJElSZjTrZ4K79ds5Db/0lmbrT1qXhQsXssUWW7R0GRLgeFTr45hUa+OYVGvjmCyYOmbf2q8j4tmU0pAWLGednAmWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEklGzp0KMDnImJJ8fVK/WMi4ucRkSJipzrbekTEvRHxcUTMiYiTGjp/KW3rKmvU1dUpAjgSeD+l9NlSziVJkiRJ2uS9mVIa0NCOiNgP2LGBXT8FPgV6A3sCv42I51NKM5qibX2lzgTfDAwr8RySJEmSpM1YRJQB/wWcV297Z+BY4JKU0pKU0l+A+4BTm6JtQ0oKwSmlR4H5pZxDkiRJkrTZ2CYi5kXEYxExtM72bwGPppReqHf8LkB1SunVOtueBwY1UdvV+JlgSZIkSVLJrrnmGoAXgW2AScD9EbFjRGwLnAVc2kCzCmBRvW0fAV0ASmm7JiV9Jnh9RMQYYAxAp97bb+zuJEmSJEktYJ999gHoATxW3LQSmAC0B65IKX3UQLMlQNd627oCi4tf/6SEtg3a6DPBKaVJKaUhKaUhZWVtN3Z3kiRJkqSWM68m/wGPA08ABwM/jIh3I+Ld4nFPFO/k/CpQFhE71znHHkDNja1KadugjT4TLEmSJEnavC1cuJCnnnoKIIo3svo6sD9wATCVVSdg3wG+CjyfUvokIu4BroiIf6Vwh+ejgS8Vj92lhLYNKvURSXcAQ4GeETEXuCyl9LNSzilJkiRJ2rQsX76ccePGQSGIzgP+BhxT76ZVAEQEFGaMPyluGgv8HHgf+BD4Zs0jjlJK7ze27ZqUFIJTSiNLaS9JkiRJ2vT16tWLZ555hoj4a3Ep9BqllKLe+/nAMevTTylta3h3aEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGVGWXN21qdzG6aO2bc5u5TWatq0aQwd6phU6+B4VGvjmFRr45hUa+OY3DQ5EyxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMaNbnBL/78UpOnPREc3YprdXChZ/w369uGmPSZ2xLkiRJpXMmWJIkSZKUGYZgSZIkSVJmGIIlSZIkSZlhCJYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmGIIlSZIkSZlhCJYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmGIKlTcjEiRMZMmQIHTp04Iwzzqjd/uSTT/KVr3yFHj160KtXL44//njeeeedVdo+99xz7L///lRUVNC7d2+uvfba2n2XXHIJu+++O2VlZYwfP36VdiklfvCDH7DddtvRtWtXTjzxRBYtWrQxL1OSJEnaaAzB0iakb9++jBs3jjPPPHOV7QsWLGDMmDHMnj2bOXPm0KVLF0aNGlW7f968eQwbNoyzzjqLDz/8kNdff51DDz20dv9OO+3Ev//7v3PEEUes1uctt9zCrbfeymOPPcbbb7/NJ598wnnnnbfxLlKSJEnaiMpKaRwRw4BrgbbATSmlfJNUJalBI0aMAGD69OnMnTu3dvvw4cNXOe7cc8/lgAMOqH3/ox/9iMMOO4yTTz4ZgA4dOrDbbrvV7j/99NMBuO2221br8/7772f06NFsu+22AFx88cUcdNBB3HDDDXTq1KmJrkySJElqHo2eCY6ItsBPgeHAZ4CREfGZpipMUuM9+uijDBo0qPb9k08+SY8ePfjSl77EVlttxVe/+lXefPPN9T5fSmmVr5ctW8Zrr73WpDVLkiRJzaGU5dBfAF5PKb2RUvoUmAoc3TRlSWqsF154gSuuuIIf/vCHtdvmzp3L5MmTufbaa3nzzTfZfvvtGTly5Hqdb9iwYdx0003Mnj2bjz76iGuuuQaApUuXbpT6JUmSpI2plBC8DfCPOu/nFretIiLGRMT0iJheXb2ihO4krcvrr7/O8OHDufbaa/nyl79cu728vJyvfe1r7L333nTs2JHLLruMxx9/nI8++mid5zzzzDMZOXIkQ4cOZdCgQRx44IEA9OvXb6NdhyRJkjZZPWvyX/E1pqULqm+j3xgrpTQppTQkpTSkrKztxu5Oyqw5c+ZwyCGHcMkll3Dqqaeusm/w4MFERO37ul+vS5s2bbj88suZPXs2c+fOZdCgQWyzzTZss81qf/OSJEmS5tXkv+JrUksXVF8pIfgtYNs67/sVt0naSKqrq6mqqmLFihWsWLGCqqoqqqureeuttzjooIM499xzOfvss1drN2rUKO69914qKytZvnw5V155Jfvttx/dunUDYPny5VRVVbFy5cpV+gCYP38+s2bNIqXEyy+/zLe//W0uvfRS2rTx5vKSJEna9JTyW+wzwM4RsX1EtAdOBO5rmrIkNWTChAmUl5eTz+eZMmUK5eXlTJgwgZtuuok33niD8ePHU1FRUfuqcdBBB3HVVVdxxBFHsNVWW/H6669z++231+7/xje+QXl5OXfccQc/+MEPKC8v59ZbbwUKj1c6/PDD6dy5M8OHD+fMM89kzJhWt6pFkiRJWi9R966vG9w44nDgJxQekfTzlNIP1nZ8t347p+GX3tLo/qSmtnDhQrbYYouWLmO9TB2zb0uXoI1s2rRpDB06tKXLkGo5JtXaOCbV2jgmVxcRz6aUhrR0HWtT0nOCU0oPAA80US2SJEmSJG1UfqhPkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmVHWnJ316dyGqWP2bc4upbWaNm0aQ4c6JiVJkqSscCZYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmdGszwl+9+OVnDjpiebsUlqrhQs/4b9fbfox6fOwJUmSpNbJmWBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIljaSiRMnMmTIEDp06MAZZ5xRu/3TTz/luOOOY8CAAUQE06ZNW6Xdj3/8Y3bYYQe6du1K3759+da3vkV1dfVq53/kkUeICMaNG7fK9jfeeIMjjzySLl260LNnT77zne9sjMuTJEmSNkmGYGkj6du3L+PGjePMM89cbd9+++3HlClT6NOnz2r7jjrqKJ577jkWLVrESy+9xPPPP8911123yjHLly/nggsuYJ999lll+6effspXvvIVDjroIN59913mzp3LKaec0rQXJkmSJG3CyhrbMCK2BW4BegMJmJRSurapCpM2dSNGjABg+vTpzJ07t3Z7+/btufDCCwFo27btau123HHH2q9TSrRp04bXX399lWP+8z//k0MPPZT3339/le0333wzffv25dvf/nbttsGDB5d8LZIkSdLmopSZ4Grg31JKnwG+CJwTEZ9pmrKkbLv99tvp2rUrPXv25Pnnn+ess86q3Tdnzhx+/vOfc+mll67W7sknn2TAgAEMHz6cnj17MnToUF588cXmLF2SJElq1RodglNK76SUnit+vRiYCWzTVIVJWXbSSSexaNEiXn31Vc4++2x69+5du+/888/nyiuvpKKiYrV2c+fOZerUqZx//vm8/fbbHHHEERx99NF8+umnzVm+JEmS1Go1yWeCI2IAsBfwVAP7xkTE9IiYXl29oim6kzJj5513ZtCgQYwdOxaA+++/n8WLF/P1r3+9wePLy8vZb7/9GD58OO3bt+eiiy7iww8/ZObMmc1ZtiRJkrKrZ03+K77GtHRB9TX6M8E1IqICuBu4MKW0qP7+lNIkYBJAt347p1L7k7KmurqaWbNmAfDHP/6R6dOn195Q66OPPqJt27a8+OKL/PrXv2bw4ME89thjLVmuJEmSsm1eSmlISxexNiXNBEdEOwoB+LaU0j1NU5K0eaiurqaqqooVK1awYsUKqqqqah91tGzZMqqqqoDCHZ2rqqpIqfA3optuuqn2hlcvv/wyV199NQcffDAAV155Ja+++iqVlZVUVlZy1FFH8Y1vfINf/OIXAJxyyik8+eSTPPzww6xYsYKf/OQn9OzZk9122625L1+SJElqlRodgiMigJ8BM1NKP2q6kqTNw4QJEygvLyefzzNlyhTKy8uZMGECALvuuivl5eW89dZbHHbYYZSXlzNnzhwAHnvsMXbffXc6d+7M4YcfzuGHH85VV10FQJcuXejTp0/tq7y8nM6dO9OjR4/a806ZMoWzzz6b7t278+tf/5r77ruP9u3bt8w3QZIkSWplSlkO/S/AqcCLEVFZ3Pa9lNIDJVclbQbGjx/P+PHjG9w3e/bsNbarmdVdHzfffPNq20aMGFH7eCZJkiRJq2p0CE4p/QWIJqxFkiRJkqSNqknuDi1JkiRJ0qbAECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJyoyy5uysT+c2TB2zb3N2Ka3VtGnTGDrUMSlJkiRlhTPBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJyoxmfU7wux+v5MRJTzRnl62ez02WJEmSpObjTLAkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxDcirz22mt07NiRU045BYB33nmHo446ir59+xIRzJ49e5XjBw0aREVFRe2rrKyMr371qwC8+uqrHH300fTq1YsePXpw2GGH8corrzT3JUmSJElSq9LoEBwRHSPi6Yh4PiJmRMTlTVlYFp1zzjnsvffete/btGnDsGHDuPvuuxs8fsaMGSxZsoQlS5awePFitt12W44//ngAFi5cyFFHHcUrr7zCe++9xxe+8AWOPvroZrkOSZIkSWqtSpkJXgYclFLaA9gTGBYRX2ySqjJo6tSpbLHFFhx88MG123r37s3YsWNXCcZr8uijjzJv3jyOPfZYAL7whS8wevRoevToQbt27fjWt77FK6+8wocffrjRrkGSJEmSWrtGh+BUsKT4tl3xlZqkqoxZtGgRl156KT/60Y8afY7Jkydz7LHH0rlz5wb3P/roo/Tp04ctt9yy0X1IkiRJ0qaurJTGEdEWeBbYCfhpSumpJqkqYy655BJGjx5Nv379GtV+6dKl3HXXXdx3330N7p87dy7nnHNOSSFbkiRJkjYHJYXglNIKYM+I2AK4NyI+m1J6qe4xETEGGAPQqff2pXS3WaqsrOThhx/mr3/9a6PPcc8999CjRw8OOOCA1fZ98MEHHHrooYwdO5aRI0eWUqokSZIkrUvPiJhe5/2klNKkFqumASWF4BoppYUR8SdgGPBSvX2TgEkA3frt7HLpeqZNm8bs2bPZbrvtAFiyZAkrVqzg5Zdf5rnnnluvc0yePJnTTjuNiFhl+4IFCzj00EM56qij+P73v9/ktUuSJElSPfNSSkNauoi1KeXu0L2KM8BERDnwFeBvTVRXZowZM4ZZs2ZRWVlJZWUlZ599NkcccQS/+93vAKiqqmLZsmUALFu2jKqqqlXaz507lz/96U+cfvrpq2xftGgRhx12GP/yL/9CPp9vnouRJEmSpFaulJngrYHJxc8FtwHuTCn9pmnKyo5OnTrRqVOn2vcVFRV07NiRXr16AVBeXl67b+DAgQCk9M8J9VtvvZV9992XHXfccZXz3nvvvTzzzDPMmDGDm2++uXb7yy+/XDvrLEmSJElZ0+gQnFJ6AdirCWsRMH78+FXe1w28Dfnud7/Ld7/73dW2n3766avNDkuSJElS1pXynGBJkiRJkjYphmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmVHWnJ316dyGqWP2bc4uJUmSJEmq5UywJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMqNZnxP87scrOXHSE83ZZaP4LGNJkiRJ2jw5EyxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBa7Bs2TJGjx5N//796dKlC3vuuScPPvggALNnzyYiqKioqH1deeWVq7R/+OGH+dznPkfnzp3p168fd955Z+2+yspKPv/5z9OpUyc+//nPU1lZ2ZyXJkmSJEmZVdbSBbRW1dXVbLvttjzyyCNst912PPDAA5xwwgm8+OKLtccsXLiQsrLVv4Uvv/wyJ510EpMnT+YrX/kKH330EQsXLgTg008/5eijj+bCCy9k7Nix3HjjjRx99NG89tprtG/fvrkuT5IkSZIyqaSZ4IjYIiLuioi/RcTMiNi3qQpraZ07d2b8+PEMGDCANm3acOSRR7L99tvz7LPPrrPthAkTOOussxg+fDhlZWVsueWW7LjjjgBMmzaN6upqLrzwQjp06MD5559PSon/+7//29iXJEmSJEmZV+py6GuBh1JKA4E9gJmll9Q6vffee7z66qsMGjSodlv//v3p168fo0aNYt68ebXbn3zySQB23313tt56a0455RTmz58PwIwZMxg8eDARUXv84MGDmTFjRjNdiSRJkiRlV6NDcER0A/YHfgaQUvo0pbSwiepqVZYvX87JJ5/M6aefzsCBA+nZsyfPPPMMc+bM4dlnn2Xx4sWcfPLJtcfPnTuXW2+9lbvvvpvXXnuNTz75hPPOOw+AJUuW0K1bt1XO361bNxYvXtys1yRJkiRJWVTKZ4K3Bz4AfhERewDPAheklD6ue1BEjAHGAHTqvX0J3bWMlStXcuqpp9K+fXsmTpwIQEVFBUOGDAGgd+/eTJw4ka233prFixfTpUsXysvLGTVqFLvssgsA3/ve9zjkkENq2y5atGiVPhYtWkSXLl2a8aokSZIkaaPoGRHT67yflFKa1GLVNKCU5dBlwOeAG1JKewEfA7n6B6WUJqWUhqSUhpSVtS2hu+aXUmL06NG899573H333bRr167B42qWNq9cuRJgteXOdb8eNGgQL7zwAiml2m0vvPDCKsusJUmSJGkTNa8m/xVfrSoAQ2kheC4wN6X0VPH9XRRC8Wbjm9/8JjNnzuT++++nvLy8dvtTTz3FK6+8wsqVK/nwww85//zzGTp0aO0y51GjRvGLX/yCN954g6VLl5LP5znyyCMBGDp0KG3btuW6665j2bJltbPLBx10UPNfoCRJkiRlTKNDcErpXeAfEbFrcdPBwMtNUlUrMGfOHG688UYqKyvp06dP7fOAb7vtNt544w2GDRtGly5d+OxnP0uHDh244447atueeeaZnHbaaeyzzz7079+fDh06cN111wHQvn17fvWrX3HLLbewxRZb8POf/5xf/epXPh5JkiRJkppBqc8JPg+4LSLaA28Ao0ovqXXo37//KkuW6xs5cuRa219++eVcfvnlDe7ba6+91utRS5IkSZKkplVSCE4pVQJDmqYUSZIkSZI2rlKfEyxJkiRJ0ibDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJyoyy5uysT+c2TB2zb3N2KUmSJElSLWeCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZpQ1Z2fvfrySEyc9UfJ5po7ZtwmqkSRJkiRljTPBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjJjkw7BEydOZMiQIXTo0IEzzjijdvunn37Kcccdx4ABA4gIpk2btkq7P/3pTxx44IF069aNAQMGrHbe2bNnc+CBB9KpUycGDhzIww8/vHEvRJIkSZLULNYZgiPi5xHxfkS8VGfb8RExIyJWRsSQjVvimvXt25dx48Zx5plnrrZvv/32Y8qUKfTp02e1fZ07d+bMM8/khz/8YYPnHTlyJHvttRcffvghP/jBDzjuuOP44IMPmrx+SZIkSVLzWp+Z4JuBYfW2vQSMAB5t6oI2xIgRIzjmmGPYcsstV9nevn17LrzwQvbbbz/atm27WrsvfOELnHrqqeywww6r7Xv11Vd57rnnuPzyyykvL+fYY49l99135+67795o1yFJkiRJah5l6zogpfRoRAyot20mQERspLJazowZM9hhhx3o0qVL7bY99tiDGTNmtGBVkiRJkqSmsEl/JnhjWLJkCd26dVtlW7du3Vi8eHELVSRJkiRJaiobPQRHxJiImB4R06urV2zs7kpWUVHBokWLVtm2aNGiVWaGJUmSJEkN6lmT/4qvMS1dUH0bPQSnlCallIaklIaUla3++dzWZtCgQbzxxhurzPw+//zzDBo0qAWrkiRJkqRNwrya/Fd8TWrpgurbpJdDV1dXU1VVxYoVK1ixYgVVVVVUV1cDsGzZMqqqqoDCI5OqqqpIKQGwcuVKqqqqWL58OSklqqqq+PTTTwHYZZdd2HPPPbn88supqqri3nvv5YUXXuDYY49tmYuUJEmSJDWZ9XlE0h3AE8CuETE3IkZHxNciYi6wL/DbiPjdxi60IRMmTKC8vJx8Ps+UKVMoLy9nwoQJAOy6666Ul5fz1ltvcdhhh1FeXs6cOXMAePTRRykvL+fwww/nzTffpLy8nEMPPbT2vFOnTmX69Ol0796dXC7HXXfdRa9evVriEiVJkiRJTWh97g49cg277m3iWjbY+PHjGT9+fIP7Zs+evcZ2Q4cOrZ0VbsiAAQOYNm1aacVJkiRJklqdTXo5tCRJkiRJG8IQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzypqzsz6d2zB1zL7N2aUkSZIkSbWcCZYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmGIIlSZIkSZlhCJYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmNOtzgt/9eCUnTnpig9v5bGFJkiRJUlNwJliSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZsUmF4NmzZ3P44YfTvXt3+vTpw7nnnkt1dfUqx9xyyy1EBDfddFPttuHDh1NRUVH7at++Pbvvvntzly9JkiRJamElh+CIaBsRf42I3zRFQWszduxYttpqK9555x0qKyt55JFHuP7662v3L1iwgKuuuopBgwat0u7BBx9kyZIlta8vfelLHH/88Ru7XEmSJElSK9MUM8EXADOb4Dzr9Pe//50TTjiBjh070qdPH4YNG8aMGTNq93/3u9/l/PPPp2fPnms8x+zZs/nzn//Maaed1hwlS5IkSZJakZJCcET0A44AblrXsU3hwgsvZOrUqSxdupS33nqLBx98kGHDhgHw9NNPM336dM4+++y1nuOWW27hy1/+MgMGDGiGiiVJkiRJrUmpM8E/Ab4DrCy9lHXbf//9mTFjBl27dqVfv34MGTKEY445hhUrVjB27FgmTpxImzZrv6RbbrmFM844oznKlSRJkiS1Mo0OwRFxJPB+SunZdRw3JiKmR8T06uoVje2OlStXMmzYMEaMGMHHH3/MvHnzWLBgARdffDHXX389gwcP5otf/OJaz/GXv/yFd999l+OOO67RdUiSJEmS1qhnTf4rvsa0dEH1lZXQ9l+AoyLicKAj0DUipqSUTql7UEppEjAJoFu/nVNjO5s/fz5vvvkm5557Lh06dKBDhw6MGjWKcePGsdNOO/HII4/wwAMP1B7717/+lcrKSiZOnFh7jsmTJzNixAgqKioaW4YkSZIkac3mpZSGtHQRa9PoEJxS+i7wXYCIGApcVD8AN6WePXuy/fbbc8MNN3DRRRexZMkSJk+ezODBg7n++uupqqqqPXbEiBEcd9xxjB49unbbJ598wp133sm99967sUqUJEmSJLVym9Rzgu+55x4eeughevXqxU477US7du348Y9/zBZbbEGfPn1qX+3bt6dr165069attu2vfvUrtthiCw488MAWvAJJkiRJUksqZTl0rZTSNGBaU5xrbfbcc0+mTVt3Nw0dM3LkSEaOHNn0RUmSJEmSNhmb1EywJEmSJEmlMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJyoyy5uysT+c2TB2zb3N2KUmSJElSLWeCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBnN+pzgdz9eyYmTnljncT5LWJIkSZK0MTgTLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzWn0Injp1KrvtthudO3dmxx135M9//jMAf/zjHxk4cCCdOnXiwAMPZM6cObVtLrroInbeeWe6dOnCwIEDueWWW1qqfEmSJElSK9KqQ/Af/vAHLr74Yn7xi1+wePFiHn30UXbYYQfmzZvHiBEjuPLKK5k/fz5Dhgzh61//em27zp07c//99/PRRx8xefJkLrjgAh5//PEWvBJJkiRJUmtQUgiOiG9FxIyIeCki7oiIjk1VGMBll13GpZdeyhe/+EXatGnDNttswzbbbMM999zDoEGDOP744+nYsSPjx4/n+eef529/+xsAl19+OQMHDqRNmzbss88+fPnLX+aJJ55oytIkSZIkSZugRofgiNgGOB8YklL6LNAWOLGpCluxYgXTp0/ngw8+YKeddqJfv36ce+65fPLJJ8yYMYM99tij9tiapdIzZsxY7TyffPIJzzzzDIMGDWqq0iRJkiRJm6iyJmhfHhHLgU7A26WXVPDee++xfPly7rrrLv785z/Trl07jj76aCZMmMCSJUvo1avXKsd369aNxYsXr3aes88+mz322IPDDjusqUqTJEmSJG2iGj0TnFJ6C/gP4E3gHeCjlNLv6x8XEWMiYnpETK+uXrHe5y8vLwfgvPPOY+utt6Znz558+9vf5oEHHqCiooJFixatcvyiRYvo0qXLKtv+3//7f7z00kvceeedRMSGXqIkSZIkacP0rMl/xdeYli6ovlKWQ3cHjga2B/oCnSPilPrHpZQmpZSGpJSGlJW1Xe/zd+/enX79+q0SXmu+HjRoEM8//3zt9o8//phZs2atsuT5sssu48EHH+T3v/89Xbt23eDrkyRJkiRtsHk1+a/4mtTSBdVXyo2xDgH+nlL6IKW0HLgH+FLTlFUwatQo/uu//ov333+fBQsW8OMf/5gjjzySr33ta7z00kvcfffdVFVVccUVVzB48GAGDhwIwNVXX83tt9/Oww8/zJZbbtmUJUmSJEmSNmGlhOA3gS9GRKcoTNEeDMxsmrIKLrnkEvbee2922WUXdtttN/baay++//3v06tXL+6++26+//3v0717d5566immTp1a2+573/seb775JjvttBMVFRVUVFRw1VVXNWVpkiRJkqRNUKNvjJVSeioi7gKeA6qBvwJNOtXdrl07rr/+eq6//vrV9h1yyCG1j0RqoLamLEOSJEmStJko6e7QKaXLgMuaqBZJkiRJkjaqUpZDS5IkSZK0STEES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIyo6w5O+vTuQ1Tx+zbnF1KkiRJklTLmWBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZhmBJkiRJUmYYgiVJkiRJmWEIliRJkiRlhiFYkiRJkpQZZc3Z2bsfr+TESU/Uvp86Zt/m7F6SJEmSlHHOBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKjBYPwcuWLWP06NH079+fLl26sOeee/Lggw/W7l+6dCljx46lZ8+edOvWjf3337923/jx42nXrh0VFRW1rzfeeKMlLkOSJEmStAkoK6VxRFwAfAMI4H9SSj/Z0HNUV1ez7bbb8sgjj7DddtvxwAMPcMIJJ/Diiy8yYMAAxowZQ3V1NTNnzqRHjx5UVlau0v7rX/86U6ZMKeUyJEmSJEkZ0egQHBGfpRCAvwB8CjwUEb9JKb2+Iefp3Lkz48ePr31/5JFHsv322/Pss89SVVXFfffdx9y5c+natSsAn//85xtbsiRJkiQp40pZDr0b8FRKaWlKqRp4BBhRakHvvfcer776KoMGDeLpp5+mf//+XHbZZfTs2ZPdd9+du+++e5Xj77//fnr06MGgQYO44YYbSu1ekiRJkrQZKyUEvwR8OSK2jIhOwOHAtqUUs3z5ck4++WROP/10Bg4cyNy5c3nppZfo1q0bb7/9NhMnTuT0009n5syZAJxwwgnMnDmTDz74gP/5n//hiiuu4I477iilBEmSJEnSZqzRITilNBO4Bvg98BBQCayof1xEjImI6RExvbp6td21Vq5cyamnnkr79u2ZOHEiAOXl5bRr145x48bRvn17DjjgAA488EB+//vfA/CZz3yGvn370rZtW770pS9xwQUXcNdddzX2kiRJkiRJpelZk/+KrzEtXVB9Jd0YK6X0M+BnABFxFTC3gWMmAZMAuvXbOa3hPIwePZr33nuPBx54gHbt2gEwePDg1Y6NiDXWExGk1GAXkiRJkqSNb15KaUhLF7E2JT0iKSK2Kv7vdhQ+D3x7Y87zzW9+k5kzZ3L//fdTXl5eu33//fdnu+224+qrr6a6uprHHnuMP/3pTxx22GEA/PrXv2bBggWklHj66ae57rrrOProo0u5JEmSJEnSZqzU5wTfHREvA/cD56SUFm7oCebMmcONN95IZWUlffr0qX3e72233Ua7du349a9/zQMPPEC3bt34xje+wS233MLAgQMBmDp1KjvttBNdunThtNNO4+KLL+b0008v8ZIkSZIkSZurUpdDf7nUAvr377/WJcyDBg3iiSeeaHCfN8GSJEmSJG2IUmeCJUmSJEnaZBiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGVGWXN21qdzG6aO2bc5u5QkSZIkqZYzwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScqMZg3B7368khMnPdGcXUqSJEmSVMuZYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGVGi4bgiRMnMmTIEDp06MAZZ5zR4DFXXHEFEcHDDz9cu+073/kO2267LV27dqV///5cddVVzVSxJEmSJGlTVlIIjojZEfFiRFRGxPQNbd+3b1/GjRvHmWee2eD+WbNm8ctf/pKtt956le2jR4/mb3/7G4sWLeLxxx/ntttu45577mncRUiSJEmSMqMpZoIPTCntmVIasqENR4wYwTHHHMOWW27Z4P5zzjmHa665hvbt26+yfdddd6Vz586179u0acPrr7++od1LkiRJkjKm1X4m+Je//CUdOnTg8MMPb3B/Pp+noqKCfv368fHHH3PSSSc1c4WSJEmSpE1NqSE4Ab+PiGcjYkxDB0TEmIiYHhHTq6tXrNdJFy9ezPe+9z2uvfbaNR6Ty+VYvHgxzz33HKeeeirdunVr1AVIkiRJkppMz5r8V3w1mBNbUqkheL+U0ueA4cA5EbF//QNSSpNSSkNSSkPKytqu10nHjx/PqaeeyoABA9Z6XESw1157UV5ezmWXXdaI8iVJkiRJTWheTf4rvia1dEH1lRSCU0pvFf/3feBe4AtNUdQf//hHrrvuOvr06UOfPn34xz/+wQknnMA111zT4PHV1dXMmjWrKbqWJEmSJG3GGh2CI6JzRHSp+Ro4FHhpQ85RXV1NVVUVK1asYMWKFVRVVVFdXc0f//hHXnrpJSorK6msrKRv377ceOONnHPOOaxcuZIbb7yRBQsWkFLi6aef5qc//SkHH3xwYy9FkiRJkpQRpcwE9wb+EhHPA08Dv00pPbQhJ5gwYQLl5eXk83mmTJlCeXk5EyZMYMstt6ydBe7Tpw9t27ale/fuVFRUAHDvvfey44470qVLF0455RTOO+88zjvvvBIuRZIkSZKUBWWNbZhSegPYo5TOx48fz/jx49d53OzZs2u/btOmDQ89tEFZW5IkSZIkoBU/IkmSJEmSpKZmCJYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmGIIlSZIkSZlhCJYkSZIkZYYhWJIkSZKUGYZgSZIkSVJmGIIlSZIkSZlhCJYkSZIkZYYhWJIkSZKUGc0agvt0bsPUMfs2Z5eSJEmSJNVyJliSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZ0awh+N2PVzZnd5IkSZIkrcKZYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZYQiWJEmSJGWGIViSJEmSlBmGYEmSJElSZhiCJUmSJEmZ0WIhePbs2Rx++OF0796dPn36cO6551JdXQ3AmDFj2HXXXWnTpg0333xzS5UoSZIkSdrMtFgIHjt2LFtttRXvvPMOlZWVPPLII1x//fUA7LHHHlx//fV87nOfa6nyJEmSJEmbobLGNoyIXYH/rbNpB+DSlNJP1qf93//+d84991w6duxInz59GDZsGDNmzADgnHPOAaBjx46NLU+SJEmSpNU0eiY4pfRKSmnPlNKewOeBpcC969v+wgsvZOrUqSxdupS33nqLBx98kGHDhjW2HEmSJEmS1qmplkMfDMxKKc1Z3wb7778/M2bMoGvXrvTr148hQ4ZwzDHHNFE5kiRJkiStrqlC8InAHQ3tiIgxETE9IqZXV68AYOXKlQwbNowRI0bw8ccfM2/ePBYsWMDFF1/cROVIkiRJklpAz5r8V3yNaemC6is5BEdEe+Ao4JcN7U8pTUopDUkpDSkrawvA/PnzefPNNzn33HPp0KEDW265JaNGjeKBBx4otRxJkiRJUsuZV5P/iq9JLV1QfU0xEzwceC6l9N76NujZsyfbb789N9xwA9XV1SxcuJDJkyczePBgAD799FOqqqpIKbF8+XKqqqpYuXJlE5QqSZIkScqypgjBI1nDUui1ueeee3jooYfo1asXO+20E+3atePHP/4xAIceeijl5eU8/vjjjBkzhvLych599NEmKFWSJEmSlGWNfkQSQER0Br4CnLWhbffcc0+mTZvW4L41bZckSZIkqRQlheCU0sfAlk1UiyRJkiRJG1VT3R1akiRJkqRWzxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScqMZg3BfTqbuSVJkiRJLcdUKkmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMwwBEuSJEmSMsMQLEmSJEnKDEOwJEmSJCkzDMGSJEmSpMxo0RD82muv0bFjR0455ZSWLEOSJEmSlBHrDMER8fOIeD8iXqqzrUdE/CEiXiv+b/fGdH7OOeew9957N6apJEmSJEkbbH1mgm8GhtXblgP+mFLaGfhj8f0GmTp1KltssQUHH3zwhjaVJEmSJKlR1hmCU0qPAvPrbT4amFz8ejJwzIZ0umjRIi699FJ+9KMfbUgzSZIkSZJK0tjPBPdOKb1T/PpdoPeGNL7kkksYPXo0/fr1a2T3kiRJkiRtuLJST5BSShGR1rQ/IsYAYwB69+5NZWUlDz/8MH/9619L7VqSJEmS1Lr0jIjpdd5PSilNarFqGtDYEPxeRGydUnonIrYG3l/TgcULngSw6667pmnTpjF79my22247AJYsWcKKFSt4+eWXee655xpZjiRJkiSpFZiXUhrS0kWsTWOXQ98HnF78+nTg1+vbcMyYMcyaNYvKykoqKys5++yzOeKII/jd737XyFIkSZIkSVo/65wJjog7gKEUprXnApcBeeDOiBgNzAFOWN8OO3XqRKdOnWrfV1RU0LFjR3r16rWBpUuSJEmStGHWGYJTSiPXsKtJnm00fvz4pjiNJEmSJEnr1Njl0JIkSZIkbXIMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScoMQ7AkSZIkKTMMwZIkSZKkzDAES5IkSZIywxAsSZIkScqMSCk1W2dlZWWLV6xY8UqzdSitQ6dOnXouXbp0XkvXIYHjUa2PY1KtjWNSrY1jskH9U0q9WrqItSlrzs4uuuiiV/L5/JDm7FNam1wuN90xqdbC8ajWxjGp1sYxqdbGMblpcjm0JEmSJCkzDMGSJEmSpMxo7hA8qZn7k9bFManWxPGo1sYxqdbGManWxjG5CWrWG2NJkiRJktSSXA4tSZIkScoMQ7AkSZIkKTOa5RFJuVyuB/Az4FBgHvDdfD5/e3P0rc1TLpc7FzgD2B24I5/Pn1Fn38HAT4HtgKeAM/L5/Jzivg7ADcBxwFLg3/P5/I+aoq2yqzg2rgcOAXoAsyj8d+7B4n7HpJpdLpebAhwMdAbepTA+biruc0yqxeRyuZ2BF4G78vn8KcVtJwFXAz2BPwBn5vP5+cV9a/09spS2yrZcLjcN+CJQXdz0Vj6f37W4zzG5GWuumeCfAp8CvYGTgRtyudygZupbm6e3gQnAz+tuzOVyPYF7gEsohJHpwP/WOWQ8sDPQHzgQ+E4ulxtWaltlXhnwD+AAoBswDrgzl8sNcEyqBV0NDMjn812Bo4AJuVzu845JtQI/BZ6peVP8nfBG4FQKvysupfCHxbrHN/h7ZCltpaJz8/l8RfFVE4Adk5u5jT4TnMvlOgPHAp/N5/NLgL/kcrn7KAyM3MbuX5unfD5/D0AulxsC9KuzawQwI5/P/7K4fzwwL5fLDczn838DTqcwa7EAWJDL5f6HwozyQyW2VYbl8/mPKfzyX+M3uVzu78DngS1xTKoF5PP5GXXepuJrRwrj0jGpFpHL5U4EFgKPAzsVN58M3J/P5x8tHnMJMDOXy3UBVrL23yNLaSutiWNyM9ccM8G7ANX5fP7VOtueB/yLhzaGQRTGF1AbTmYBg3K5XHdg67r7WXUsltJWqpXL5XpT+G/fDByTakG5XO76XC63FPgb8A7wAI5JtZBcLtcVuAL4dr1d9cfVLAozZbuw7t8jS2krAVydy+Xm5XK5x3K53NDiNsfkZq45QnAFsKjeto+ALs3Qt7KngsL4qqtmvFXUeV9/X6ltJQByuVw74DZgcnFmzDGpFpPP58dSGBNfprCMeRmOSbWcK4Gf5fP5ufW2r2tcre33yFLaShcDOwDbUHje7/25XG5HHJObvea4MdYSoGu9bV2Bxc3Qt7JnbeNtSZ33VfX2ldpWIpfLtQFupfAX33OLmx2TalH5fH4FhSV3pwDfxDGpFpDL5fakcPPAvRrYvbZxtXIt+0ptq4zL5/NP1Xk7OZfLjQQOxzG52WuOmeBXgbLinQBr7EFhmaDU1GZQGF9A7WfSd6TwGbYFFJYD7lHn+LpjsZS2yrhcLhcU7vbYGzg2n88vL+5yTKq1KKM4fnBMqvkNBQYAb+ZyuXeBi4Bjc7ncc6w+rnYAOlD4HXJdv0eW0laqLwGBY3KzFymljd5JLpebSmFQ/SuwJ4XPJH2p3k07pPWWy+XKKPxCdxmFG2N9g8Lt7bsDrwNnAr8FLgcOyOfzXyy2ywP7AsdQCCt/Akbl8/mHcrlcr8a23fhXrNYul8v9N4X/vh1SvNlFzfZGjyvHpBorl8ttBRwE/Ab4hMIM3D3ASOAJHJNqZrlcrhOrzoBdRCEUfxPYisK4PAJ4jsKddcvy+fyJxbZr/D2yeFfdRrXdeFerTUEul9sC2Ad4hMLvkF+nsCR6L6AdjsnNWnM9ImksUA68D9wBfNMftEo0jsIvdjnglOLX4/L5/AcU7rr3A2ABhf+4nVin3WUUbuIyh8J/9H5Y88tZKW2Vbblcrj9wFoX/M3s3l8stKb5OdkyqhSQK4WIuhbHzH8CF+Xz+PsekWkI+n1+az+ffrXlRWDJalc/nPyj+Tng2hfspvE/h85Fj6zRf4++RpbRV5rWj8LjNDyg8r/c84Jh8Pv+qY3Lz1ywzwZIkSZIktQbNNRMsSZIkSVKLMwRLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTMMARLkiRJkjLDECxJkiRJygxDsCRJkiQpMwzBkiRJkqTM+P9keAzrroDvtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_y_train = pd.DataFrame(y_train, columns=['Classe'])\n",
    "distribution(df_y_train, 'Classe', 'treino')\n",
    "df_y_test = pd.DataFrame(y_test, columns=['Classe'])\n",
    "distribution(df_y_test, 'Classe', 'teste')\n",
    "df_y_val = pd.DataFrame(y_val, columns=['Classe'])\n",
    "distribution(df_y_val, 'Classe', 'validação')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008cf21",
   "metadata": {},
   "source": [
    "# Redes neurais\n",
    "\n",
    "A partir daqui serão explorados alguns algoritmos de redes neurais para solução do problema acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6b023",
   "metadata": {},
   "source": [
    "Configuração para execução com a GPU, quando disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c2f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bf022",
   "metadata": {},
   "source": [
    "A seguir são apresentadas três classes de redes neurais.\n",
    "\n",
    "- ``LogisticRegression`` trata-se de uma regressão logística simples, ou seja, contendo apenas uma camada linear de ativação.\n",
    "- ``Perceptron`` trata-se de uma evolução da regressão logística. Além da camada de ativação linear, há também uma camada do tipo `ReLu`.\n",
    "- ``MultiLayer`` trata-se de uma rede neural multi camadas. Além dos parâmetros de entrada (`n_input`) e saída (`n_output`), é possível também definir a saída entrada entre as camadas internas (`n_hidden_size`).\n",
    "\n",
    "Todas as classes acima contam com o processo de retroalimentação, ou `backpropagation`, implementados automaticamente pelo Pytorch a partir do método `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c7d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.Linear = nn.Linear(n_input, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.Linear(x)\n",
    "        return outputs\n",
    "    \n",
    "    def model_name(self):\n",
    "        return 'Logistic Regression'\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(n_input, n_output)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(x)\n",
    "        return output\n",
    "    \n",
    "    def model_name(self):\n",
    "        return 'Perceptron'\n",
    "\n",
    "class MultiLayer(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden_size, n_output, n_dropout=0.0):\n",
    "        super(MultiLayer, self).__init__()\n",
    "        self.input_size = n_input\n",
    "        self.hidden_size  = n_hidden_size\n",
    "        self.output_size = n_output\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(n_dropout)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        dropout = self.dropout(hidden)\n",
    "        relu = self.relu(dropout)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "    def model_name(self):\n",
    "        return 'Multi Layer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0ef95",
   "metadata": {},
   "source": [
    "Conversão dos conjuntos de treino, teste e validação em ``torch.tensor`` para utilização por parte do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "825c1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test,  y_test, = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
    ")\n",
    "\n",
    "X_val, y_val = map(\n",
    "torch.tensor, (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411a532",
   "metadata": {},
   "source": [
    "Abaixo são definidos alguns parâmetros gerais:\n",
    "- `input_size`: dimensão dos dados na camada de entrada do modelo.\n",
    "- `output_size`: dimensão de saída dos dados, representado pela quantidade de classes disponíveis para classificação.\n",
    "- `bs`: batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e47e8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = len(y.unique())\n",
    "bs = 256 #batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9938c",
   "metadata": {},
   "source": [
    "A seguir são criados conjuntos do tipo `DataLoader`, que é uma abstração do PyTorch. Isto facilitará as iterações (épocas) sobre os modelos.\n",
    "Além disso, ele facilita a utilização do batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57707876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, )\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs * 2)\n",
    "valid_ds = TensorDataset(X_val, y_val)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ac8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_nn_model(model_name: str, \n",
    "                    device, \n",
    "                    learning_rate,\n",
    "                    input_size: int = 0,\n",
    "                    output_size: int = 0,\n",
    "                    dropout: float = 0.0,\n",
    "                    hidden: int = 1,\n",
    "                    optimizer: str = 'SGD',\n",
    "                    momentum: float = 0.0,\n",
    "                    weight_decay: float = 0.0,\n",
    "                    ):\n",
    "    '''\n",
    "    Identificar e retorna o modelo e o otimizador que será utilizado.\n",
    "    \n",
    "    Args:\n",
    "        model_name: nome do modelo. Suporte à: 'logisticregression', 'perceptron', 'multilayer'.\n",
    "        device: dispositivo em que será executado o processamento ('cpu' ou 'cuda').\n",
    "        learning_rate: taxa de aprendizado a ser adotada pelo otimizador.\n",
    "        input_size: dimensão dos dados na camada de entrada do modelo.\n",
    "        output_size: dimensão dos dados na camada de saída do modelo (quantidade de classes).\n",
    "        dropout: taxa de dropout que será utilizada entre certas camadas do modelo.\n",
    "        hidden: tamanho da quantidade de camadas ocultas.\n",
    "        optimizer: otimizador que será adotado para ajuste dos pesos. Suporte à: 'SGD', 'Adam', 'AdamW', 'ASGD'.\n",
    "        momentum: fator momentum. É utilizado apenas pelo otimizador SGD.\n",
    "        weight_decay: taxa adotada pelo regularizador L2 para aplicação no modelo. Utilizado apenas nos \n",
    "        otimizadores Adam e derivados.\n",
    "    '''\n",
    "    model = None\n",
    "    opt = None\n",
    "    \n",
    "    if model_name == 'logisticregression':\n",
    "        model = LogisticRegression(input_size, output_size)\n",
    "    elif model_name == 'perceptron':\n",
    "        model = Perceptron(input_size, output_size)\n",
    "    elif model_name == 'multilayer':\n",
    "        model = MultiLayer(input_size, hidden, output_size, dropout)\n",
    "        \n",
    "    opt = choose_optimizer(optimizer, learning_rate, model, momentum, weight_decay)\n",
    "    \n",
    "    if model is not None:\n",
    "        model.double()\n",
    "        \n",
    "    return model, opt\n",
    "\n",
    "def choose_optimizer(optimizer: str, learning_rate, model, momentum: float=0.0, weight_decay: float=0.0):\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    opt = None\n",
    "    \n",
    "    if optimizer == 'SGD':\n",
    "        opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer == 'Adam':\n",
    "        opt = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer == 'AdamW':\n",
    "        opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer == 'ASGD':\n",
    "        opt = optim.ASGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    return opt        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2c5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, device, xb, yb, opt=None):\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    output = model(xb)\n",
    "    loss = criterion(output, yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def valid_batch(model, device, xb, yb):\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    output = model(xb)\n",
    "    loss = criterion(output, yb)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    correct = pred == yb.view(*pred.shape)\n",
    "\n",
    "    return loss.item(), torch.sum(correct).item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68528ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, epochs, train_dl, test_dl, device):\n",
    "    print(f'\\nExecutando o classificador: {model}')\n",
    "    \n",
    "    scheduler = None\n",
    "    if opt is not None:\n",
    "        scheduler = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Iniciando época {epoch + 1}/{epochs}')\n",
    "        model.train()\n",
    "        losses, nums = zip(\n",
    "            *[loss_batch(model, device, xb, yb, opt) for xb, yb in train_dl])\n",
    "        train_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, corrects, nums = zip(\n",
    "                *[valid_batch(model, device, xb, yb) for xb, yb in test_dl]\n",
    "            )      \n",
    "            \n",
    "            test_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "            test_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "            print(f\"- Train loss: {train_loss:.6f}\\t\",\n",
    "                  f\"Test loss: {test_loss:.6f}\\t\",\n",
    "                  f\"Test accuracy: {test_accuracy:.3f}%\")\n",
    "        \n",
    "        if opt is None:\n",
    "            print('Não há otimizador para este modelo. Loop de épocas será interrompido.')\n",
    "            return model\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9f6f4",
   "metadata": {},
   "source": [
    "### Definição de parâmetros gerais\n",
    "\n",
    "A seguir são definidos os parâmetros gerais utilizados pelos modelos.\n",
    "- O `lr` é o _learning rate_.\n",
    "- O `epochs` representa a quantidade de épocas ou iterações que serão executadas para treino do modelo.\n",
    "- O `momentum` é um parâmetro exclusivo para o otimizador do tipo SGD (Stochastic Gradient Descent).\n",
    "- O `l2_reg` é taxa de regularização ou penalização L2.\n",
    "- O `dropout` é autoexplicativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb43ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 50\n",
    "momentum = 0.9\n",
    "l2_reg = 0.001\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae46d6",
   "metadata": {},
   "source": [
    "Abaixo são definidos os modelos com os respectivos parâmetros. Eles foram organizados de acordo com o otimizador,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d461e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(choose_nn_model('perceptron', device, lr, input_size, output_size, optimizer = None))\n",
    "\n",
    "models.append(choose_nn_model('logisticregression', device, lr, input_size, output_size, optimizer = 'SGD', momentum=momentum, weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('logisticregression', device, lr, input_size, output_size, optimizer = 'SGD', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, optimizer = 'SGD', momentum=momentum, weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, optimizer = 'SGD', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, optimizer = 'SGD'))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'SGD', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, hidden=3, optimizer = 'SGD', weight_decay=l2_reg))\n",
    "\n",
    "models.append(choose_nn_model('logisticregression', device, lr, input_size, output_size, optimizer = 'Adam', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'Adam'))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, optimizer = 'Adam', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'Adam', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, hidden=3, optimizer = 'Adam', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'Adam'))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, optimizer = 'Adam'))\n",
    "\n",
    "models.append(choose_nn_model('logisticregression', device, lr, input_size, output_size, optimizer = 'AdamW', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'AdamW', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, hidden=3, optimizer = 'AdamW', weight_decay=l2_reg))\n",
    "\n",
    "models.append(choose_nn_model('logisticregression', device, lr, input_size, output_size, optimizer = 'ASGD', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, optimizer = 'ASGD', weight_decay=l2_reg))\n",
    "models.append(choose_nn_model('multilayer', device, lr, input_size, output_size, dropout=dropout, hidden=3, optimizer = 'ASGD', weight_decay=l2_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dadce2",
   "metadata": {},
   "source": [
    "Definição da função de custo. Além da função utilizada `F.cross_entropy` também é possível utilizar a função `F.multi_margin_loss`, sem que seja necessário efetuar quaisquer modificações nos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ebe450",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = F.cross_entropy\n",
    "#criterion = F.multi_margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "204c22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executando o classificador: Perceptron(\n",
      "  (fc): Linear(in_features=38, out_features=12, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 3.998804\t Test loss: 3.998864\t Test accuracy: 2.019%\n",
      "Não há otimizador para este modelo. Loop de épocas será interrompido.\n",
      "\n",
      "Executando o classificador: LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 0.996642\t Test loss: 0.916486\t Test accuracy: 67.433%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 0.899231\t Test loss: 0.888444\t Test accuracy: 68.410%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 0.883070\t Test loss: 0.878692\t Test accuracy: 68.640%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 0.876543\t Test loss: 0.874489\t Test accuracy: 68.871%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 0.873256\t Test loss: 0.871973\t Test accuracy: 68.891%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 0.871458\t Test loss: 0.870782\t Test accuracy: 69.041%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 0.870311\t Test loss: 0.869920\t Test accuracy: 69.003%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 0.869520\t Test loss: 0.869489\t Test accuracy: 69.271%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 0.869242\t Test loss: 0.868892\t Test accuracy: 69.114%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 0.868868\t Test loss: 0.868644\t Test accuracy: 69.237%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 0.868455\t Test loss: 0.868438\t Test accuracy: 69.152%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 0.868413\t Test loss: 0.868249\t Test accuracy: 69.248%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 0.868389\t Test loss: 0.868227\t Test accuracy: 69.296%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 0.868284\t Test loss: 0.869034\t Test accuracy: 69.481%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 0.868314\t Test loss: 0.868064\t Test accuracy: 69.241%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 0.868198\t Test loss: 0.868187\t Test accuracy: 69.209%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 0.868184\t Test loss: 0.868090\t Test accuracy: 69.240%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 0.868179\t Test loss: 0.868065\t Test accuracy: 69.265%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 0.868150\t Test loss: 0.868519\t Test accuracy: 69.456%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 0.868134\t Test loss: 0.867993\t Test accuracy: 69.179%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 0.868024\t Test loss: 0.867971\t Test accuracy: 69.266%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 0.868043\t Test loss: 0.867974\t Test accuracy: 69.312%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 0.867996\t Test loss: 0.868204\t Test accuracy: 69.376%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 0.868009\t Test loss: 0.867969\t Test accuracy: 69.195%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 0.868011\t Test loss: 0.867898\t Test accuracy: 69.313%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 0.867969\t Test loss: 0.867943\t Test accuracy: 69.372%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 0.867980\t Test loss: 0.867993\t Test accuracy: 69.366%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 0.867995\t Test loss: 0.867952\t Test accuracy: 69.260%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 0.867981\t Test loss: 0.867952\t Test accuracy: 69.361%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 0.867987\t Test loss: 0.867872\t Test accuracy: 69.310%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 0.867902\t Test loss: 0.867883\t Test accuracy: 69.240%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 0.867912\t Test loss: 0.867857\t Test accuracy: 69.299%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 0.867917\t Test loss: 0.867887\t Test accuracy: 69.239%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 0.867924\t Test loss: 0.867914\t Test accuracy: 69.332%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 0.867905\t Test loss: 0.867925\t Test accuracy: 69.282%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 0.867916\t Test loss: 0.867990\t Test accuracy: 69.403%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 0.867909\t Test loss: 0.867902\t Test accuracy: 69.251%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 0.867911\t Test loss: 0.867873\t Test accuracy: 69.326%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 0.867907\t Test loss: 0.867898\t Test accuracy: 69.319%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 0.867916\t Test loss: 0.867856\t Test accuracy: 69.283%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 0.867873\t Test loss: 0.867875\t Test accuracy: 69.349%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 0.867882\t Test loss: 0.867859\t Test accuracy: 69.317%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 0.867872\t Test loss: 0.867867\t Test accuracy: 69.266%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 0.867884\t Test loss: 0.867913\t Test accuracy: 69.348%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 0.867880\t Test loss: 0.867873\t Test accuracy: 69.304%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 0.867873\t Test loss: 0.867865\t Test accuracy: 69.322%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 0.867880\t Test loss: 0.867894\t Test accuracy: 69.299%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 0.867876\t Test loss: 0.867864\t Test accuracy: 69.308%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 0.867880\t Test loss: 0.867863\t Test accuracy: 69.288%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 0.867878\t Test loss: 0.867879\t Test accuracy: 69.255%\n",
      "\n",
      "Executando o classificador: LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.211583\t Test loss: 1.114629\t Test accuracy: 61.522%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.075484\t Test loss: 1.046658\t Test accuracy: 62.276%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.024201\t Test loss: 1.007139\t Test accuracy: 63.225%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 0.992188\t Test loss: 0.980748\t Test accuracy: 63.934%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 0.970021\t Test loss: 0.961894\t Test accuracy: 65.614%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 0.953797\t Test loss: 0.947680\t Test accuracy: 66.123%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 0.941436\t Test loss: 0.936757\t Test accuracy: 66.426%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 0.931801\t Test loss: 0.928068\t Test accuracy: 66.716%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 0.924064\t Test loss: 0.921054\t Test accuracy: 67.234%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 0.917749\t Test loss: 0.915318\t Test accuracy: 67.384%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 0.913706\t Test loss: 0.912765\t Test accuracy: 67.514%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 0.911308\t Test loss: 0.910464\t Test accuracy: 67.701%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 0.909113\t Test loss: 0.908365\t Test accuracy: 67.797%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 0.907089\t Test loss: 0.906367\t Test accuracy: 67.735%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 0.905215\t Test loss: 0.904549\t Test accuracy: 67.790%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 0.903482\t Test loss: 0.902868\t Test accuracy: 67.891%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 0.901873\t Test loss: 0.901299\t Test accuracy: 67.879%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 0.900366\t Test loss: 0.899837\t Test accuracy: 67.952%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 0.898958\t Test loss: 0.898466\t Test accuracy: 67.954%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 0.897654\t Test loss: 0.897217\t Test accuracy: 68.047%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 0.896710\t Test loss: 0.896573\t Test accuracy: 68.029%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 0.896116\t Test loss: 0.895986\t Test accuracy: 68.035%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 0.895535\t Test loss: 0.895412\t Test accuracy: 68.053%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 0.894972\t Test loss: 0.894860\t Test accuracy: 68.061%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 0.894428\t Test loss: 0.894316\t Test accuracy: 68.085%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 0.893908\t Test loss: 0.893794\t Test accuracy: 68.113%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 0.893393\t Test loss: 0.893284\t Test accuracy: 68.108%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 0.892898\t Test loss: 0.892795\t Test accuracy: 68.158%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 0.892415\t Test loss: 0.892335\t Test accuracy: 68.161%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 0.891947\t Test loss: 0.891855\t Test accuracy: 68.217%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 0.891595\t Test loss: 0.891624\t Test accuracy: 68.206%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 0.891372\t Test loss: 0.891396\t Test accuracy: 68.237%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 0.891147\t Test loss: 0.891175\t Test accuracy: 68.238%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 0.890929\t Test loss: 0.890959\t Test accuracy: 68.246%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 0.890713\t Test loss: 0.890740\t Test accuracy: 68.255%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 0.890501\t Test loss: 0.890532\t Test accuracy: 68.248%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 0.890291\t Test loss: 0.890319\t Test accuracy: 68.273%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 0.890083\t Test loss: 0.890115\t Test accuracy: 68.285%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 0.889881\t Test loss: 0.889910\t Test accuracy: 68.293%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 0.889678\t Test loss: 0.889708\t Test accuracy: 68.293%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 0.889526\t Test loss: 0.889604\t Test accuracy: 68.290%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 0.889425\t Test loss: 0.889503\t Test accuracy: 68.282%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 0.889328\t Test loss: 0.889404\t Test accuracy: 68.292%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 0.889230\t Test loss: 0.889308\t Test accuracy: 68.296%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 0.889133\t Test loss: 0.889211\t Test accuracy: 68.292%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 0.889036\t Test loss: 0.889115\t Test accuracy: 68.298%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 0.888940\t Test loss: 0.889017\t Test accuracy: 68.295%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 0.888845\t Test loss: 0.888923\t Test accuracy: 68.302%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 0.888750\t Test loss: 0.888826\t Test accuracy: 68.304%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 0.888657\t Test loss: 0.888733\t Test accuracy: 68.303%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.065421\t Test loss: 1.997341\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.987911\t Test loss: 1.983567\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.981189\t Test loss: 1.980644\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.979182\t Test loss: 1.977925\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.958571\t Test loss: 1.938301\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.930507\t Test loss: 1.926394\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.923260\t Test loss: 1.921730\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.920071\t Test loss: 1.919780\t Test accuracy: 61.533%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.918914\t Test loss: 1.919279\t Test accuracy: 61.531%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.918709\t Test loss: 1.919301\t Test accuracy: 61.528%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.918791\t Test loss: 1.919410\t Test accuracy: 61.529%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.918919\t Test loss: 1.919530\t Test accuracy: 61.528%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.919073\t Test loss: 1.919681\t Test accuracy: 61.528%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.919205\t Test loss: 1.919830\t Test accuracy: 61.528%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.919351\t Test loss: 1.919990\t Test accuracy: 61.528%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.919505\t Test loss: 1.920151\t Test accuracy: 61.528%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.919649\t Test loss: 1.920266\t Test accuracy: 61.527%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.919783\t Test loss: 1.920407\t Test accuracy: 61.527%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.919912\t Test loss: 1.920518\t Test accuracy: 61.527%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.920010\t Test loss: 1.920634\t Test accuracy: 61.526%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.920093\t Test loss: 1.920674\t Test accuracy: 61.526%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.920145\t Test loss: 1.920729\t Test accuracy: 61.527%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.920191\t Test loss: 1.920769\t Test accuracy: 61.526%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.920241\t Test loss: 1.920818\t Test accuracy: 61.526%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.920283\t Test loss: 1.920856\t Test accuracy: 61.526%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.920324\t Test loss: 1.920897\t Test accuracy: 61.526%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.920363\t Test loss: 1.920939\t Test accuracy: 61.524%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.920400\t Test loss: 1.920972\t Test accuracy: 61.526%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.920444\t Test loss: 1.921007\t Test accuracy: 61.526%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.920463\t Test loss: 1.921033\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.920485\t Test loss: 1.921049\t Test accuracy: 61.524%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.920502\t Test loss: 1.921070\t Test accuracy: 61.526%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.920516\t Test loss: 1.921085\t Test accuracy: 61.525%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.920533\t Test loss: 1.921098\t Test accuracy: 61.525%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.920541\t Test loss: 1.921108\t Test accuracy: 61.524%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.920559\t Test loss: 1.921122\t Test accuracy: 61.524%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.920573\t Test loss: 1.921137\t Test accuracy: 61.525%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.920584\t Test loss: 1.921146\t Test accuracy: 61.524%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.920599\t Test loss: 1.921160\t Test accuracy: 61.524%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.920609\t Test loss: 1.921181\t Test accuracy: 61.525%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.920619\t Test loss: 1.921182\t Test accuracy: 61.525%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.920622\t Test loss: 1.921183\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.920628\t Test loss: 1.921192\t Test accuracy: 61.524%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.920636\t Test loss: 1.921197\t Test accuracy: 61.524%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.920641\t Test loss: 1.921202\t Test accuracy: 61.524%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.920648\t Test loss: 1.921207\t Test accuracy: 61.524%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.920653\t Test loss: 1.921216\t Test accuracy: 61.524%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.920658\t Test loss: 1.921218\t Test accuracy: 61.524%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.920662\t Test loss: 1.921230\t Test accuracy: 61.525%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.920668\t Test loss: 1.921228\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.314545\t Test loss: 2.207085\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 2.161955\t Test loss: 2.126697\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 2.101644\t Test loss: 2.081458\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 2.065947\t Test loss: 2.053629\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 2.043425\t Test loss: 2.035552\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 2.028438\t Test loss: 2.023195\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 2.017972\t Test loss: 2.014372\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 2.010370\t Test loss: 2.007847\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 2.004669\t Test loss: 2.002879\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 2.000276\t Test loss: 1.999008\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.997587\t Test loss: 1.997385\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.996051\t Test loss: 1.995933\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.994673\t Test loss: 1.994628\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.993433\t Test loss: 1.993452\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.992313\t Test loss: 1.992388\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.991298\t Test loss: 1.991423\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.990377\t Test loss: 1.990545\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.989538\t Test loss: 1.989745\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.988773\t Test loss: 1.989014\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.988073\t Test loss: 1.988345\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.987584\t Test loss: 1.988031\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.987277\t Test loss: 1.987731\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.986983\t Test loss: 1.987443\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.986700\t Test loss: 1.987167\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.986429\t Test loss: 1.986901\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.986169\t Test loss: 1.986647\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.985920\t Test loss: 1.986403\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.985680\t Test loss: 1.986168\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.985449\t Test loss: 1.985943\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.985228\t Test loss: 1.985726\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.985068\t Test loss: 1.985621\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.984963\t Test loss: 1.985517\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.984861\t Test loss: 1.985416\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.984761\t Test loss: 1.985317\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.984662\t Test loss: 1.985220\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.984566\t Test loss: 1.985124\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.984471\t Test loss: 1.985030\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.984378\t Test loss: 1.984938\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.984287\t Test loss: 1.984848\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.984198\t Test loss: 1.984760\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.984132\t Test loss: 1.984716\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.984088\t Test loss: 1.984673\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.984045\t Test loss: 1.984630\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.984002\t Test loss: 1.984588\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.983960\t Test loss: 1.984545\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.983918\t Test loss: 1.984504\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.983877\t Test loss: 1.984463\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.983836\t Test loss: 1.984422\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.983795\t Test loss: 1.984381\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.983755\t Test loss: 1.984341\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.118038\t Test loss: 1.960508\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.954665\t Test loss: 1.952436\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.950868\t Test loss: 1.950670\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.949688\t Test loss: 1.949889\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.949086\t Test loss: 1.949429\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.948702\t Test loss: 1.949113\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.948427\t Test loss: 1.948874\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.948211\t Test loss: 1.948678\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.948030\t Test loss: 1.948510\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.947867\t Test loss: 1.948364\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.947758\t Test loss: 1.948291\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.947689\t Test loss: 1.948221\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.947621\t Test loss: 1.948156\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.947556\t Test loss: 1.948089\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.947490\t Test loss: 1.948031\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.947429\t Test loss: 1.947963\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.947365\t Test loss: 1.947902\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.947303\t Test loss: 1.947838\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.947242\t Test loss: 1.947776\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.947180\t Test loss: 1.947716\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.947133\t Test loss: 1.947684\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.947102\t Test loss: 1.947653\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.947071\t Test loss: 1.947622\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.947040\t Test loss: 1.947590\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.947010\t Test loss: 1.947559\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.946979\t Test loss: 1.947527\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.946947\t Test loss: 1.947496\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.946916\t Test loss: 1.947464\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.946885\t Test loss: 1.947432\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.946852\t Test loss: 1.947400\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.946828\t Test loss: 1.947384\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.946812\t Test loss: 1.947368\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.946796\t Test loss: 1.947352\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.946780\t Test loss: 1.947336\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.946764\t Test loss: 1.947320\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.946748\t Test loss: 1.947304\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.946732\t Test loss: 1.947287\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.946715\t Test loss: 1.947271\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.946699\t Test loss: 1.947254\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.946683\t Test loss: 1.947238\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.946670\t Test loss: 1.947229\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.946662\t Test loss: 1.947221\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.946653\t Test loss: 1.947213\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.946645\t Test loss: 1.947205\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.946637\t Test loss: 1.947196\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.946629\t Test loss: 1.947188\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.946620\t Test loss: 1.947179\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.946612\t Test loss: 1.947171\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.946604\t Test loss: 1.947163\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.946595\t Test loss: 1.947154\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.224111\t Test loss: 2.011405\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 2.069987\t Test loss: 1.975037\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 2.041832\t Test loss: 1.966590\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 2.026158\t Test loss: 1.962891\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 2.015665\t Test loss: 1.960671\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 2.008049\t Test loss: 1.959340\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 2.002317\t Test loss: 1.958475\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.997806\t Test loss: 1.957881\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.994218\t Test loss: 1.957456\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.991236\t Test loss: 1.957172\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.989446\t Test loss: 1.957049\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.988454\t Test loss: 1.956959\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.987575\t Test loss: 1.956895\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.986620\t Test loss: 1.956808\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.985704\t Test loss: 1.956744\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.985031\t Test loss: 1.956695\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.984269\t Test loss: 1.956663\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.983720\t Test loss: 1.956643\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.983062\t Test loss: 1.956600\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.982520\t Test loss: 1.956596\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.982131\t Test loss: 1.956582\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.982031\t Test loss: 1.956581\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.981780\t Test loss: 1.956579\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.981449\t Test loss: 1.956570\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.981116\t Test loss: 1.956566\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.981030\t Test loss: 1.956561\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.980910\t Test loss: 1.956565\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.980671\t Test loss: 1.956557\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.980462\t Test loss: 1.956538\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.980414\t Test loss: 1.956545\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.980107\t Test loss: 1.956543\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.980048\t Test loss: 1.956544\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.980012\t Test loss: 1.956543\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.979866\t Test loss: 1.956545\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.979807\t Test loss: 1.956545\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.979736\t Test loss: 1.956544\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.979610\t Test loss: 1.956544\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.979537\t Test loss: 1.956543\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.979480\t Test loss: 1.956541\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.979390\t Test loss: 1.956543\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.979331\t Test loss: 1.956548\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.979302\t Test loss: 1.956551\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.979271\t Test loss: 1.956551\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.979250\t Test loss: 1.956549\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.979265\t Test loss: 1.956551\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.979162\t Test loss: 1.956551\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.979193\t Test loss: 1.956553\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.979110\t Test loss: 1.956554\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.979145\t Test loss: 1.956555\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.979035\t Test loss: 1.956555\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.191713\t Test loss: 1.995285\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 2.071167\t Test loss: 1.965227\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 2.020702\t Test loss: 1.955760\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 2.004655\t Test loss: 1.953199\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.998105\t Test loss: 1.952272\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.993752\t Test loss: 1.951725\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.990561\t Test loss: 1.951262\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.987798\t Test loss: 1.950973\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.985689\t Test loss: 1.950915\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.983652\t Test loss: 1.950863\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.982632\t Test loss: 1.950820\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.981928\t Test loss: 1.950774\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.981238\t Test loss: 1.950754\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.980517\t Test loss: 1.950732\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.979979\t Test loss: 1.950721\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.979412\t Test loss: 1.950718\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.978947\t Test loss: 1.950723\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.978380\t Test loss: 1.950690\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.978068\t Test loss: 1.950716\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.977505\t Test loss: 1.950696\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.977292\t Test loss: 1.950697\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.977002\t Test loss: 1.950708\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.976801\t Test loss: 1.950705\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.976625\t Test loss: 1.950700\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.976394\t Test loss: 1.950687\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.976242\t Test loss: 1.950696\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.976119\t Test loss: 1.950701\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.975967\t Test loss: 1.950701\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.975801\t Test loss: 1.950714\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.975628\t Test loss: 1.950719\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.975472\t Test loss: 1.950716\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.975404\t Test loss: 1.950717\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.975421\t Test loss: 1.950719\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.975176\t Test loss: 1.950719\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.975176\t Test loss: 1.950716\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.975373\t Test loss: 1.950719\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.975154\t Test loss: 1.950715\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.975136\t Test loss: 1.950712\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.975132\t Test loss: 1.950721\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.974872\t Test loss: 1.950717\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.974796\t Test loss: 1.950714\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.974657\t Test loss: 1.950713\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.974896\t Test loss: 1.950718\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.974651\t Test loss: 1.950715\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.974757\t Test loss: 1.950720\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.974721\t Test loss: 1.950719\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.974652\t Test loss: 1.950721\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.974695\t Test loss: 1.950723\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.974579\t Test loss: 1.950723\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.974676\t Test loss: 1.950723\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 0.894335\t Test loss: 0.871874\t Test accuracy: 69.036%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 0.871662\t Test loss: 0.869915\t Test accuracy: 69.179%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 0.871263\t Test loss: 0.871375\t Test accuracy: 69.772%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 0.871138\t Test loss: 0.872198\t Test accuracy: 69.204%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 0.871357\t Test loss: 0.870717\t Test accuracy: 68.836%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 0.871273\t Test loss: 0.869075\t Test accuracy: 69.251%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 0.871265\t Test loss: 0.869513\t Test accuracy: 68.813%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 0.871252\t Test loss: 0.872220\t Test accuracy: 68.975%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 0.871347\t Test loss: 0.869130\t Test accuracy: 69.020%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 0.871393\t Test loss: 0.869565\t Test accuracy: 69.332%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 0.869527\t Test loss: 0.871295\t Test accuracy: 68.887%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 0.869726\t Test loss: 0.868197\t Test accuracy: 68.868%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 0.869656\t Test loss: 0.868916\t Test accuracy: 69.097%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 0.869706\t Test loss: 0.872471\t Test accuracy: 69.030%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 0.869579\t Test loss: 0.869690\t Test accuracy: 69.411%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 0.869573\t Test loss: 0.870352\t Test accuracy: 69.214%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 0.869595\t Test loss: 0.871434\t Test accuracy: 69.139%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 0.869647\t Test loss: 0.869117\t Test accuracy: 69.245%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 0.869676\t Test loss: 0.869458\t Test accuracy: 69.372%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 0.869575\t Test loss: 0.868339\t Test accuracy: 68.956%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 0.868754\t Test loss: 0.869002\t Test accuracy: 69.055%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 0.868850\t Test loss: 0.868436\t Test accuracy: 69.308%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 0.868778\t Test loss: 0.868012\t Test accuracy: 69.222%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 0.868744\t Test loss: 0.868609\t Test accuracy: 69.506%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 0.868826\t Test loss: 0.868233\t Test accuracy: 69.174%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 0.868713\t Test loss: 0.869603\t Test accuracy: 69.357%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 0.868804\t Test loss: 0.868623\t Test accuracy: 69.430%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 0.868762\t Test loss: 0.868467\t Test accuracy: 69.362%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 0.868784\t Test loss: 0.868717\t Test accuracy: 69.038%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 0.868768\t Test loss: 0.868902\t Test accuracy: 69.204%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 0.868443\t Test loss: 0.868345\t Test accuracy: 69.283%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 0.868359\t Test loss: 0.868835\t Test accuracy: 69.348%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 0.868359\t Test loss: 0.868672\t Test accuracy: 69.368%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 0.868298\t Test loss: 0.868232\t Test accuracy: 69.301%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 0.868372\t Test loss: 0.868128\t Test accuracy: 69.560%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 0.868291\t Test loss: 0.867991\t Test accuracy: 69.239%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 0.868364\t Test loss: 0.868300\t Test accuracy: 69.200%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 0.868307\t Test loss: 0.868220\t Test accuracy: 69.298%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 0.868336\t Test loss: 0.868049\t Test accuracy: 69.143%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 0.868318\t Test loss: 0.868130\t Test accuracy: 69.389%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 0.868044\t Test loss: 0.868309\t Test accuracy: 69.237%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 0.868069\t Test loss: 0.868086\t Test accuracy: 69.310%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 0.868132\t Test loss: 0.867977\t Test accuracy: 69.383%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 0.868074\t Test loss: 0.868108\t Test accuracy: 69.294%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 0.868039\t Test loss: 0.868207\t Test accuracy: 69.247%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 0.868183\t Test loss: 0.868045\t Test accuracy: 69.203%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 0.868103\t Test loss: 0.867933\t Test accuracy: 69.347%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 0.868116\t Test loss: 0.867936\t Test accuracy: 69.213%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 0.868055\t Test loss: 0.868304\t Test accuracy: 69.447%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 0.868104\t Test loss: 0.867950\t Test accuracy: 69.246%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.980315\t Test loss: 1.950638\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.949095\t Test loss: 1.949266\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.948521\t Test loss: 1.949107\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.948422\t Test loss: 1.949021\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.948396\t Test loss: 1.949042\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.948399\t Test loss: 1.949048\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.948402\t Test loss: 1.949038\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.948402\t Test loss: 1.949025\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.948400\t Test loss: 1.949019\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.948399\t Test loss: 1.949026\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.948394\t Test loss: 1.949019\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949027\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.948393\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.948389\t Test loss: 1.949045\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949014\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.948390\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.948395\t Test loss: 1.949016\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.948390\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949033\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.948389\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.948389\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.948388\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.948388\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.948388\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.948389\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949027\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949014\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949014\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949014\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.965092\t Test loss: 1.921069\t Test accuracy: 61.522%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.921190\t Test loss: 1.923131\t Test accuracy: 61.522%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.921334\t Test loss: 1.922548\t Test accuracy: 61.522%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.921321\t Test loss: 1.922074\t Test accuracy: 61.522%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.921293\t Test loss: 1.922030\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.921358\t Test loss: 1.921104\t Test accuracy: 61.522%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.921299\t Test loss: 1.921831\t Test accuracy: 61.527%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.921307\t Test loss: 1.921504\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.921328\t Test loss: 1.922085\t Test accuracy: 61.522%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.921332\t Test loss: 1.922041\t Test accuracy: 61.528%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.921188\t Test loss: 1.921810\t Test accuracy: 61.522%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.921151\t Test loss: 1.921961\t Test accuracy: 61.522%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.921154\t Test loss: 1.921756\t Test accuracy: 61.522%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.921199\t Test loss: 1.921358\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.921143\t Test loss: 1.922043\t Test accuracy: 61.522%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.921194\t Test loss: 1.922471\t Test accuracy: 61.522%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.921190\t Test loss: 1.921010\t Test accuracy: 61.522%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.921165\t Test loss: 1.921645\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.921155\t Test loss: 1.922091\t Test accuracy: 61.522%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.921191\t Test loss: 1.921944\t Test accuracy: 61.525%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.921138\t Test loss: 1.921300\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.921096\t Test loss: 1.921641\t Test accuracy: 61.522%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.921096\t Test loss: 1.921662\t Test accuracy: 61.525%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.921114\t Test loss: 1.921302\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.921092\t Test loss: 1.921652\t Test accuracy: 61.522%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.921113\t Test loss: 1.921606\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.921065\t Test loss: 1.921743\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.921087\t Test loss: 1.921661\t Test accuracy: 61.525%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.921108\t Test loss: 1.921594\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.921115\t Test loss: 1.921187\t Test accuracy: 61.525%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.921046\t Test loss: 1.921691\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.921064\t Test loss: 1.921664\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.921013\t Test loss: 1.921825\t Test accuracy: 61.522%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.921087\t Test loss: 1.921400\t Test accuracy: 61.527%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.921070\t Test loss: 1.921431\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.921056\t Test loss: 1.921711\t Test accuracy: 61.522%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.921064\t Test loss: 1.921511\t Test accuracy: 61.522%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.921044\t Test loss: 1.921886\t Test accuracy: 61.522%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.921112\t Test loss: 1.921414\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.921035\t Test loss: 1.921609\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.921026\t Test loss: 1.921619\t Test accuracy: 61.522%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.921073\t Test loss: 1.921643\t Test accuracy: 61.522%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.921019\t Test loss: 1.921633\t Test accuracy: 61.524%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.921050\t Test loss: 1.921665\t Test accuracy: 61.522%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.921020\t Test loss: 1.921597\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.921049\t Test loss: 1.921536\t Test accuracy: 61.522%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.921046\t Test loss: 1.921592\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.921042\t Test loss: 1.921595\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.921043\t Test loss: 1.921558\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.921034\t Test loss: 1.921609\t Test accuracy: 61.525%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.003048\t Test loss: 1.956833\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.974005\t Test loss: 1.957009\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.973941\t Test loss: 1.956376\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.974014\t Test loss: 1.956395\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.973868\t Test loss: 1.957135\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.973983\t Test loss: 1.956650\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.973936\t Test loss: 1.956109\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.973956\t Test loss: 1.958095\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.973992\t Test loss: 1.957263\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.973937\t Test loss: 1.956774\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.973882\t Test loss: 1.957526\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.973891\t Test loss: 1.956101\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.973919\t Test loss: 1.956278\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.973947\t Test loss: 1.956102\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.974023\t Test loss: 1.956570\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.973852\t Test loss: 1.956982\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.973970\t Test loss: 1.956532\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.973951\t Test loss: 1.956823\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.973937\t Test loss: 1.956847\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.973919\t Test loss: 1.957022\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.973912\t Test loss: 1.956623\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.973941\t Test loss: 1.956793\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.973880\t Test loss: 1.955927\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.973887\t Test loss: 1.956532\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.973964\t Test loss: 1.956047\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.973886\t Test loss: 1.956848\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.973915\t Test loss: 1.956394\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.973896\t Test loss: 1.956417\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.973860\t Test loss: 1.956716\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.973856\t Test loss: 1.956324\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.973808\t Test loss: 1.956506\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.973943\t Test loss: 1.956686\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.973939\t Test loss: 1.956754\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.973847\t Test loss: 1.956838\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.973950\t Test loss: 1.956351\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.973951\t Test loss: 1.956453\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.973920\t Test loss: 1.956667\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.973884\t Test loss: 1.956527\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.973881\t Test loss: 1.956473\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.973854\t Test loss: 1.956593\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.973863\t Test loss: 1.956366\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.973814\t Test loss: 1.956594\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.973862\t Test loss: 1.956449\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.973869\t Test loss: 1.956533\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.973920\t Test loss: 1.956660\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.973913\t Test loss: 1.956262\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.973893\t Test loss: 1.956519\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.973934\t Test loss: 1.956335\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.973843\t Test loss: 1.956484\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.973808\t Test loss: 1.956626\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.979053\t Test loss: 1.950826\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.966263\t Test loss: 1.948584\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.964869\t Test loss: 1.949041\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.964917\t Test loss: 1.948425\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.964935\t Test loss: 1.948548\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.964949\t Test loss: 1.948497\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.964963\t Test loss: 1.948630\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.965014\t Test loss: 1.948700\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.965049\t Test loss: 1.948128\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.965032\t Test loss: 1.948749\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.964996\t Test loss: 1.948568\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.964988\t Test loss: 1.948558\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.964990\t Test loss: 1.948683\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.965049\t Test loss: 1.948575\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.964973\t Test loss: 1.948807\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.964855\t Test loss: 1.948754\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.964908\t Test loss: 1.948959\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.964913\t Test loss: 1.948723\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.965026\t Test loss: 1.948764\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.965001\t Test loss: 1.948505\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.964892\t Test loss: 1.948554\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.964789\t Test loss: 1.948550\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.964985\t Test loss: 1.948679\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.964949\t Test loss: 1.948672\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.964913\t Test loss: 1.948568\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.964872\t Test loss: 1.948621\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.964906\t Test loss: 1.948719\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.964900\t Test loss: 1.948743\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.964966\t Test loss: 1.948973\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.964852\t Test loss: 1.948423\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.964849\t Test loss: 1.948758\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.964922\t Test loss: 1.948784\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.964863\t Test loss: 1.948758\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.964863\t Test loss: 1.948658\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.964816\t Test loss: 1.948845\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.964977\t Test loss: 1.948824\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.964921\t Test loss: 1.948666\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.964922\t Test loss: 1.948668\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.964828\t Test loss: 1.948663\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.964889\t Test loss: 1.948664\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.964885\t Test loss: 1.948806\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.964893\t Test loss: 1.948656\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.964889\t Test loss: 1.948667\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.964924\t Test loss: 1.948714\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.964923\t Test loss: 1.948750\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.964900\t Test loss: 1.948718\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.964866\t Test loss: 1.948745\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.964881\t Test loss: 1.948672\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.964910\t Test loss: 1.948826\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.964902\t Test loss: 1.948711\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.949765\t Test loss: 1.886663\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.906575\t Test loss: 1.881750\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.905180\t Test loss: 1.882727\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.905312\t Test loss: 1.876390\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.904606\t Test loss: 1.878419\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.904894\t Test loss: 1.872045\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.904723\t Test loss: 1.876314\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.904760\t Test loss: 1.867909\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.904905\t Test loss: 1.872787\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.904956\t Test loss: 1.878713\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.904148\t Test loss: 1.873694\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.904284\t Test loss: 1.882067\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.904115\t Test loss: 1.873063\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.904567\t Test loss: 1.880253\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.904540\t Test loss: 1.876284\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.904332\t Test loss: 1.876162\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.904301\t Test loss: 1.877465\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.904398\t Test loss: 1.875715\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.904435\t Test loss: 1.872328\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.904364\t Test loss: 1.880297\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.904441\t Test loss: 1.874740\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.904012\t Test loss: 1.876670\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.903778\t Test loss: 1.874687\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.904368\t Test loss: 1.874497\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.904241\t Test loss: 1.877006\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.904089\t Test loss: 1.877210\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.903750\t Test loss: 1.872789\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.904062\t Test loss: 1.873788\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.904185\t Test loss: 1.874716\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.903970\t Test loss: 1.871979\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.904215\t Test loss: 1.876352\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.903992\t Test loss: 1.876638\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.903738\t Test loss: 1.874467\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.904232\t Test loss: 1.876011\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.904091\t Test loss: 1.877879\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.903808\t Test loss: 1.877006\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.903763\t Test loss: 1.877068\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.903922\t Test loss: 1.875336\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.903741\t Test loss: 1.875376\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.903902\t Test loss: 1.876638\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.903876\t Test loss: 1.875184\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.903718\t Test loss: 1.874133\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.903812\t Test loss: 1.875320\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.903988\t Test loss: 1.875208\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.903878\t Test loss: 1.875683\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.903675\t Test loss: 1.875132\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.903714\t Test loss: 1.877682\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.903783\t Test loss: 1.875818\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.903656\t Test loss: 1.876788\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.903932\t Test loss: 1.876712\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.975690\t Test loss: 1.950423\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.949000\t Test loss: 1.949256\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.948509\t Test loss: 1.949062\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.948420\t Test loss: 1.949019\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.948401\t Test loss: 1.949030\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.948398\t Test loss: 1.949031\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.948405\t Test loss: 1.949018\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.948398\t Test loss: 1.949046\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.948403\t Test loss: 1.949024\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.948403\t Test loss: 1.949022\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.948394\t Test loss: 1.949029\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.948390\t Test loss: 1.949020\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.948393\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.948393\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.948393\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.948396\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.948390\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.948382\t Test loss: 1.949051\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.948392\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949020\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949014\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949018\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.948388\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.948387\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.948388\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949012\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949017\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.948385\t Test loss: 1.949013\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.948386\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.948383\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949010\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.948384\t Test loss: 1.949011\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 0.827372\t Test loss: 0.765103\t Test accuracy: 73.090%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 0.749315\t Test loss: 0.736171\t Test accuracy: 74.129%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 0.733964\t Test loss: 0.730212\t Test accuracy: 74.601%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 0.728990\t Test loss: 0.726414\t Test accuracy: 74.621%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 0.726951\t Test loss: 0.727124\t Test accuracy: 75.017%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 0.726069\t Test loss: 0.732064\t Test accuracy: 75.069%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 0.725517\t Test loss: 0.723302\t Test accuracy: 74.830%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 0.725155\t Test loss: 0.724066\t Test accuracy: 74.579%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 0.724873\t Test loss: 0.724152\t Test accuracy: 74.615%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 0.724620\t Test loss: 0.722283\t Test accuracy: 74.949%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 0.722920\t Test loss: 0.723858\t Test accuracy: 75.196%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 0.722866\t Test loss: 0.722083\t Test accuracy: 74.890%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 0.722672\t Test loss: 0.722197\t Test accuracy: 74.849%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 0.722763\t Test loss: 0.722827\t Test accuracy: 74.683%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 0.722571\t Test loss: 0.724753\t Test accuracy: 75.175%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 0.722635\t Test loss: 0.722128\t Test accuracy: 75.130%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 0.722523\t Test loss: 0.721744\t Test accuracy: 74.755%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 0.722557\t Test loss: 0.721536\t Test accuracy: 75.101%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 0.722477\t Test loss: 0.722198\t Test accuracy: 74.841%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 0.722467\t Test loss: 0.721968\t Test accuracy: 74.698%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 0.721569\t Test loss: 0.720859\t Test accuracy: 75.103%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 0.721529\t Test loss: 0.721018\t Test accuracy: 75.088%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 0.721514\t Test loss: 0.720763\t Test accuracy: 74.855%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 0.721468\t Test loss: 0.721180\t Test accuracy: 74.899%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 0.721516\t Test loss: 0.720865\t Test accuracy: 75.191%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 0.721412\t Test loss: 0.721118\t Test accuracy: 75.065%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 0.721438\t Test loss: 0.721121\t Test accuracy: 74.887%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 0.721428\t Test loss: 0.721486\t Test accuracy: 75.256%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 0.721385\t Test loss: 0.721231\t Test accuracy: 74.882%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 0.721339\t Test loss: 0.721051\t Test accuracy: 74.997%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 0.720934\t Test loss: 0.720391\t Test accuracy: 75.094%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 0.720925\t Test loss: 0.720453\t Test accuracy: 75.030%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 0.720915\t Test loss: 0.720275\t Test accuracy: 75.014%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 0.720906\t Test loss: 0.720116\t Test accuracy: 75.121%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 0.720874\t Test loss: 0.720487\t Test accuracy: 74.950%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 0.720926\t Test loss: 0.720357\t Test accuracy: 75.038%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 0.720890\t Test loss: 0.720441\t Test accuracy: 75.044%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 0.720863\t Test loss: 0.720193\t Test accuracy: 75.088%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 0.720911\t Test loss: 0.720230\t Test accuracy: 75.048%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 0.720858\t Test loss: 0.720282\t Test accuracy: 74.976%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 0.720641\t Test loss: 0.720206\t Test accuracy: 75.027%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 0.720647\t Test loss: 0.720194\t Test accuracy: 74.963%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 0.720622\t Test loss: 0.720145\t Test accuracy: 75.127%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 0.720636\t Test loss: 0.720097\t Test accuracy: 75.107%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 0.720615\t Test loss: 0.720202\t Test accuracy: 75.147%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 0.720616\t Test loss: 0.720051\t Test accuracy: 75.105%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 0.720611\t Test loss: 0.720161\t Test accuracy: 75.166%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 0.720620\t Test loss: 0.720073\t Test accuracy: 75.071%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 0.720617\t Test loss: 0.720047\t Test accuracy: 75.079%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 0.720577\t Test loss: 0.720044\t Test accuracy: 75.115%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.947653\t Test loss: 1.889418\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.912384\t Test loss: 1.887722\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.910280\t Test loss: 1.884738\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.909461\t Test loss: 1.886924\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.909344\t Test loss: 1.875557\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.909014\t Test loss: 1.876403\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.909003\t Test loss: 1.876878\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.908816\t Test loss: 1.878192\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.908978\t Test loss: 1.876863\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.908882\t Test loss: 1.877741\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.908348\t Test loss: 1.876282\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.908446\t Test loss: 1.879476\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.908304\t Test loss: 1.878184\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.908294\t Test loss: 1.881060\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.908628\t Test loss: 1.877092\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.908354\t Test loss: 1.884249\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.908727\t Test loss: 1.874817\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.908217\t Test loss: 1.879529\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.908643\t Test loss: 1.878303\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.908364\t Test loss: 1.878924\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.908009\t Test loss: 1.880556\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.908220\t Test loss: 1.883141\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.908244\t Test loss: 1.882351\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.908238\t Test loss: 1.878944\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.908263\t Test loss: 1.879815\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.908118\t Test loss: 1.879446\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.907908\t Test loss: 1.881031\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.908328\t Test loss: 1.876174\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.907992\t Test loss: 1.880140\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.908160\t Test loss: 1.879017\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.908059\t Test loss: 1.878558\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.908018\t Test loss: 1.881238\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.907975\t Test loss: 1.877661\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.908085\t Test loss: 1.878766\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.907867\t Test loss: 1.875689\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.908008\t Test loss: 1.878111\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.908060\t Test loss: 1.879814\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.907889\t Test loss: 1.879365\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.907997\t Test loss: 1.876811\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.908058\t Test loss: 1.877183\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.907967\t Test loss: 1.877715\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.907814\t Test loss: 1.878358\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.907898\t Test loss: 1.877575\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.908037\t Test loss: 1.878497\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.907988\t Test loss: 1.879273\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.907825\t Test loss: 1.878956\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.907754\t Test loss: 1.876481\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.907731\t Test loss: 1.879454\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.907953\t Test loss: 1.878754\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.907850\t Test loss: 1.881054\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.931641\t Test loss: 1.876409\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.896728\t Test loss: 1.869712\t Test accuracy: 62.204%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.892378\t Test loss: 1.864809\t Test accuracy: 62.059%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 1.890926\t Test loss: 1.863545\t Test accuracy: 62.089%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.890570\t Test loss: 1.861352\t Test accuracy: 62.261%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.890262\t Test loss: 1.860328\t Test accuracy: 61.934%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.889837\t Test loss: 1.862068\t Test accuracy: 61.976%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.889942\t Test loss: 1.860885\t Test accuracy: 60.944%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.889692\t Test loss: 1.862155\t Test accuracy: 60.550%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.889882\t Test loss: 1.859795\t Test accuracy: 61.223%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.888893\t Test loss: 1.860263\t Test accuracy: 60.268%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.888786\t Test loss: 1.860567\t Test accuracy: 60.530%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.888570\t Test loss: 1.860580\t Test accuracy: 59.960%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.888759\t Test loss: 1.861466\t Test accuracy: 60.119%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.888741\t Test loss: 1.860655\t Test accuracy: 60.169%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.888951\t Test loss: 1.860902\t Test accuracy: 59.929%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.888826\t Test loss: 1.865350\t Test accuracy: 59.937%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.889138\t Test loss: 1.860179\t Test accuracy: 59.135%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.888569\t Test loss: 1.863517\t Test accuracy: 59.921%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.888927\t Test loss: 1.860505\t Test accuracy: 59.291%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.888140\t Test loss: 1.861656\t Test accuracy: 59.568%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.888369\t Test loss: 1.860564\t Test accuracy: 59.066%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.888158\t Test loss: 1.860404\t Test accuracy: 59.532%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.887968\t Test loss: 1.862669\t Test accuracy: 59.437%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.888340\t Test loss: 1.865066\t Test accuracy: 59.990%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.888053\t Test loss: 1.861469\t Test accuracy: 59.248%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.887874\t Test loss: 1.861711\t Test accuracy: 59.138%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.888430\t Test loss: 1.860945\t Test accuracy: 58.827%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.887903\t Test loss: 1.863499\t Test accuracy: 59.503%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.887888\t Test loss: 1.862230\t Test accuracy: 59.063%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.887751\t Test loss: 1.861108\t Test accuracy: 59.227%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.888080\t Test loss: 1.861294\t Test accuracy: 59.127%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.888049\t Test loss: 1.860969\t Test accuracy: 59.042%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.887832\t Test loss: 1.863292\t Test accuracy: 59.473%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.887817\t Test loss: 1.860689\t Test accuracy: 59.157%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.887687\t Test loss: 1.861701\t Test accuracy: 58.966%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.888248\t Test loss: 1.860496\t Test accuracy: 59.108%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.887937\t Test loss: 1.861801\t Test accuracy: 59.256%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.888003\t Test loss: 1.861798\t Test accuracy: 59.076%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.887680\t Test loss: 1.860331\t Test accuracy: 58.731%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.887792\t Test loss: 1.860387\t Test accuracy: 59.144%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.887983\t Test loss: 1.860624\t Test accuracy: 58.903%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.888017\t Test loss: 1.862173\t Test accuracy: 59.191%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.887504\t Test loss: 1.860765\t Test accuracy: 58.803%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.887574\t Test loss: 1.861088\t Test accuracy: 58.982%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.887778\t Test loss: 1.861592\t Test accuracy: 59.164%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.887824\t Test loss: 1.861459\t Test accuracy: 59.058%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.887594\t Test loss: 1.861321\t Test accuracy: 59.095%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.887853\t Test loss: 1.861412\t Test accuracy: 59.136%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.887888\t Test loss: 1.861321\t Test accuracy: 59.243%\n",
      "\n",
      "Executando o classificador: LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 1.203713\t Test loss: 1.107334\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 1.070168\t Test loss: 1.042869\t Test accuracy: 61.987%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 1.021579\t Test loss: 1.005361\t Test accuracy: 63.313%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 0.991166\t Test loss: 0.980270\t Test accuracy: 63.970%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 0.970061\t Test loss: 0.962263\t Test accuracy: 65.533%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 0.954554\t Test loss: 0.948691\t Test accuracy: 66.121%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 0.942736\t Test loss: 0.938249\t Test accuracy: 66.458%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 0.933464\t Test loss: 0.929892\t Test accuracy: 67.046%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 0.926026\t Test loss: 0.923170\t Test accuracy: 67.242%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 0.919940\t Test loss: 0.917544\t Test accuracy: 67.405%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 0.915993\t Test loss: 0.915077\t Test accuracy: 67.545%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 0.913652\t Test loss: 0.912823\t Test accuracy: 67.612%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 0.911511\t Test loss: 0.910769\t Test accuracy: 67.616%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 0.909536\t Test loss: 0.908874\t Test accuracy: 67.642%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 0.907708\t Test loss: 0.907076\t Test accuracy: 67.700%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 0.906021\t Test loss: 0.905427\t Test accuracy: 67.787%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 0.904444\t Test loss: 0.903923\t Test accuracy: 67.783%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 0.902981\t Test loss: 0.902480\t Test accuracy: 67.873%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 0.901611\t Test loss: 0.901159\t Test accuracy: 67.958%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 0.900343\t Test loss: 0.899913\t Test accuracy: 67.959%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 0.899424\t Test loss: 0.899310\t Test accuracy: 67.954%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 0.898832\t Test loss: 0.898730\t Test accuracy: 68.015%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 0.898272\t Test loss: 0.898166\t Test accuracy: 68.007%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 0.897724\t Test loss: 0.897622\t Test accuracy: 68.023%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 0.897186\t Test loss: 0.897101\t Test accuracy: 68.028%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 0.896673\t Test loss: 0.896588\t Test accuracy: 68.021%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 0.896173\t Test loss: 0.896093\t Test accuracy: 68.053%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 0.895692\t Test loss: 0.895618\t Test accuracy: 68.044%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 0.895221\t Test loss: 0.895145\t Test accuracy: 68.050%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 0.894763\t Test loss: 0.894695\t Test accuracy: 68.071%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 0.894421\t Test loss: 0.894461\t Test accuracy: 68.075%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 0.894199\t Test loss: 0.894241\t Test accuracy: 68.083%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 0.893980\t Test loss: 0.894026\t Test accuracy: 68.088%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 0.893766\t Test loss: 0.893810\t Test accuracy: 68.092%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 0.893555\t Test loss: 0.893605\t Test accuracy: 68.108%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 0.893347\t Test loss: 0.893392\t Test accuracy: 68.107%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 0.893140\t Test loss: 0.893189\t Test accuracy: 68.097%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 0.892939\t Test loss: 0.892983\t Test accuracy: 68.112%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 0.892736\t Test loss: 0.892785\t Test accuracy: 68.113%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 0.892542\t Test loss: 0.892586\t Test accuracy: 68.129%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 0.892390\t Test loss: 0.892488\t Test accuracy: 68.135%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 0.892293\t Test loss: 0.892391\t Test accuracy: 68.141%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 0.892197\t Test loss: 0.892294\t Test accuracy: 68.141%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 0.892101\t Test loss: 0.892198\t Test accuracy: 68.151%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 0.892005\t Test loss: 0.892104\t Test accuracy: 68.143%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 0.891911\t Test loss: 0.892008\t Test accuracy: 68.161%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 0.891815\t Test loss: 0.891917\t Test accuracy: 68.188%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 0.891724\t Test loss: 0.891820\t Test accuracy: 68.173%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 0.891631\t Test loss: 0.891728\t Test accuracy: 68.194%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 0.891539\t Test loss: 0.891634\t Test accuracy: 68.178%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.283936\t Test loss: 2.190959\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 2.147190\t Test loss: 2.112997\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 2.089541\t Test loss: 2.070893\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 2.056934\t Test loss: 2.045977\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 2.036973\t Test loss: 2.030123\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 2.023886\t Test loss: 2.019389\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 2.014810\t Test loss: 2.011759\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 2.008236\t Test loss: 2.006124\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 2.003308\t Test loss: 2.001837\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.999517\t Test loss: 1.998495\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.997186\t Test loss: 1.997078\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.995845\t Test loss: 1.995812\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.994644\t Test loss: 1.994676\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.993564\t Test loss: 1.993653\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.992590\t Test loss: 1.992728\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.991708\t Test loss: 1.991891\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.990909\t Test loss: 1.991131\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.990183\t Test loss: 1.990438\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.989519\t Test loss: 1.989807\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.988914\t Test loss: 1.989229\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.988493\t Test loss: 1.988955\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.988224\t Test loss: 1.988694\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.987968\t Test loss: 1.988443\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.987722\t Test loss: 1.988203\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.987487\t Test loss: 1.987973\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.987261\t Test loss: 1.987752\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.987045\t Test loss: 1.987541\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.986838\t Test loss: 1.987338\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.986638\t Test loss: 1.987143\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.986448\t Test loss: 1.986956\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.986308\t Test loss: 1.986865\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.986218\t Test loss: 1.986775\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.986129\t Test loss: 1.986687\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.986042\t Test loss: 1.986601\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.985957\t Test loss: 1.986517\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.985872\t Test loss: 1.986434\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.985791\t Test loss: 1.986353\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.985710\t Test loss: 1.986274\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.985632\t Test loss: 1.986196\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.985554\t Test loss: 1.986119\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.985497\t Test loss: 1.986082\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.985459\t Test loss: 1.986044\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.985422\t Test loss: 1.986007\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.985385\t Test loss: 1.985970\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.985348\t Test loss: 1.985934\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.985312\t Test loss: 1.985898\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.985276\t Test loss: 1.985862\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.985240\t Test loss: 1.985827\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.985205\t Test loss: 1.985792\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.985171\t Test loss: 1.985757\t Test accuracy: 61.523%\n",
      "\n",
      "Executando o classificador: MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Iniciando época 1/50\n",
      "- Train loss: 2.181306\t Test loss: 2.004683\t Test accuracy: 61.523%\n",
      "Iniciando época 2/50\n",
      "- Train loss: 2.079505\t Test loss: 1.978448\t Test accuracy: 61.523%\n",
      "Iniciando época 3/50\n",
      "- Train loss: 2.046491\t Test loss: 1.965176\t Test accuracy: 61.523%\n",
      "Iniciando época 4/50\n",
      "- Train loss: 2.008668\t Test loss: 1.956169\t Test accuracy: 61.523%\n",
      "Iniciando época 5/50\n",
      "- Train loss: 1.993891\t Test loss: 1.953666\t Test accuracy: 61.523%\n",
      "Iniciando época 6/50\n",
      "- Train loss: 1.989497\t Test loss: 1.952702\t Test accuracy: 61.523%\n",
      "Iniciando época 7/50\n",
      "- Train loss: 1.986843\t Test loss: 1.952207\t Test accuracy: 61.523%\n",
      "Iniciando época 8/50\n",
      "- Train loss: 1.984828\t Test loss: 1.951918\t Test accuracy: 61.523%\n",
      "Iniciando época 9/50\n",
      "- Train loss: 1.983002\t Test loss: 1.951702\t Test accuracy: 61.523%\n",
      "Iniciando época 10/50\n",
      "- Train loss: 1.981821\t Test loss: 1.951586\t Test accuracy: 61.523%\n",
      "Iniciando época 11/50\n",
      "- Train loss: 1.980909\t Test loss: 1.951544\t Test accuracy: 61.523%\n",
      "Iniciando época 12/50\n",
      "- Train loss: 1.980264\t Test loss: 1.951488\t Test accuracy: 61.523%\n",
      "Iniciando época 13/50\n",
      "- Train loss: 1.979968\t Test loss: 1.951470\t Test accuracy: 61.523%\n",
      "Iniciando época 14/50\n",
      "- Train loss: 1.979256\t Test loss: 1.951441\t Test accuracy: 61.523%\n",
      "Iniciando época 15/50\n",
      "- Train loss: 1.978961\t Test loss: 1.951406\t Test accuracy: 61.523%\n",
      "Iniciando época 16/50\n",
      "- Train loss: 1.978480\t Test loss: 1.951397\t Test accuracy: 61.523%\n",
      "Iniciando época 17/50\n",
      "- Train loss: 1.978079\t Test loss: 1.951378\t Test accuracy: 61.523%\n",
      "Iniciando época 18/50\n",
      "- Train loss: 1.977790\t Test loss: 1.951369\t Test accuracy: 61.523%\n",
      "Iniciando época 19/50\n",
      "- Train loss: 1.977466\t Test loss: 1.951361\t Test accuracy: 61.523%\n",
      "Iniciando época 20/50\n",
      "- Train loss: 1.977224\t Test loss: 1.951348\t Test accuracy: 61.523%\n",
      "Iniciando época 21/50\n",
      "- Train loss: 1.976923\t Test loss: 1.951350\t Test accuracy: 61.523%\n",
      "Iniciando época 22/50\n",
      "- Train loss: 1.976750\t Test loss: 1.951346\t Test accuracy: 61.523%\n",
      "Iniciando época 23/50\n",
      "- Train loss: 1.976595\t Test loss: 1.951338\t Test accuracy: 61.523%\n",
      "Iniciando época 24/50\n",
      "- Train loss: 1.976531\t Test loss: 1.951341\t Test accuracy: 61.523%\n",
      "Iniciando época 25/50\n",
      "- Train loss: 1.976364\t Test loss: 1.951338\t Test accuracy: 61.523%\n",
      "Iniciando época 26/50\n",
      "- Train loss: 1.976130\t Test loss: 1.951323\t Test accuracy: 61.523%\n",
      "Iniciando época 27/50\n",
      "- Train loss: 1.976140\t Test loss: 1.951330\t Test accuracy: 61.523%\n",
      "Iniciando época 28/50\n",
      "- Train loss: 1.976008\t Test loss: 1.951323\t Test accuracy: 61.523%\n",
      "Iniciando época 29/50\n",
      "- Train loss: 1.975823\t Test loss: 1.951321\t Test accuracy: 61.523%\n",
      "Iniciando época 30/50\n",
      "- Train loss: 1.975642\t Test loss: 1.951321\t Test accuracy: 61.523%\n",
      "Iniciando época 31/50\n",
      "- Train loss: 1.975606\t Test loss: 1.951324\t Test accuracy: 61.523%\n",
      "Iniciando época 32/50\n",
      "- Train loss: 1.975704\t Test loss: 1.951327\t Test accuracy: 61.523%\n",
      "Iniciando época 33/50\n",
      "- Train loss: 1.975580\t Test loss: 1.951326\t Test accuracy: 61.523%\n",
      "Iniciando época 34/50\n",
      "- Train loss: 1.975556\t Test loss: 1.951331\t Test accuracy: 61.523%\n",
      "Iniciando época 35/50\n",
      "- Train loss: 1.975485\t Test loss: 1.951329\t Test accuracy: 61.523%\n",
      "Iniciando época 36/50\n",
      "- Train loss: 1.975439\t Test loss: 1.951329\t Test accuracy: 61.523%\n",
      "Iniciando época 37/50\n",
      "- Train loss: 1.975292\t Test loss: 1.951329\t Test accuracy: 61.523%\n",
      "Iniciando época 38/50\n",
      "- Train loss: 1.975285\t Test loss: 1.951327\t Test accuracy: 61.523%\n",
      "Iniciando época 39/50\n",
      "- Train loss: 1.975366\t Test loss: 1.951326\t Test accuracy: 61.523%\n",
      "Iniciando época 40/50\n",
      "- Train loss: 1.975221\t Test loss: 1.951322\t Test accuracy: 61.523%\n",
      "Iniciando época 41/50\n",
      "- Train loss: 1.975230\t Test loss: 1.951323\t Test accuracy: 61.523%\n",
      "Iniciando época 42/50\n",
      "- Train loss: 1.975179\t Test loss: 1.951327\t Test accuracy: 61.523%\n",
      "Iniciando época 43/50\n",
      "- Train loss: 1.975103\t Test loss: 1.951328\t Test accuracy: 61.523%\n",
      "Iniciando época 44/50\n",
      "- Train loss: 1.975120\t Test loss: 1.951328\t Test accuracy: 61.523%\n",
      "Iniciando época 45/50\n",
      "- Train loss: 1.975012\t Test loss: 1.951328\t Test accuracy: 61.523%\n",
      "Iniciando época 46/50\n",
      "- Train loss: 1.974919\t Test loss: 1.951328\t Test accuracy: 61.523%\n",
      "Iniciando época 47/50\n",
      "- Train loss: 1.974996\t Test loss: 1.951326\t Test accuracy: 61.523%\n",
      "Iniciando época 48/50\n",
      "- Train loss: 1.974984\t Test loss: 1.951325\t Test accuracy: 61.523%\n",
      "Iniciando época 49/50\n",
      "- Train loss: 1.975017\t Test loss: 1.951325\t Test accuracy: 61.523%\n",
      "Iniciando época 50/50\n",
      "- Train loss: 1.974920\t Test loss: 1.951326\t Test accuracy: 61.523%\n"
     ]
    }
   ],
   "source": [
    "fitted_models = []\n",
    "\n",
    "for model, opt in models:\n",
    "    model.to(device)\n",
    "    new_model = fit(model, opt, epochs, train_dl, test_dl, device)\n",
    "    fitted_models.append(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac888f",
   "metadata": {},
   "source": [
    "## Validação do modelo\n",
    "\n",
    "Após executar os modelos e aferi-los com o conjunto de treino e testes, chegou a hora de aplicá-lo sobre o conjunto de validação.\n",
    "\n",
    "Para isto serão utilizados os modelos já treinados inseridos no ``fitted_models``.\n",
    "\n",
    "Para cada modelo treinado, o conjunto de validação será executado e avaliado.\n",
    "\n",
    "Ao final, apenas o melhor modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27197efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses, corrects, nums = zip(\n",
    "            *[valid_batch(model, device, xb, yb) for xb, yb in dataloader]\n",
    "        )      \n",
    "        \n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        val_accuracy = np.sum(corrects) / np.sum(nums) * 100\n",
    "        print(f'Modelo {model.model_name()}\\t',\n",
    "              f'Loss: {val_loss:.6f}\\t',\n",
    "              f'Acurácia: {val_accuracy:3f}')\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a29eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron(\n",
      "  (fc): Linear(in_features=38, out_features=12, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Modelo Perceptron\t Loss: 3.998781\t Acurácia: 2.017337\n",
      "LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Modelo Logistic Regression\t Loss: 0.866127\t Acurácia: 69.171536\n",
      "LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Modelo Logistic Regression\t Loss: 0.886835\t Acurácia: 68.215836\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.920775\t Acurácia: 61.557826\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.984115\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.946913\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.956332\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.950499\t Acurácia: 61.556687\n",
      "LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Modelo Logistic Regression\t Loss: 0.866214\t Acurácia: 69.171536\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.948768\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.921160\t Acurácia: 61.558965\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.956390\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.948476\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.875730\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.948770\t Acurácia: 61.556687\n",
      "LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Modelo Logistic Regression\t Loss: 0.717175\t Acurácia: 75.240634\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.880736\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.861401\t Acurácia: 59.221543\n",
      "LogisticRegression(\n",
      "  (Linear): Linear(in_features=38, out_features=12, bias=True)\n",
      ")\n",
      "Modelo Logistic Regression\t Loss: 0.889838\t Acurácia: 68.071171\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=1, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.985531\t Acurácia: 61.556687\n",
      "MultiLayer(\n",
      "  (fc1): Linear(in_features=38, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Modelo Multi Layer\t Loss: 1.951108\t Acurácia: 61.556687\n"
     ]
    }
   ],
   "source": [
    "models_score = []\n",
    "for model in fitted_models:\n",
    "    print(model)\n",
    "    acc = validate(model, device, valid_dl)\n",
    "    models_score.append([model, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b917061",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [None, 0]\n",
    "for i, model in enumerate(models_score):\n",
    "    # Se a acurácia for superior, será o escolhido\n",
    "    if model[1] > best_model[1]:\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b048333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleção do melhor modelo\n",
    "best_model = best_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed20f8",
   "metadata": {},
   "source": [
    "## Aplicação do melhor modelo no dataset `hidden.csv`\n",
    "\n",
    "Após identificar o melhor modelo através da acurácia, o mesmo será aplicado sobre o dataset `hidden.csv`.\n",
    "Naturalmente, as mesmas etapas de pré-processamento (encode) dos dados será necessária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d4c5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_df = pd.read_csv('hidden.csv', sep=';' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e05dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_features_hidden(X):\n",
    "    X_mms = mms.transform(X[mms_columns])\n",
    "    X_ohe = ohe.transform(X[ohe_columns])\n",
    "    X_processed = np.hstack([X_ohe, X_mms])\n",
    "    \n",
    "    X_processed, n = map(torch.tensor, (X_processed, []))\n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86f76a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4558e-01, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6498e-01, 5.6433e-04,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4670e-01, 5.6433e-04,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4754e-01, 5.6433e-04,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3854e-01, 5.6433e-04,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.2286e-01, 5.6433e-04,\n",
       "         0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplica as transformações necessárias nas colunas preditoras\n",
    "X_hidden = return_features_hidden(hidden_df)\n",
    "X_hidden.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ced3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega o mapeamento das labels das classes com seus códigos\n",
    "label_encoder_dic = get_labelencoder_mapping(le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a58e20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realiza a predição utilizando o melhor modelo\n",
    "predictions = []\n",
    "best_model.to('cpu')\n",
    "best_model.eval()\n",
    "output = best_model(X_hidden)\n",
    "\n",
    "#A predição realizada identifica a probabilidade para cada classe.\n",
    "#Neste caso, pega-se apenas a classe com maior probabilidade para cada registro\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d0c3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte as prediçoes em um ndarray\n",
    "pred = predictions.cpu().detach().numpy()\n",
    "#Para cada predição, identificamos o alias (label) original da classe\n",
    "pred = [label_encoder_dic[x] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60b0f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hidden = pd.DataFrame(pred, columns=['lithology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76c4da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hidden.to_csv('hidden_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ea5db",
   "metadata": {},
   "source": [
    "# Considerações Finais\n",
    "\n",
    "A aplicação de redes neurais acima, contou com alguns recursos e técnicas para incrementar a performance geral dos modelos. Com destaque para:\n",
    "- Aplicação de penalidade/ regularização L2;\n",
    "- Aplicação do termo momentun no otimizador SGD para otimizar a velocidade de conversão da rede neural;\n",
    "- Aplicação de dropout (desligamento aleatório de neurônios) em alguns casos;\n",
    "- Aplicação de diversos otimizadores de inicialização e ajuste de pesos, sendo eles: SGD, Adam, AdamW e ASGD;\n",
    "- Aplicação de taxa de aprendizado variável, que reduz a cada \"x\" épocas executadas (vide ``scheduler.step()``)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
